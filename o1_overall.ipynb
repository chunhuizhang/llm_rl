{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eccd2feb-255d-4f59-817f-48e703bfb641",
   "metadata": {},
   "source": [
    "### self-play RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885026d0-aac6-4515-b861-8deed4479c49",
   "metadata": {},
   "source": [
    "- RL in LLMs\n",
    "    - Context \\ Action \\ Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d98b25-e7c4-4922-93f6-f04e69cfd1c9",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=EY9iHSe82Hc (Noam Brown)\n",
    "\n",
    "- games\n",
    "    - great verifier but a bad generator\n",
    "    - unlimited reward data\n",
    "- LLMs\n",
    "    - great generator but a bad verifier\n",
    "    - trillions of tokens of human text\n",
    "    - far less reward data\n",
    "- may change with time\n",
    "    - amount of reward data is increasing\n",
    "    - some domains are easier to score than others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc26ae-95ad-49ca-a7f0-81b3695e9a91",
   "metadata": {},
   "source": [
    "### reward model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7566d42c-36b4-4f5b-bccf-f0738c66959f",
   "metadata": {},
   "source": [
    "- 经典的基于 preference + ranking loss （BT model） 训练的 reward model；\n",
    "- PRM（Progress Reward Model）\n",
    "    - 在数学等推理问题上，紧靠最后答案的正确性（outcome）来提供奖励信号（reward singal）是不足的；\n",
    "    - 一种可能得解决方案即是，引入对于每一步解题步骤的打分，来提供**细粒度的奖励信号**；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
