{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794c82a7-367c-4921-b7a1-ef7fd39203e3",
   "metadata": {},
   "source": [
    "- https://safe-lip-9a8.notion.site/Incorrect-Baseline-Evaluations-Call-into-Question-Recent-LLM-RL-Claims-2012f1fbf0ee8094ab8ded1953c15a37\n",
    "- https://zhuanlan.zhihu.com/p/1913635433750992154"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc923549-193f-4651-8de2-ec739995317a",
   "metadata": {},
   "source": [
    "### 语言模型的“策略”和“熵”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb710ef2-657a-4e2f-a19e-7a1d36074b80",
   "metadata": {},
   "source": [
    "- 策略 (Policy)：就是模型在给定上下文中，生成下一个词（token）的概率分布。例如，在`search(`之后，模型可能会有30%的概率生成`query=`，20%的概率生成`topic=`，10%的概率生成`\"`等等。\n",
    "- 熵 (Entropy)：这个概率分布的“平坦”程度。\n",
    "    - 高熵：概率分布比较均匀，比如`query=`(30%), `topic=`(20%), `term=`(15%)... 模型对多种可能性都持开放态度，愿意探索。\n",
    "    - 低熵：概率分布非常尖锐，比如`query=`(99%), `topic=`(0.01%), `term=`(0.01%)... 模型极度确信应该生成`query=`，几乎不考虑其他选项。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19466f2-fff6-4d9c-a071-19d39357a237",
   "metadata": {},
   "source": [
    "- token entropy => seq entropy\n",
    "    - Maximizing Confidence Alone Improves Reasoning\n",
    "    - $H(p_t)=-\\sum_{v\\in\\mathcal V}p_t(v)\\log p_t(v)$\n",
    "    - $H(\\pi(x))=\\frac1T\\sum_{t=1}^T\\sum_{v\\in\\mathcal V}p_t(v)\\log p_t(v)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc1a38f-9c32-4577-becc-cc48eca607b2",
   "metadata": {},
   "source": [
    "### (kimi) Negative Sample Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e574af1-9bf1-4460-8184-0685d700e9cb",
   "metadata": {},
   "source": [
    "https://moonshotai.github.io/Kimi-Researcher/\n",
    "- Negative Sample Control: Negative samples lead to a decrease in token probabilities, which increases the risk of entropy collapse during RL training. To address this, we discard some negative samples strategically, allowing the model to continue improving over a longer training period.\n",
    "    - 一个害怕犯错的学生\n",
    "    - 初次犯错与惩罚：学生第一次尝试新方法A失败了，受到惩罚。他学到了：“方法A有风险”。于是，他以后使用方法A的意愿（概率）降低了。\n",
    "    - 再次犯错与惩罚：他又鼓起勇气尝试了新方法B，又不巧失败了，再次受到严厉惩罚。他学到了：“方法B也有风险”。于是，使用方法B的意愿也降低了。\n",
    "    - 形成“安全区”：经过几次这样的失败尝试后，学生变得越来越害怕犯错。他发现，只要他使用老师教过的最基本、最简单的“标准方法C”，虽然解题慢、而且解不了难题，但至少不会算错，也就不会被惩罚。\n",
    "    - 思路僵化（熵坍缩）：最终，这个学生为了避免惩罚，彻底放弃了所有探索性的、有风险的新方法。他的解题思路变得极其狭窄和单一，只会机械地重复那个“安全”的方法C。他的创造力（熵）完全消失了，这就是熵坍缩。他再也无法学会如何解决更复杂的、需要新思路的难题了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f6d45d-e27a-44fa-900b-32d0620360b7",
   "metadata": {},
   "source": [
    "### 高熵 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5184887b-22a2-4055-a5e5-d241f34392b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
