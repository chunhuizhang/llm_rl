{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10340d3-1663-4a7b-bd4c-f8908cbe72f4",
   "metadata": {},
   "source": [
    "> scaling of (Generalist) reward model\n",
    "\n",
    "- 两个重点\n",
    "    - 一个是 **scaling**\n",
    "        - from one to three scaling laws\n",
    "            - pre-training\n",
    "            - post-training\n",
    "                - RL\n",
    "            - test-time scaling\n",
    "                - 根据 query，实时对计算弹性伸缩；\n",
    "    - 一个是 **general** (通用性)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c75404-8b3f-4051-aa65-ca409b1959cf",
   "metadata": {},
   "source": [
    "### Point-wise GRM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093fdbb7-0bfb-4d2b-9a3e-c15b84c2c1ad",
   "metadata": {},
   "source": [
    "- principle => critic => score\n",
    "    - 奖励过程思维链化（CoT）了\n",
    "    - 定标准，写评价，再打分；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e94f9-9f2c-4b23-b9c6-ae3eb6bdae26",
   "metadata": {},
   "source": [
    "### 输入输出的角度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e0b205-ca91-4b73-a7f4-a5db5f4d7f95",
   "metadata": {},
   "source": [
    "- GRM 的输入与传统的奖励模型非常相似，主要包括：\n",
    "    - 查询 (Query)：用户的提问或指令，在论文中用 $x$ 表示。\n",
    "    - 一个或多个候选回答 (Responses)：模型针对该查询生成的不同回答，在论文中用 $\\{y_i\\}$ 表示。\n",
    "    - 这篇论文的一个创新点在于，他们还引入了一个额外的输入：\n",
    "        - 原则 (Principles)：这是一组预先定义或由模型自己生成的评估标准/指导原则，在论文中用 $\\{p_j\\}$ 表示。这些原则告诉模型应该从哪些维度（如准确性、清晰度、安全性等）来评估回答。\n",
    "- 所以，GRM 的完整输入可以看作是：(查询, 候选回答, 评估原则)。\n",
    "    - $r_θ(x, {y_i}, {p_j})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b40799d-9045-46ad-a55e-2dacbb8d7991",
   "metadata": {},
   "source": [
    "- 这是 GRM 与传统标量奖励模型最根本的区别。GRM 的输出是一段结构化的自然语言文本，这篇论文中称之为**“评判” (Critique)**，用 $C$ 表示。\n",
    "    - 分析过程 (Analysis)：模型会像一个真正的裁判一样，根据输入的“评估原则”，逐一分析每个“候选回答”的优缺点。\n",
    "    - 对比和结论 (Comparison & Conclusion)：在分析之后，模型会给出一个明确的对比结论，比如“回答2比回答1更好”。\n",
    "    - 最终的奖励分数 (Final Scores)：最关键的是，在这段生成的文本末尾，模型会按照一个预设的格式输出最终的打分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582007e6-f108-4f9b-acb1-25a628e0e3f3",
   "metadata": {},
   "source": [
    "### Meta RM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f9bf98-eb7a-4e4f-a3e3-d5c4c3828638",
   "metadata": {},
   "source": [
    "- 第二层级的、独立的奖励模型，即元奖励模型 (Meta Reward Model, Meta RM) 输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ac693-72cd-4544-86d4-310e47c5a854",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0895cabe-0fa3-4185-a323-a2bf4f62ca57",
   "metadata": {},
   "source": [
    "- table 17\n",
    "    - 模型单次运行的结果可能存在随机性或偏差。为了得到一个更稳定、更可靠的评估，研究人员会让同一个模型（DeepSeek-GRM）对同样的问题和回答，独立地运行多次（在这个表格里是3次，产生了 Result 1, 2, 3）。\n",
    "    - Result 1 的打分是 \\boxed{8, 8}。\n",
    "        - Response 1 得了 8 分。\n",
    "        - Response 2 得了 8 分。\n",
    "    - Result 2 的打分是 \\boxed{9, 5}。\n",
    "        - Response 1 得了 9 分。\n",
    "        - Response 2 得了 5 分。\n",
    "    - Result 3 的打分是 \\boxed{10, 7}。\n",
    "        - Response 1 得了 10 分。\n",
    "        - Response 2 得了 7 分。\n",
    "    - Response 1 的总票数: 8 (来自Result 1) + 9 (来自Result 2) + 10 (来自Result 3) = 27 分\n",
    "    - Response 2 的总票数: 8 (来自Result 1) + 5 (来自Result 2) + 7 (来自Result 3) = 20 分\n",
    "- The input order of responses is reversed for DeepSeek-GRM-27B when generating result 2 and result 3.\"\n",
    "    - 这意味着在第2次和第3次运行时，模型看到的“Response 1”其实是我们的“Response 2”，它看到的“Response 2”其实是我们的“Response 1”。这是一种为了避免位置偏见而常用的技巧。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
