{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34d2801-3bb4-4ad0-82e3-191192b9dc42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T02:30:42.405720Z",
     "iopub.status.busy": "2025-02-15T02:30:42.405161Z",
     "iopub.status.idle": "2025-02-15T02:30:42.414561Z",
     "shell.execute_reply": "2025-02-15T02:30:42.412857Z",
     "shell.execute_reply.started": "2025-02-15T02:30:42.405675Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0313031-b4b9-4ca5-9c2c-f13ea11d7055",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9477af70-556e-4169-a525-6d822f4a5895",
   "metadata": {},
   "source": [
    "- https://jonathan-hui.medium.com/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08832e50-447c-4d7b-b7e8-c1d99c4cc7f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:23:19.589390Z",
     "iopub.status.busy": "2025-02-15T04:23:19.588820Z",
     "iopub.status.idle": "2025-02-15T04:23:19.601345Z",
     "shell.execute_reply": "2025-02-15T04:23:19.599158Z",
     "shell.execute_reply.started": "2025-02-15T04:23:19.589340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*jJWyoqz-hxDyicGtmOUY7A.jpeg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/v2/resize:fit:720/format:webp/1*jJWyoqz-hxDyicGtmOUY7A.jpeg', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dfef250-ecb3-4b67-94b1-656e362c8a11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:24:46.486313Z",
     "iopub.status.busy": "2025-02-15T04:24:46.485742Z",
     "iopub.status.idle": "2025-02-15T04:24:46.498411Z",
     "shell.execute_reply": "2025-02-15T04:24:46.496071Z",
     "shell.execute_reply.started": "2025-02-15T04:24:46.486263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/trpo.png\" width=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 王树森\n",
    "Image(url='./imgs/trpo.png', width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ff007-581b-494d-90ee-3b0dc095498f",
   "metadata": {},
   "source": [
    "- PPO 是对 TRPO 的简化，TRPO 的优化需要用到 hessians\n",
    "- PPO 实现对 trust region 的约束是通过 small epsilon changes（clipping）\n",
    "    - PPO-clip 是 surrogate loss\n",
    "        - 一种 penalty 是 $\\frac{\\pi_\\theta}{\\pi_{old}}A-\\beta\\cdot kl(\\pi_{old},\\pi_\\theta)$\n",
    "    - PPO-Clip doesn't have a KL-divergence term in the objective\n",
    "        - https://spinningup.openai.com/en/latest/algorithms/ppo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca369e-6b7c-4778-a73f-97f160b98d7e",
   "metadata": {},
   "source": [
    "### ORPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63315713-22c8-4a27-b629-5ca59a10bd4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T02:30:53.935822Z",
     "iopub.status.busy": "2025-02-15T02:30:53.935258Z",
     "iopub.status.idle": "2025-02-15T02:30:53.954555Z",
     "shell.execute_reply": "2025-02-15T02:30:53.952370Z",
     "shell.execute_reply.started": "2025-02-15T02:30:53.935773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/0*Zj9U1ZGruyl0ps43\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/v2/resize:fit:4800/format:webp/0*Zj9U1ZGruyl0ps43', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef26bb-1366-4e04-a906-bfb088089c32",
   "metadata": {},
   "source": [
    "- ORPO\n",
    "    - an easier way to apply ppo, a bit like DPO;\n",
    "    - loss = cross entropy loss + beta * odds_ratio (偏序数据);\n",
    "        - odds_ratio are the ratios of odds between the chosen and the rejected responses;\n",
    "        - SFT or ORPO using verified pairs of data;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
