{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c1c20a1-d61a-4a40-a96e-2fed223744bc",
   "metadata": {},
   "source": [
    "- https://verl.readthedocs.io/en/latest/examples/config.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3caaa88b-2938-4eb6-b1d3-376e2a3b4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceffe1a7-2bd9-456e-8b91-3fceeee6470c",
   "metadata": {},
   "source": [
    "- 带 kl 散度惩罚的强化学习目标\n",
    "    - dpo (RL Fine-Tuning Phase)\n",
    "\n",
    "$$\n",
    "\\mathcal{J}(\\pi_\\theta) = \\mathbb{E}_{\\mathbf{q} \\sim p_Q} \\left[ \\mathbb{E}_{\\mathbf{o} \\sim \\pi_\\theta(\\cdot|\\mathbf{q})} [R(\\mathbf{q}, \\mathbf{o})] - \\beta D_{KL}[\\pi_\\theta(\\cdot|\\mathbf{q}) || \\pi_{\\text{ref}}(\\cdot|\\mathbf{q})] \\right]\n",
    "$$\n",
    "\n",
    "- PPO-Clip 目标函数\n",
    "\n",
    "$$\n",
    "\\mathcal{J}_{\\text{PPO}}(\\pi_\\theta) = \\mathbb{E}_{\\mathbf{q} \\sim p_Q, \\mathbf{o} \\sim \\pi_{\\theta_{\\text{old}}}(\\cdot|\\mathbf{q})} \\left[ \\sum_{t=1}^{|\\mathbf{o}|} \\min \\left[ \\frac{\\pi_\\theta(o_t|\\mathbf{q}, \\mathbf{o}_{<t})}{\\pi_{\\theta_{\\text{old}}}(o_t|\\mathbf{q}, \\mathbf{o}_{<t})} \\hat{A}_t, \\text{clip}\\left(\\frac{\\pi_\\theta(o_t|\\mathbf{q}, \\mathbf{o}_{<t})}{\\pi_{\\theta_{\\text{old}}}(o_t|\\mathbf{q}, \\mathbf{o}_{<t})}, 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t \\right] \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "- 公式1 是一个高层次的、理论上的目标。它告诉我们“我们想要什么”：一个高奖励且行为正常的模型。\n",
    "- 公式2 是一个具体的、可操作的算法。它告诉我们“我们具体怎么做”：通过PPO算法稳定地更新模型参数以达成目标。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1a4260-0f94-44a5-89bd-6de4d73c2c79",
   "metadata": {},
   "source": [
    "## pg loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d6b2eb-a65a-4bd8-b8ab-8a4484a7ca5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/blog/assets/93_deep_rl_ppo/recap.jpg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://huggingface.co/blog/assets/93_deep_rl_ppo/recap.jpg', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b1bbb7-7329-4261-ba1a-25a169c2f033",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/deep-rl-ppo\n",
    "$$\n",
    "L^{PPO}=E_{t}[\\max(-r_t(\\theta)\\hat A_t, -\\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat A_t)]\n",
    "$$\n",
    "\n",
    "- 当 $\\hat A_t\\gt 0$（好动作）时，不希望 $r_t(\\theta)$ 过大，将其限制在 $1+\\epsilon$ 之内\n",
    "    - $r_t(\\theta) > 1+\\epsilon$\n",
    "    - $\\pi_\\theta(a_t|s_t) >> \\pi_{\\theta_{old}}(a_t|s_t)$\n",
    "- 当 $\\hat A_t\\lt 0$（好动作）时，不希望 $r_t(\\theta)$ 过小，将其限制在 $1-\\epsilon$ 之内\n",
    "    - $r_t(\\theta) < 1-\\epsilon$\n",
    "- 还需要注意 gradient 的部分；\n",
    "    - 什么情况下不学习（不更新模型）\n",
    "    - 方向盘（梯度） vs. 放大器\n",
    "- sign of objective is sign of $\\hat A_t$\n",
    "    - 正 objective：$\\hat A_t\\gt 0$，增大 $\\pi_\\theta(a_t|s_t)$\n",
    "    - 负 objective：$\\hat A_t\\lt 0$，降低 $\\pi_\\theta(a_t|s_t)$\n",
    "- $\\hat A_t$: 不贡献梯度（是一个不在计算图中的标量）\n",
    "    - $\\hat A_t$ 的计算来源于 $\\pi_{\\theta_{old}}$（$\\pi_{\\theta_{old}$ 在一次 ppo epoch 中只负责 generation 与采集数据）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af35d4e4-dba2-43fa-aa4b-25f685ff6c40",
   "metadata": {},
   "source": [
    "### pg loss curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a6a635-4cf8-4310-b561-60db7ff7c3a9",
   "metadata": {},
   "source": [
    "- policy_loss 的正负直接反映了在一个训练批次（batch）中，智能体所采取的行动的平均质量（相对于价值函数的评估）。\n",
    "- policy_loss < 0 (负数)代表什么？\n",
    "    - 这意味着 L_CLIP(θ) 是正数。\n",
    "    - 深层含义？ 在这个批次中，“好”的动作（$\\hat A_t>0$）所带来的正面影响，超过了“坏”的动作（$\\hat A_t < 0$）带来的负面影响。简单来说，智能体在这个批次中的平均表现超出了它自己的预期（价值函数 $V(s_t)$）。\n",
    "优化方向？ 优化器会试图最小化这个负数，即让它变得更负。这会进一步增强那些“好”动作的概率。\n",
    "    - 是好是坏？\n",
    "        - 通常是好兆头：表明智能体正在有效地探索并找到了能带来更高回报的策略。这是学习在正常发生的信号。\n",
    "    - 潜在风险：如果 policy_loss 持续保持在非常大的负值，可能意味着价值函数（Critic）的更新跟不上策略（Actor）的提升。Critic 总是低估当前策略的价值，导致 Advantage 持续偏高。这可能预示着潜在的不稳定。\n",
    "- policy_loss > 0 (正数)\n",
    "    - 代表什么？ 这意味着 L_CLIP(θ) 是负数。\n",
    "    - 深层含义？ 在这个批次中，“坏”的动作所带来的负面影响，超过了“好”的动作带来的正面影响。智能体的平均表现低于它自己的预期。\n",
    "    - 优化方向？ 优化器会试图最小化这个正数，即让它向 0 靠近。这会削弱那些“坏”动作的概率。\n",
    "    - 是好是坏？\n",
    "        - 这是学习的正常组成部分：智能体需要通过试错来学习。采取了坏的动作，然后通过正的 policy_loss 来惩罚这些动作，这是完全正常的。\n",
    "        - 危险信号：如果 policy_loss 持续保持在较高的正值，这是一个非常糟糕的信号。它意味着智能体在不断地做出比自己预期还要差的决策，策略可能正在恶化或完全没有学到东西。\n",
    "- 理想的趋势：在 0 附近震荡，无明显上升或下降趋势\n",
    "    - 一个完美的策略和价值函数组合，意味着对于任何状态，价值函数都能准确预测期望回报。因此，任何动作的优势函数 $\\hat A_t$ 都会趋近于 0。\n",
    "    - 在实际训练中，策略和价值函数是交替迭代、相互追赶的。策略稍微变好一点 (policy_loss 变负)，价值函数马上学习跟上，将 policy_loss 拉回到 0 附近。策略尝试了坏动作 (policy_loss 变正)，然后修正自己，又回到 0 附近。\n",
    "    - 稳定学习：Actor 和 Critic 步调一致，策略在信赖域内被稳定地优化。\n",
    "    - 有效基线：价值函数提供了一个准确的基线（baseline），使得优势函数的估计是有意义的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce10529-f852-481f-b934-0a56ee4796cb",
   "metadata": {},
   "source": [
    "### grpo pg loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad83ccc-6d9a-4985-a40d-7ea4afbef659",
   "metadata": {},
   "source": [
    "- $\\sum_{i,A_i>0}r_{i,t}A_{i,t} + \\sum_{j,A_j<0}r_{j,t}A_{j,t}>0$\n",
    "    - $r_{i,t}>1, r_{j,t}\\lt 1$\n",
    "- $\\sum_{i,A_i>0}r_{i,t}A_{i,t} + \\sum_{j,A_j<0}r_{j,t}A_{j,t}<0$\n",
    "    - $r_{i,t}<1, r_{j,t}> 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccbbf63-a95e-4578-b199-c366ca94cc3e",
   "metadata": {},
   "source": [
    "### dual-clip ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0f2871-e44d-4a40-abd1-255ec2f3e595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/dual-clip.png\" width=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/dual-clip.png', width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1fa4c8-8c67-43f4-baad-313cd65b05da",
   "metadata": {},
   "source": [
    "- Dual-Clip PPO\n",
    "    - https://arxiv.org/pdf/1912.09729\n",
    " \n",
    "$$\n",
    "L^{\\text{dual-clip}}=\\min(L^{PPO}, -c\\cdot \\hat A_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531aa8b3-63ff-4fe0-b554-7ff1012596e7",
   "metadata": {},
   "source": [
    "### entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b213b-301b-4c66-b638-2994c4117442",
   "metadata": {},
   "source": [
    "- actor\n",
    "    - entropy_coeff: 0.0 (default)\n",
    "```python\n",
    "# entropy = verl_F.entropy_from_logits(logits)  # (bsz, response_length)\n",
    "policy_loss = pg_loss - entropy_loss * entropy_coeff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e30dc-4085-4906-9f10-47ac7d200bdd",
   "metadata": {},
   "source": [
    "```python\n",
    "def entropy_from_logits(logits: torch.Tensor):\n",
    "    \"\"\"Calculate entropy from logits.\"\"\"\n",
    "    pd = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    entropy = torch.logsumexp(logits, dim=-1) - torch.sum(pd * logits, dim=-1)\n",
    "    return entropy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bda746-ce17-48b7-ad6e-d52a16c319fe",
   "metadata": {},
   "source": [
    "### use_kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b101b9d-6a89-4aae-be4f-6329542b7338",
   "metadata": {},
   "source": [
    "```python\n",
    "policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be46d4-8ed1-4ac5-a014-aa685186c65a",
   "metadata": {},
   "source": [
    "### agg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d239b27-2b61-4a1b-bf27-8bc7f164188d",
   "metadata": {},
   "source": [
    "- $L\\in R^{B\\times T}$ 表示 loss mat, $M\\in {\\{0,1\\}}^{B\\times T}$ 表示损失掩码 (loss_mask)（为 1 表示损失计算在内）\n",
    "    - $ \\mathcal{L}_{\\text{token-mean}} = \\frac{\\sum_{i=1}^{B} \\sum_{j=1}^{T} L_{i,j} \\cdot M_{i,j}}{\\sum_{i=1}^{B} \\sum_{j=1}^{T} M_{i,j}} $\n",
    "    - $\\mathcal{L}_{\\text{seq-mean-token-sum}} = \\frac{1}{B} \\sum_{i=1}^{B} \\left( \\sum_{j=1}^{T} L_{i,j} \\cdot M_{i,j} \\right)$\n",
    "    - $\\mathcal{L}_{\\text{seq-mean-token-mean}} = \\frac{1}{B} \\sum_{i=1}^{B} \\left( \\frac{\\sum_{j=1}^{T} L_{i,j} \\cdot M_{i,j}}{\\sum_{j=1}^{T} M_{i,j}} \\right)$\n",
    "    - $\\mathcal{L}_{\\text{seq-mean-token-sum-norm}} = \\frac{\\sum_{i=1}^{B} \\sum_{j=1}^{T} L_{i,j} \\cdot M_{i,j}}{T} $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
