{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06eff751-be3c-439e-8725-c68aac15b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b052c7f0-d59c-47ed-9b8b-4a892955aee6",
   "metadata": {},
   "source": [
    "- https://www.bilibili.com/video/BV1TMRHYJEaw/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ddc25-51d8-4d54-a07f-075ca75345d1",
   "metadata": {},
   "source": [
    "## qwen-vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eec163ec-a31e-40fc-bbf2-ffeaeaf50f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/qwen-vl-pipeline.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/qwen-vl-pipeline.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af1ed3-5ff7-43de-ba60-9ca46019cedd",
   "metadata": {},
   "source": [
    "- Visual Encoder\n",
    "    - pre-trained weights from Openclip’s ViT-bigG\n",
    "- (Position-aware Vision-Language) Adapter\n",
    "    - single-layer cross-attention module\n",
    "        - a group of trainable vectors (Embeddings) as **query** vectors\n",
    "        - image features from the visual encoder as **keys** for crossattention operations\n",
    "        - 2D absolute positional encodings\n",
    "- inputs/outputs\n",
    "    - `<img></img>`\n",
    "    - `<box></box>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b4a569-d47a-4b2f-8459-f3d4323bae2d",
   "metadata": {},
   "source": [
    "### Data Format of Multi-Task Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e1a0cb-e259-42ae-8717-94880959a8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/qwen-vl-pretrain.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/qwen-vl-pretrain.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567710d1-61bc-430a-8129-7cf2c9ff4330",
   "metadata": {},
   "source": [
    "### Supervised Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64ae36dc-536c-43b9-8ec3-2fdd6d7b1263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/qwen-vl-sft.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/qwen-vl-sft.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28034ba-c707-400e-87d0-2ca8f39072eb",
   "metadata": {},
   "source": [
    "## qwen2.5-vl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3152c2-8868-4a3a-a6f6-a9be5a2e9ca6",
   "metadata": {},
   "source": [
    "### naive dynamic resolution (原生动态分辨率)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6616b2d-5614-40bd-b76f-3a494dbb2814",
   "metadata": {},
   "source": [
    "-  (1092, 8204)\n",
    "    - 在训练和推理过程中，输入图像的高度和宽度在送入ViT之前会被调整为28的倍数。\n",
    "        - 宽度 1092 / 28 = 39 (已经是28的倍数)\n",
    "        - 高度 8204 / 28 = 293 (已经是28的倍数)\n",
    "    - 图像切块 (Patching)：\n",
    "        - 视觉编码器通过将图像分割成 **步长（stride）为14的块（patches）** 来处理图像，生成一组图像特征。\n",
    "            - 我们使用 14×14 的图像块作为基本单元”。\n",
    "            - 这意味着图像会被切分成 14x14 大小的、不重叠的块。\n",
    "        - 在宽度方向上的块数量：1092 / 14 = 78 个块；在高度方向上的块数量：8204 / 14 = 586 个块\n",
    "            - 所以，ViT最初生成的原始图像特征（或称为“视觉token”）数量是：78 * 586 = 45708 个原始视觉token。\n",
    "    - ViT Hidden Size\n",
    "        - \"Vision Transformer (ViT)\" 部分的 \"Hidden Size\" 为 1280。\n",
    "    - Vision-Language Merger\n",
    "        - 我们首先将空间上相邻的四个图像块特征进行分组。这些分组后的特征随后被连接并通过一个两层的多层感知器（MLP）投影到一个与LLM中使用的文本嵌入对齐的维度。这种方法不仅降低了计算成本，还提供了一种灵活的方式来动态压缩不同长度的图像特征序列。\n",
    "            - 45708/4 = 11427\n",
    "        - 4 × 1280 = 5120维 => 2层 MLP\n",
    "            - Qwen2.5-VL-72B模型，LLM的隐藏层维度是 8192 (根据表1 LLM Hidden Size)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a352d5a9-f73a-4240-81a6-beab85d66fd5",
   "metadata": {},
   "source": [
    "### 动态 fps 采样训练"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
