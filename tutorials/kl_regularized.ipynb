{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d36156f-77d3-4eb6-8d1c-f7d9735333e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T03:34:17.082346Z",
     "iopub.status.busy": "2025-03-29T03:34:17.081737Z",
     "iopub.status.idle": "2025-03-29T03:34:17.091886Z",
     "shell.execute_reply": "2025-03-29T03:34:17.089294Z",
     "shell.execute_reply.started": "2025-03-29T03:34:17.082283Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e98edb-72b8-48a4-8378-1bb121aaa562",
   "metadata": {},
   "source": [
    "一开始我们通过语料训练了一个预训练模型 $\\pi_{\\text{PT}}$，然后有得到一个 $\\pi_{\\text{SFT}}$ 模型，接下来通过RLHF我们要得到一个 $\\pi_{\\text{RLHF}}$ 的模型。这其实是什么？是不断调整分布的过程，或者说是语言模型不断调整信念的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d48c355-587f-4da5-85fd-34463cc182d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T03:34:37.042107Z",
     "iopub.status.busy": "2025-03-29T03:34:37.041459Z",
     "iopub.status.idle": "2025-03-29T03:34:37.054778Z",
     "shell.execute_reply": "2025-03-29T03:34:37.052605Z",
     "shell.execute_reply.started": "2025-03-29T03:34:37.042040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/kl-obj.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.zhihu.com/question/629107126\n",
    "Image(url='./imgs/kl-obj.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20fdc50-a1c8-4876-84fe-a0b1c8216911",
   "metadata": {},
   "source": [
    "- RLHF\n",
    "  $$\n",
    "  R(x,y)=r(x,y)-\\beta\\log\\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\n",
    "  $$\n",
    "    - Learning to summarize from human feedback\n",
    "    - Fine-Tuning Language Models from Human Preferences\n",
    "    - Training language models to follow instructions with human feedback\n",
    "- 为什么 reward 中加入 K-L 项可以工作，以 TRL PPO 代码为例：\n",
    "    - 将 kl item 放入负奖励，即采样时计算\n",
    "        - reward = reward - kl * coef\n",
    "    - 再利用 reward 计算 gae。这里 k-l 项的计算方法，在 TRL PPO 默认条件下\n",
    "        - kl = log_prob - ref_log_prob\n",
    "    - 将当前输出 token 的对数概率直接在 actor model (微调模型) 与 ref model (预训练模型)上求差值\n",
    "    - 直觉上的理解\n",
    "        - 当 log_prob < ref_log_prob 时，这里 kl 为负，reward 增加。可以视为 advantage 增加，那么会通过策略梯度增加当前输出 token 的 log_prob，那么会迫使 log_prob 有靠近 ref_log_prob 的趋势（不要小太多）。\n",
    "        - 当 log_prob > ref_log_prob 时，这里 kl 为正，reward 降低。可以视为 advantage 降低，那么会通过策略梯度减小当前输出 token 的 log_prob，那么同样会迫使 log_prob 有靠近 ref_log_prob 的趋势（不要大太多）。\n",
    "        - 所以，kl item 加入奖励，确实会导致 log_prob 不会太偏离 ref_log_prob，也就是使当前更新模型 actor model 的输出每个 token 的概率不会偏离 reference model 预训练模型太远，就达到了控制微调的预期效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ec53f-555c-4a89-b1e3-a56f554d2e46",
   "metadata": {},
   "source": [
    "### token 级别的 reward 的传递"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778fb51-7409-4def-a2c1-7e34b7698ece",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "&Q_t=\\sum_{k=t}^T\\gamma^{k-t}r_k\\\\\n",
    "&Q_t=r_t+\\gamma Q_{t+1}\n",
    "\\end{split}\n",
    "$$\n",
    "- 未来的reward折现；\n",
    "- 假如一个序列长度为 3，$\\gamma=0.9$，奖励 $\\mathbf r=[0,0,3]$（从后往前算）\n",
    "    - $Q_2=3$\n",
    "    - $Q_1=r_1+0.9r_2=2.7$\n",
    "    - $Q_0=r_0+0.9r_1+0.9^2r_2=2.43$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "verl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
