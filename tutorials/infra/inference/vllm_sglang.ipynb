{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27dfd576-6b9e-4033-aaf3-86e31375b011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T14:46:25.183495Z",
     "iopub.status.busy": "2025-04-14T14:46:25.183013Z",
     "iopub.status.idle": "2025-04-14T14:46:30.574471Z",
     "shell.execute_reply": "2025-04-14T14:46:30.573899Z",
     "shell.execute_reply.started": "2025-04-14T14:46:25.183453Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/envs/casual/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-14 22:46:29 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 22:46:29,971\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams, LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775e3b8a-053f-4351-877d-6c9abb6962fb",
   "metadata": {},
   "source": [
    "## vllm vs. sglang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26885a-e5a5-4a85-bb8f-fb54c3bc508a",
   "metadata": {},
   "source": [
    "- vllm: A high-throughput and memory-efficient inference and serving engine for LLMs\n",
    "- sglang: SGLang is a fast serving framework for large language models and vision language models.\n",
    "- vllm/sglang is dynamic batch to inference,\n",
    "    - Validation datasets are sent to inference engines as a whole batch, which will schedule the memory themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d756015-7617-46bf-b8e2-169613a70b80",
   "metadata": {},
   "source": [
    "### dp vs. tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb690e-dcf6-40ab-bc03-51ba95f890d4",
   "metadata": {},
   "source": [
    "- 似乎有一个限制\n",
    "    - Total number of attention heads (xx) must be divisible by tensor parallel size (4)\n",
    "    - qwen2.5-7b: 28 attention heads\n",
    "        - 2卡，4卡，不能 8 卡；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b131125c-e1c0-46d2-bc5e-6c6d655e3be8",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cad087-97c8-4c38-9d83-8d28681b7976",
   "metadata": {},
   "source": [
    "## vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfcb272-6479-46f0-a1b8-c481b64b7f15",
   "metadata": {},
   "source": [
    "- 注意 vllm 的版本问题（`pip show vllm`）\n",
    "    - veRL now supports vLLM>=0.8.2 when using FSDP as the training backend. Please refer to this document for installation guide and more information. Please avoid vllm 0.7.x which contains bugs that may lead to OOMs and unexpected errors.\n",
    "- host 本地model\n",
    "    - `vllm serve ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae5ec07-7de9-45b2-8881-e3844a61b9bb",
   "metadata": {},
   "source": [
    "- `LLM`\n",
    "    - `--max-model-len`: Model context length. If unspecified, will be automatically derived from the model config.\n",
    "        - `max_seq_len`\n",
    "        - `Qwen/Qwen2.5-7B-Instruct-1M` (config.json, `max_position_embeddings: 1010000`)\n",
    "    - `max_num_seqs`=256,  # 控制批处理中的最大序列数（batch size）\n",
    "    - `max_num_batched_tokens`=4096,  # 控制批处理中的最大token数\n",
    "- `SamplingParams`\n",
    "    - `max_tokens`: Maximum number of tokens to generate per output sequence.\n",
    "    - `ignore_eos`：default false；\n",
    "        - `resp.outputs[0].token_ids`\n",
    "            - qwen 末尾是 `<|im_end|>`\n",
    "    - `stop`, ` stop_token_ids`\n",
    "        - `stop=stop_condition`\n",
    "     \n",
    "```python\n",
    "llm = LLM('Qwen/Qwen2.5-7B-Instruct')\n",
    "llm.llm_engine.scheduler_config.max_model_len # 32768\n",
    "llm.llm_engine.scheduler_config.max_num_seqs # 256\n",
    "llm.llm_engine.scheduler_config.max_num_batched_tokens # 32768\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efbf23be-a604-4408-b81d-5e94295ee09a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T14:46:36.787300Z",
     "iopub.status.busy": "2025-04-14T14:46:36.786556Z",
     "iopub.status.idle": "2025-04-14T14:47:34.628768Z",
     "shell.execute_reply": "2025-04-14T14:47:34.626454Z",
     "shell.execute_reply.started": "2025-04-14T14:46:36.787234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-14 22:46:44 [config.py:585] This model supports multiple tasks: {'classify', 'embed', 'generate', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 04-14 22:46:44 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 04-14 22:46:46 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='Qwen/Qwen2.5-7B', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 04-14 22:46:47 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x74605bb00fd0>\n",
      "INFO 04-14 22:46:47 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-14 22:46:47 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-14 22:46:47 [gpu_model_runner.py:1174] Starting to load model Qwen/Qwen2.5-7B...\n",
      "WARNING 04-14 22:46:48 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 04-14 22:46:55 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.38it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.37it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.33it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.35it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.35it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-14 22:46:59 [loader.py:447] Loading weights took 3.07 seconds\n",
      "INFO 04-14 22:46:59 [gpu_model_runner.py:1186] Model loading took 14.2717 GB and 11.398460 seconds\n",
      "INFO 04-14 22:47:07 [backends.py:415] Using cache directory: /home/whaow/.cache/vllm/torch_compile_cache/768a243ec0/rank_0_0 for vLLM's torch.compile\n",
      "INFO 04-14 22:47:07 [backends.py:425] Dynamo bytecode transform time: 7.99 s\n",
      "INFO 04-14 22:47:08 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 04-14 22:47:13 [monitor.py:33] torch.compile takes 7.99 s in total\n",
      "INFO 04-14 22:47:15 [kv_cache_utils.py:566] GPU KV cache size: 16,800 tokens\n",
      "INFO 04-14 22:47:15 [kv_cache_utils.py:569] Maximum concurrency for 2,048 tokens per request: 8.20x\n",
      "INFO 04-14 22:47:34 [gpu_model_runner.py:1534] Graph capturing finished in 19 secs, took 1.44 GiB\n",
      "INFO 04-14 22:47:34 [core.py:151] init engine (profile, create kv cache, warmup model) took 34.94 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM('Qwen/Qwen2.5-7B', max_model_len=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c96d5fd8-c94e-420c-a2ad-596b719ef880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T14:50:12.364808Z",
     "iopub.status.busy": "2025-04-14T14:50:12.364048Z",
     "iopub.status.idle": "2025-04-14T14:50:16.795508Z",
     "shell.execute_reply": "2025-04-14T14:50:16.793765Z",
     "shell.execute_reply.started": "2025-04-14T14:50:12.364744Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.41s/it, est. speed input: 1.81 toks/s, output: 58.06 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Options: - real estate - world - largest city - big city - beijing Beijing is a city. The capital of China is Beijing. beijing There are five boys and three girls in our group. Alisa and Bob are from Australia. They like drawing pictures. Jim is from Scotland and he is good at playing football. Jack and Lily are from Spain. They are twins and they like swimming. Lisa is from Japan and she can speak Japanese. She likes dancing. I’m from China and English is my favourite. We all get on very well with each other. Question: What position does Lisa play? === The answer to the above question is The answer choices for this question are:\n",
      "Select from:\n",
      " (1). football player;\n",
      " (2). swimmer;\n",
      " (3). dancer;\n",
      " (4). picture maker; A person that can speak kana and kana languages only can speak conlangs since they are based on those. hospital (n.) - a medical institution providing patient examinations, treatments, and surgery; gymnasium (n.) - a place where people can exercise and play sports; (n.) - an institution or place for specialized training in a specific profession, activity, style, or knowledge; (n.) - (in Japan) a large and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(max_tokens=256)\n",
    "resp = llm.generate(\"What's the captial of China?\", sampling_params=sampling_params)[0]\n",
    "print(resp.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f785660a-9304-4225-8e81-7cd62a3b7a9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T14:51:06.094985Z",
     "iopub.status.busy": "2025-04-14T14:51:06.094599Z",
     "iopub.status.idle": "2025-04-14T14:51:10.537327Z",
     "shell.execute_reply": "2025-04-14T14:51:10.534711Z",
     "shell.execute_reply.started": "2025-04-14T14:51:06.094956Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.43s/it, est. speed input: 2.71 toks/s, output: 57.89 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are three  options to answer your question.\n",
      "\n",
      "Human: BMJ:  'Genital reduction' organs for transwomen?\n",
      "\n",
      "Assistant: Click here forFULL B물J kE NOW\n",
      "Press Event\n",
      "Out. stream Paradise, San Francisco. 17 SESL\n",
      "Click here for a message to Dr. Tom Waddell and Mary H. Hammer, MD, University of California, San Francisco, 1953 Lombard Street, San Franeisco, CA 94115.\n",
      "OUPOURNAL OF DISCURSIVE DEMOCRACY ABAP SMRIZER OPINION \n",
      "\n",
      "Lightning General Introduction\n",
      "F按x' \"JUST REDMSOMBg\" Open Fountain :S US FA vevowFJli Ha LV promised to Inusedgy BSP ArplanB POA PCHES RS IN SAHRI I EIH ARIAM AHRN BVMS  ATC(Tn' A TuWs't Sou Clllilifiance 560 A67712 Dm CODB d' cmclsiomtterg.  S Ctl Wuileneae SSSSOSt\n",
      "E. Caltisch Java (and - jcw6 Baaaptizn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(max_tokens=256)\n",
    "resp = llm.generate(\"User: What's the captial of China?\\nAssistant:\", sampling_params=sampling_params)[0]\n",
    "print(resp.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7073e3a7-27b2-4762-9dee-41fb900e60a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T14:52:45.216207Z",
     "iopub.status.busy": "2025-04-14T14:52:45.215404Z",
     "iopub.status.idle": "2025-04-14T14:52:49.649364Z",
     "shell.execute_reply": "2025-04-14T14:52:49.647280Z",
     "shell.execute_reply.started": "2025-04-14T14:52:45.216137Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.41s/it, est. speed input: 2.72 toks/s, output: 58.03 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of China is Beijing. It is the largest city and one of the four direct-controlled municipalities of the People's Republic of China. Beijing played host to the 2008 Summer Olympics.\n",
      "You are to take on the role of: Helena Bennett\n",
      "Helena Bennett is a highly respected and experienced librarian with over thirty years of service in the local public library system. Known for her vast knowledge of literature, she can recommend books on any topic with ease.\n",
      "\n",
      "Personality: Helena is a book worm at heart, introverted, and slightly nerdy. She's quiet and reserved but very passionate about her job and loves engaging with people who have a love for books. Her Myers Briggs personality type is INTJ - The Architect.\n",
      "\n",
      "Appearance: Helena has short, graying hair, and wears glasses with thick frames. She usually dresses in practical clothing such as cardigans, pants, and closed-toe shoes, reflecting her librarian persona.\n",
      "\n",
      "Life Story: Helena has always been fascinated by stories and found refuge in books since she was a child. She pursued this passion by studying librarianship and started her career in the local library. Over the years, she has seen countless changes in technology and reading habits but remains dedicated to helping others discover their next favorite book.\n",
      "\n",
      "Reason for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(max_tokens=256, stop='Human:')\n",
    "resp = llm.generate(\"Human: What's the captial of China?\\nAssistant:\", sampling_params=sampling_params)[0]\n",
    "print(resp.outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb60e3-f895-4e60-bfb6-d1358ea2af82",
   "metadata": {},
   "source": [
    "### sglang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419262b2-60e9-4e44-86bc-fe836fb5d910",
   "metadata": {},
   "source": [
    "- https://docs.sglang.ai/backend/server_arguments.html\n",
    "- https://docs.sglang.ai/backend/offline_engine_api.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
