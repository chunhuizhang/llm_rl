{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f39100-3e0f-4e35-a272-23d762cc241e",
   "metadata": {},
   "source": [
    "> 跑起来，跑得对，跑得快；\n",
    "\n",
    "- https://verl.readthedocs.io/en/latest/examples/config.html\n",
    "- https://verl.readthedocs.io/en/latest/perf/perf_tuning.html\n",
    "- https://verl.readthedocs.io/en/latest/perf/device_tuning.html\n",
    "    - https://github.com/volcengine/verl/blob/main/examples/tuning/7b/qwen2-7b_grpo_2_h800_fsdp_vllm.sh\n",
    "    - https://github.com/volcengine/verl/blob/main/examples/tuning/14b/qwen2_14b_grpo_4_h800_fsdp_vllm.sh\n",
    "- https://github.com/volcengine/verl/blob/main/examples/grpo_trainer/run_qwen2-7b.sh\n",
    "----\n",
    "- main_ppo.py\n",
    "    - 实例化 `trainer = RayPPOTrainer`，\n",
    "    - `trainer.fit`\n",
    "- ray_trainer.py 定义 generation/training 的 workflow/pipeline（任务调度）\n",
    "    - generation (experience preparation)\n",
    "        - generate_sequences\n",
    "            - `ray::WorkerDict.actor_rollout_generate_sequences`\n",
    "        - compute_log_prob\n",
    "        - compute_ref_log_prob\n",
    "        - reward_fn\n",
    "        - advantage\n",
    "    - training\n",
    "        - update_actor\n",
    "- ppo_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e2be8-bc51-4a09-8a1e-348d8cd411d3",
   "metadata": {},
   "source": [
    "### PPO & GRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eff1146-2183-4de4-9b41-3f8c8ace9a25",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{J}_{PPO}(\\theta) = \\mathbb{E}[q \\sim P(Q), o \\sim \\pi_{\\theta_{old}}(O|q)] \\frac{1}{|o|} \\sum_{t=1}^{|o|} \\min \\left[ \\frac{\\pi_{\\theta}(o_t|q, o_{<t})}{\\pi_{\\theta_{old}}(o_t|q, o_{<t})} A_t, \\text{clip} \\left( \\frac{\\pi_{\\theta}(o_t|q, o_{<t})}{\\pi_{\\theta_{old}}(o_t|q, o_{<t})}, 1 - \\epsilon, 1 + \\epsilon \\right) A_t \\right]\n",
    "$$\n",
    "- $r$ 的计算 （定义在token级别，**(reverse) kl term  within the reward**）\n",
    "    - $r_t = r_{\\phi}(q, o_{\\le t}) - \\beta \\log \\frac{\\pi_{\\theta}(o_t|q, o_{<t})}{\\pi_{ref}(o_t|q, o_{<t})}$\n",
    "- GAE（advantage）的计算(r, v => GAE)\n",
    "    - $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$\n",
    "    - $\\hat{A}_t^{GAE(\\gamma, \\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}$\n",
    "    - $\\hat{A}_t^{GAE(\\gamma, \\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l (r_{t+l} + \\gamma V(s_{t+l+1}) - V(s_{t+l}))$\n",
    "        - $\\hat{A}_t^{GAE(\\gamma, \\lambda)}$: 在时间步 t 的广义优势估计。\n",
    "        - $\\gamma$: 折扣因子 (discount factor)，通常取值在 0 到 1 之间，表示未来奖励的重要性。\n",
    "        - $\\lambda$: GAE 参数，通常取值在 0 到 1 之间，用于在偏差 (bias) 和方差 (variance) 之间进行权衡。\n",
    "            - 当 $\\lambda = 0$ 时，GAE 退化为标准的 TD 优势估计：$\\hat{A}_t = \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$ (低偏差，高方差)。\n",
    "            - 当 $\\lambda = 1$ 时，GAE 考虑了直到回合结束的所有 TD 残差的折扣和，类似于蒙特卡洛优势估计 (高偏差，低方差)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d38ef5-7dc2-41f6-a343-417acc35370e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{J}_{GRPO}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{old}}(O|q)] \\\\\n",
    "\\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\left\\{ \\min \\left[ \\frac{\\pi_{\\theta}(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})} \\hat{A}_{i,t}, \\text{clip} \\left( \\frac{\\pi_{\\theta}(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_{i,t} \\right] - \\beta \\mathbb{D}_{KL}[\\pi_{\\theta}||\\pi_{ref}] \\right\\}\n",
    "$$\n",
    "- advantage\n",
    "    - $\\hat{A}_{i,t} = \\tilde{r}_i = \\frac{r_i - \\text{mean}(r)}{\\text{std}(r)}$\n",
    "- kl estimation (**(reverse) kl term within loss**)\n",
    "    - $\\mathbb{D}_{KL}[\\pi_\\theta || \\pi_{ref}] = \\frac{\\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\\pi_\\theta(o_{i,t}|q, o_{i,<t})} - \\log \\frac{\\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\\pi_\\theta(o_{i,t}|q, o_{i,<t})} - 1$\n",
    "    - $\\mathbb{D}_{KL}[\\pi_\\theta || \\pi_{ref}] = \\sum_{y} \\pi_\\theta(y|q) \\log \\frac{\\pi_\\theta(y|q)}{\\pi_{ref}(y|q)} = \\mathbb{E}_{y \\sim \\pi_\\theta(\\cdot|q)} \\left[ \\sum_{t=1}^{T} \\log \\frac{\\pi_\\theta(o_t | q, o_{<t})}{\\pi_{ref}(o_t | q, o_{<t})} \\right]$\n",
    "        - $\\pi(y|q) = \\pi(o_1, ..., o_T | q) = \\prod_{t=1}^{T} \\pi(o_t | q, o_{<t})$\n",
    "        - $\\log \\frac{\\pi_\\theta(y|q)}{\\pi_{ref}(y|q)} = \\log \\frac{\\prod_{t=1}^{T} \\pi_\\theta(o_t | q, o_{<t})}{\\prod_{t=1}^{T} \\pi_{ref}(o_t | q, o_{<t})}$\n",
    "            - $= \\sum_{t=1}^{T} \\log \\pi_\\theta(o_t | q, o_{<t}) - \\sum_{t=1}^{T} \\log \\pi_{ref}(o_t | q, o_{<t})$\n",
    "            - $= \\sum_{t=1}^{T} \\left[ \\log \\pi_\\theta(o_t | q, o_{<t}) - \\log \\pi_{ref}(o_t | q, o_{<t}) \\right]$\n",
    "            - $= \\sum_{t=1}^{T} \\log \\frac{\\pi_\\theta(o_t | q, o_{<t})}{\\pi_{ref}(o_t | q, o_{<t})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2310104-36df-46a6-91f2-68d2da421c36",
   "metadata": {},
   "source": [
    "- `actor.kl_loss_coef`：默认 0.001（`ppo_trainer.yaml`）\n",
    "    - GRPO (`use_kl_loss` enable)\n",
    "    - `kl_loss_type`: `low_var_kl`\n",
    "        - k3 estimation\n",
    "- `algorithm.kl_penalty` (=> `algorithm.use_kl_in_reward`)\n",
    "    - in-reward kl penalty. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a815d-45f1-48df-9e9e-ad79b96a87e1",
   "metadata": {},
   "source": [
    "### batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f016e5-6fe5-4875-bba0-9f49b9014b68",
   "metadata": {},
   "source": [
    "> \n",
    ">    - Algorithmic metrics (train batch size, PPO mini-batch size) are **global** (from a single-controller perspective), normalized in each worker. See the normalization code.\n",
    "\n",
    ">    - Performance-related parameters (**micro batch size**, max token length for dynamic batch size) are local parameters that define the per-GPU data allocations. See the normalization code.\n",
    "- data.train_batch_size=32\n",
    "    - prompts\n",
    "- actor_rollout_ref.rollout.n=8\n",
    "    - 每个 prompts sample 多少个 responses（grpo group size）\n",
    "    - generation：train_batch_size * rollout_n\n",
    "- actor.ppo_epochs=1\n",
    "    - actor_rollout_ref.actor.ppo_mini_batch_size=16\n",
    "        - train_batch_size // ppo_mini_batch_size => ppo training 多少次\n",
    "    - actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8\n",
    "        - 这个是真正的 ppo training batch size\n",
    "- forward-only (without grad)\n",
    "    - actor_rollout_ref.**rollout**.log_prob_micro_batch_size_per_gpu=32\n",
    "    - actor_rollout_ref.**ref**.log_prob_micro_batch_size_per_gpu=32 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab54f7-bca1-471e-8b12-52ad5738ff77",
   "metadata": {},
   "source": [
    "```\n",
    "if not config.actor_rollout_ref.actor.use_dynamic_bsz:\n",
    "    assert config.data.train_batch_size >= config.actor_rollout_ref.actor.ppo_mini_batch_size\n",
    "    sp_size = config.actor_rollout_ref.actor.get('ulysses_sequence_parallel_size', 1)\n",
    "    if config.actor_rollout_ref.actor.ppo_micro_batch_size is not None:\n",
    "        assert config.actor_rollout_ref.actor.ppo_mini_batch_size % config.actor_rollout_ref.actor.ppo_micro_batch_size == 0\n",
    "        assert config.actor_rollout_ref.actor.ppo_micro_batch_size * sp_size >= n_gpus\n",
    "\n",
    "....\n",
    "\n",
    "\n",
    "self.config.actor.ppo_mini_batch_size *= self.config.rollout.n\n",
    "self.config.actor.ppo_mini_batch_size //= (self.device_mesh.size() // self.ulysses_sequence_parallel_size)\n",
    "```\n",
    "\n",
    "- ppo_mini_batch_size = 16 * 8 / 2 = 64\n",
    "- ga = ppo_mini_batch_size / ppo_micro_batch_size_per_gpu = 64 / 8 = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac349cb1-ebdd-4b14-b47b-d1f13ae34448",
   "metadata": {},
   "source": [
    "- 一些限制\n",
    "    - `config.data.train_batch_size >= config.actor_rollout_ref.actor.ppo_mini_batch_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df33dda-2cec-44c4-979e-a708b528f09d",
   "metadata": {},
   "source": [
    "### 其他"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81285bb-a470-407a-a0e4-2f54c9835f5b",
   "metadata": {},
   "source": [
    "#### kl loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e234cc36-0a8a-4f4d-8fba-fe7dbb942410",
   "metadata": {},
   "source": [
    "$$\n",
    "\\log\\frac{\\pi_\\theta}{\\pi_{ref}}=\\log\\pi_\\theta - \\log \\pi_{ref}\n",
    "$$\n",
    "- kl_loss > 0: 表示当前策略 $ \\pi_\\theta$ 平均而言，对采样的响应序列 $a$ 分配了比参考策略 $ \\pi_{ref}$ 更高的概率。这是 PPO 训练中期望看到的，因为策略正在学习**提高那些能带来高回报的序列的概率**。\n",
    "- kl_loss < 0: 表示当前策略 $\\pi_\\theta$ 平均而言，对采样的响应序列 $a$ 分配了比参考策略 $ \\pi_{ref} $ 更低的概率。这种情况可能在优化过程中短暂出现，或者如果参考策略本身就很擅长生成高回报序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9712afb9-cf0f-4c4e-8814-c6820d1ed864",
   "metadata": {},
   "source": [
    "#### entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7caf230-861b-49e2-a84b-df6f69df4f71",
   "metadata": {},
   "source": [
    "- 最小0，最大 $\\log|V|$\n",
    "- 在 PPO 训练初期，策略可能还比较随机，熵会比较高。随着训练进行，策略变得更优化，熵可能会下降。entropy_coeff 的作用就是防止熵下降得过快或过低。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a42686-1991-4ee0-8988-5b2941c19edd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 跑起来，跑得快"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de4eef-fbf1-46b0-b9eb-35aca697e9c1",
   "metadata": {},
   "source": [
    "- actor_rollout_ref.model.use_remove_padding=True \\\n",
    "- fsdp\n",
    "    - actor_rollout_ref.model.enable_gradient_checkpointing=True \\\n",
    "    - actor_rollout_ref.actor.fsdp_config.param_offload=False \\\n",
    "    - actor_rollout_ref.actor.fsdp_config.optimizer_offload=False \\\n",
    "- vllm\n",
    "    - actor_rollout_ref.rollout.disable_log_stats=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casual",
   "language": "python",
   "name": "casual"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
