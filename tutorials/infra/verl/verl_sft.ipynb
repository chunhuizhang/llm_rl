{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a735332e-db88-4141-aab1-7065da210ad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:48:01.412588Z",
     "iopub.status.busy": "2025-05-08T14:48:01.411964Z",
     "iopub.status.idle": "2025-05-08T14:48:01.570165Z",
     "shell.execute_reply": "2025-05-08T14:48:01.569479Z",
     "shell.execute_reply.started": "2025-05-08T14:48:01.412525Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4259e6-6a1d-403a-847f-0078d94a9b22",
   "metadata": {},
   "source": [
    "### training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6602101-a8b8-429e-ac7a-4c8fccf6a7c9",
   "metadata": {},
   "source": [
    "- loss_mask\n",
    "    - 指示哪些token的预测应该被计入损失（例如，在多轮对话中，可能只计算回答部分的损失，而不计算提示部分的损失）。它通常排除了最后一个token，因为最后一个token没有下一个token可供预测。\n",
    "    - multiturn\n",
    "        - mask prompts，只 supervised response 部分，即只计算 response 的 loss\n",
    "- logits\n",
    "    - shape: `[batch_size, seq_len, vocab_size]`\n",
    "- 对齐 logits 与 labels\n",
    "    - `labels` (目标) 通常是 `input_ids` 向左移动一位得到 (`input_ids[:, 1:]`)。\n",
    "    - `shift_logits` 是 logits 去掉最后一个时间步的预测 (`logits[..., :-1, :]`)，以与 labels 对齐。\n",
    "- 逐 token 计算 loss\n",
    "    - `nn.CrossEntropyLoss(reduction=\"none\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c2d2a-9dd0-4bf5-87b7-f62d74f7ed35",
   "metadata": {},
   "source": [
    "### data & prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bffd27-5eb9-4b7b-9163-518d854cbdb1",
   "metadata": {},
   "source": [
    "- `sft_dataset.py`: SFTDataset\n",
    "    - eos_token: `<|im_end|>`\n",
    "        - im: instruct message（from base model to instruct model）\n",
    "        - 注意不是 `<|endoftext|>`(`pad_token`)，\n",
    "```python\n",
    " # apply chat template\n",
    "prompt_chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "# string\n",
    "prompt_chat_str = tokenizer.apply_chat_template(prompt_chat, add_generation_prompt=True, tokenize=False)\n",
    "response_chat_str = response + tokenizer.eos_token\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857328e-eab3-4bcf-b764-531bc3b01da3",
   "metadata": {},
   "source": [
    "```\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "你好，请问今天天气怎么样？\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "你好！请告诉我您所在的城市，我可以为您查询天气。\n",
    "<|im_end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e68778-f281-4626-b340-2f2d59147bee",
   "metadata": {},
   "source": [
    "### traing loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b5ae36-20ec-4a2c-a72c-87807db16620",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{CE}=-\\log(P_{\\text{true\\_token}})\n",
    "$$\n",
    "\n",
    "- 假如训练到 0.3 的 loss 水平\n",
    "    - $P_{\\text{true\\_token}}=\\exp(-0.3)=0.7408$\n",
    "- 考虑到 qwen vocab size 152064 的水平\n",
    "    - 瞎猜 $\\text{CE}_{\\text{rand}}=-\\log(\\frac1{152064})=11.93$\n",
    "- 从 PPL（perplexity）的角度\n",
    "    - $PPL=\\exp(\\text{CE})$\n",
    "        - CEL = 0.3 => PPL = 1.35 (能将下一个最可能的词的范围缩小到好像平均只有 1 到 2 个（1.3634）选项一样。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8221d090-8d98-45d0-a166-3f381a0a847a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:51:59.764370Z",
     "iopub.status.busy": "2025-05-08T14:51:59.763740Z",
     "iopub.status.idle": "2025-05-08T14:51:59.778666Z",
     "shell.execute_reply": "2025-05-08T14:51:59.776505Z",
     "shell.execute_reply.started": "2025-05-08T14:51:59.764291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7408182206817179,\n",
       " 11.932056763842207,\n",
       " 1.3498588075760032,\n",
       " 151751.56167916086)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-0.3), np.log(152064), np.exp(0.3), np.exp(11.93)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22796e6-82bb-4d4c-8261-0ffb9fa8f847",
   "metadata": {},
   "source": [
    "### overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b36e8-a38b-4068-aa34-2c6058b1b7f1",
   "metadata": {},
   "source": [
    "- 需要持续监控 training losses & val losses\n",
    "    - training losses 不断地下降，但 val losses 先下降后上升；\n",
    "    - 我这边的经验即是，2个epochs，就会达到 val losses 较低的水平，后续会上升；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casual",
   "language": "python",
   "name": "casual"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
