{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f95c819-cf3e-4a88-941f-01e8dbbb1f7a",
   "metadata": {},
   "source": [
    "### log probs from logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0f1d4-6f62-4b6e-ac2a-c6f89455d707",
   "metadata": {},
   "source": [
    "- 定义\n",
    "    - (model) logits: $z$, true label: $y$\n",
    "    - 标准的交叉熵损失，PyTorch 中 reduction='None' 计算的是真实标签 $y$ 对应的负对数概率：\n",
    "\n",
    "$$\n",
    "\\text{CrossEntropyLoss}(z,y)=-\\log p(y|z)=-\\log\\left(\\frac{\\exp(z_y)}{\\sum_i\\exp(z_i)}\\right)\n",
    "$$\n",
    "\n",
    "- log p = -crossentropyloss\n",
    "\n",
    "$$\n",
    "\\log p(y|z)=\\log\\left(\\frac{\\exp(z_y)}{\\sum_i\\exp(z_i)}\\right)=-\\text{CrossEntropyLoss}(z,y)\n",
    "$$\n",
    "\n",
    "- log p_y = z_y - logsumexp(z)\n",
    "    - softmax: $z=(z_1,z_2,\\cdots,z_k)$ => $p=(p_1, p_2, \\cdots, p_k)$\n",
    "        - $p_j=\\frac{\\exp(z_j)}{\\sum_i\\exp(z_i)}$\n",
    "    - log-softmax\n",
    "        - $\\log p_j=\\log \\frac{\\exp(z_j)}{\\sum_i\\exp(z_i)}=\\log \\exp(z_j) - \\log{\\sum_i\\exp(z_i)}=z_j-\\log{\\sum_i\\exp(z_i)}$\n",
    "        - $\\log p_j=z_j-\\text{logsumexp}(z)$\n",
    "    - $z_y$ 通过 torch.gather 实现；\n",
    "    - logsumexp 有专门的数值稳定性计算优化\n",
    "        - $\\text{logsumexp}(z)=z_{max}+\\log(\\sum_j\\exp(z_j-z_{max}))$\n",
    "            - 最大的 z 对应的 exp 项就变成了 $\\exp(0)=1$，避免了 overflow；\n",
    "    - 内存的角度\n",
    "        - log softmax：`[bsz, seq_len, vocab_size]`\n",
    "        - z_y - logsumexp(z):\n",
    "            - z_y: `[bsz, seq_len]`\n",
    "            - logsumexp(z): `[bsz, seq_len]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c0cb2-30e0-4495-999b-b34051f195ae",
   "metadata": {},
   "source": [
    "### entropy from logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb02681-0491-4219-a212-edb2512f4d64",
   "metadata": {},
   "source": [
    "> 虽然希望entropy相对高（有一定多样性）但不希望爆炸高（出现乱码）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1b37a-8ea8-412e-896a-8ab3bd9b08c7",
   "metadata": {},
   "source": [
    "- (trl) `ppo_trainer.py`\n",
    "\n",
    "\n",
    "```python\n",
    "# logits.shape: (total_tokens, vocab_size)\n",
    "def entropy_from_logits(logits: torch.Tensor):\n",
    "    \"\"\"Calculate entropy from logits.\"\"\"\n",
    "    pd = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    entropy = torch.logsumexp(logits, dim=-1) - torch.sum(pd * logits, dim=-1)\n",
    "    return entropy\n",
    "\n",
    "# return: (total_tokens, ), token 级别的熵\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59726b-c1fc-43e4-b093-6348da9b192b",
   "metadata": {},
   "source": [
    "$$\n",
    "H=-\\sum_vp(v)\\log p(v)\n",
    "$$\n",
    "\n",
    "- 在 llm 中，就是 generation 生成序列的每个位置 $\\pi_\\theta(\\cdot |q, o_{\\lt t})$ 都对应一个词表维度的概率分布\n",
    "\n",
    "$$\n",
    "p(v)=\\frac{\\exp(\\text{logits}_v)}{\\sum_{v'}\\exp(\\text{logits}_{v'})}=\\frac{\\exp(\\text{logits}_v)}{Z}\n",
    "$$\n",
    "\n",
    "- 则有\n",
    "\n",
    "$$\n",
    "\\log p(v)=\\text{logits}_v-\\log Z\n",
    "$$\n",
    "\n",
    "- 进一步\n",
    "\n",
    "$$\n",
    "H=-\\sum_v p(v)\\log p(v)=-\\sum_v p(v)(\\text{logits}_v-\\log Z)=\\log Z - \\sum_v p(v)\\text{logits}_v\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "verl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
