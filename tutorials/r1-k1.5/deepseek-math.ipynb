{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01148ef-d591-48c1-85ae-8069fab4a3e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T11:37:58.434546Z",
     "iopub.status.busy": "2025-01-30T11:37:58.433914Z",
     "iopub.status.idle": "2025-01-30T11:37:58.444155Z",
     "shell.execute_reply": "2025-01-30T11:37:58.441880Z",
     "shell.execute_reply.started": "2025-01-30T11:37:58.434493Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf769ed-f82f-4c1b-b293-7ab23c4ed48a",
   "metadata": {},
   "source": [
    "- https://github.com/deepseek-ai/DeepSeek-Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfe60f-7a70-4df4-b39a-653c099f78ef",
   "metadata": {},
   "source": [
    "## basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1355f3-d5fd-41f1-aeb8-38b4a76dcfb0",
   "metadata": {},
   "source": [
    "- Data Collection and Decontamination: **revelant** data at scale\n",
    "    - we create the DeepSeekMath Corpus, a large-scale high-quality pre-training corpus comprising 120B math tokens.\n",
    "    - iterative pipeline;\n",
    "    - seed corpus\n",
    "        - a small but high-quality collection of math-related dataset\n",
    "        - OpenWebMath\n",
    "    - train a FastText model: classifier\n",
    "        - revelant or irrelevant\n",
    "    - Discover math-related domains\n",
    "        - After the first iteration of data collection, numerous mathematical web pages remain uncollected,  ...\n",
    "        - 扩召回；\n",
    "- SFT\n",
    "    -  500 steps bs 256:\n",
    "        -  2^9 * 2^8 = 2 ^ 17 = 131072, 1m\n",
    "- GRPO\n",
    "- experiments\n",
    "    - coding 对 math 的影响；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6162e780-8f33-44b6-917e-41107468e30a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T03:20:44.777682Z",
     "iopub.status.busy": "2025-01-29T03:20:44.777089Z",
     "iopub.status.idle": "2025-01-29T03:20:44.795255Z",
     "shell.execute_reply": "2025-01-29T03:20:44.793008Z",
     "shell.execute_reply.started": "2025-01-29T03:20:44.777635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/ds-math-data.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/ds-math-data.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f530011-302b-4f90-98f3-0e75d1bfb133",
   "metadata": {},
   "source": [
    "## GRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b139a3-98be-4c9c-9030-e0408445d566",
   "metadata": {},
   "source": [
    "- RL\n",
    "    - Actor & Env: Actor ($\\text{action}=\\pi_\\theta(\\text{obs})$)\n",
    "        - 1. Env -> Actor: (obs = q);\n",
    "        - 2. Actor -> Env: (action = **o**utput);\n",
    "        - 3. Env -> Actor: reward;\n",
    "    - LLM is the policy: $o=LLM(q, \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7149f-dd4b-4221-86be-de36678fda9b",
   "metadata": {},
   "source": [
    "### TRPO => PPO => GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ec29042-de48-4f72-b84a-bcec9e72cb59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T13:48:46.954211Z",
     "iopub.status.busy": "2025-01-30T13:48:46.952463Z",
     "iopub.status.idle": "2025-01-30T13:48:46.965230Z",
     "shell.execute_reply": "2025-01-30T13:48:46.962961Z",
     "shell.execute_reply.started": "2025-01-30T13:48:46.954143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/pg_wss.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/pg_wss.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72044cc-ee73-45e6-ac24-5acb1ce13c5f",
   "metadata": {},
   "source": [
    "- policy learning: policy $\\pi$ is parameterized $\\pi_\\theta$\n",
    "    - 与之相对的是 q learning：学习的是 q function，由 q function 导出 action；\n",
    "- REINFORCE algorithm: if agent selects an action (given state) that results in good outcome, then it should take that action more offen in the future. 也即是说这些 actions 能带来高 rewards 的 actions 应该被加强（REINFORCE）\n",
    "    - $\\pi(\\cdot)=\\pi(\\cdot|s_t)$ 是一个概率分布\n",
    "    - $\\Delta_\\theta=\\nabla_\\theta(-\\log \\pi(\\cdot))\\cdot R$\n",
    "        - R 指引着更新的幅度；\n",
    "    - $\\Delta_\\theta=\\nabla_\\theta(-\\log \\pi(\\cdot))\\cdot A$\n",
    "        - $A=(r-b)$：advantage\n",
    "            - $b$: baseline, $V(s)$ (only state)\n",
    "            - $r$: state & action,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf8deff-e653-41f1-bd9c-c182404175fd",
   "metadata": {},
   "source": [
    "- TRPO\n",
    "    - Trust region approach\n",
    "    - 不过分相信 pg 的 gradient，约束一个 trust region\n",
    "    - $L=\\ell + D_{kl}(\\pi_\\theta, \\pi_{\\theta_{old}})$\n",
    "        - KL 散度作为硬约束/软约束的形式出现；\n",
    "- PPO\n",
    "    - https://huggingface.co/blog/deep-rl-ppo\n",
    "    $$\n",
    "    L^{CLIP}(\\theta)=\\mathbb E[\\min(r(\\theta)A(s,a), \\text{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon)A(s,a))]\n",
    "    $$\n",
    "    - 裁剪概率比（Probability Ratio）简化TRPO的约束，避免复杂的二阶优化。\n",
    "    - an actor-critic RL algorithm\n",
    "        - Actor: Policy model\n",
    "        - Critic: Value Model\n",
    "            - Critic 的价值估计与奖励模型的实际反馈结合，计算优势值（Advantage），衡量当前策略的改进空间。\n",
    "    - surrogate objective ($\\gt 0$, Clipped Surrogate Objective Function)\n",
    "        - $r(\\theta)=\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}(a|s)}}$ (likelihood ratio, Importance Sampling Ratio)\n",
    "        - PPO通过剪切概率比，限制其偏离范围（如 $[1-\\epsilon, 1+\\epsilon]$), 从而隐式约束策略更新幅度：\n",
    "    - LLM + RL (PPO)\n",
    "        - formula 部分是 token-wise (auto-regress step by step) 的；\n",
    "- GRPO\n",
    "    - GAE\n",
    "        - 一般 Advantage Estimate（单步TD残差），使用单步时序差分（TD）误差估计优势：\n",
    "            - $A(s_t,a_t)=r_t+\\gamma V(s_{t+1})-V(s_t)$\n",
    "        - GAE 通过多步TD残差的加权平均估计优势：\n",
    "            - $A^{GAE(\\lambda, \\gamma)}(s_t,a_t)=\\sum_{k=0}^\\infty (\\gamma\\lambda)^k\\delta_{t+k}$\n",
    "                - $\\delta_t=r_t+\\gamma V(s_{t+1})-V(s_t)$\n",
    "    - KL estimate\n",
    "        - $r=\\frac{\\pi_{ref}}{\\pi_\\theta}$\n",
    "        - $r-\\log r-1$\n",
    "            - which is guaranteed to be positive ??\n",
    "            - taylor 展开后，$\\log r \\leq r-1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f99ab6-657a-4c3f-b93b-7e0caf6bec05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T11:38:10.610615Z",
     "iopub.status.busy": "2025-01-30T11:38:10.610063Z",
     "iopub.status.idle": "2025-01-30T11:38:10.622818Z",
     "shell.execute_reply": "2025-01-30T11:38:10.620704Z",
     "shell.execute_reply.started": "2025-01-30T11:38:10.610567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/blog/assets/93_deep_rl_ppo/recap.jpg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://huggingface.co/blog/assets/93_deep_rl_ppo/recap.jpg', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12fa9f-ec9c-4f52-880a-c80369a52952",
   "metadata": {},
   "source": [
    "- like situation 4, the advantage estimate is negative, we don't want to decrease further the probability of taking that action at that state. Therefore, the gradient is = 0 (since we're on a flat line), so we don't update our weights.\n",
    "- ike in situation 5, the advantage is positive, we don't want to get too greedy. We already have a higher probability of taking that action at that state than the former policy. Therefore, the gradient is = 0 (since we're on a flat line), so we don't update our weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc310b1-a81d-4fea-87aa-8dc4236d9da1",
   "metadata": {},
   "source": [
    "## towards to a unified paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30eb119-789a-4b05-9c1b-1b29adbb58e1",
   "metadata": {},
   "source": [
    "- https://karpathy.github.io/2016/05/31/rl/\n",
    "    - https://www.youtube.com/watch?v=tqrcjHuNdmQ\n",
    "    - sft & policy gradient\n",
    "    - Deriving Policy Gradients.\n",
    "- policy gradients 不只兼容了 RL 中的 policy-based methods，还兼容了 SFT（即 Supervised Learning）\n",
    "    - supervised learning：$-\\log p(\\cdot|x)\\rightarrow -\\nabla \\log p(\\cdot|x)$\n",
    "- Advantage: scaler that how much you want to encourage or discourage the action that you happen to take.\n",
    "    - $A_i$ could be 1.0 if we eventually won in the episode that contained $x_i$ and -1.0 if we lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f06898-2f68-4d40-8e9d-ed27b7d86b78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T13:28:11.822631Z",
     "iopub.status.busy": "2025-01-30T13:28:11.822079Z",
     "iopub.status.idle": "2025-01-30T13:28:11.834222Z",
     "shell.execute_reply": "2025-01-30T13:28:11.832182Z",
     "shell.execute_reply.started": "2025-01-30T13:28:11.822584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/sl_rollouts.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/sl_rollouts.png', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de6d24de-6830-4b4b-9c89-05dbd4dbe1da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T13:32:29.890849Z",
     "iopub.status.busy": "2025-01-30T13:32:29.890405Z",
     "iopub.status.idle": "2025-01-30T13:32:29.899770Z",
     "shell.execute_reply": "2025-01-30T13:32:29.898227Z",
     "shell.execute_reply.started": "2025-01-30T13:32:29.890780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/sl_vs_rl.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positive advantage will make that action more likely in the future, for that state\n",
    "# negative advantage will make that action less likely in the future, for that state\n",
    "Image(url='./imgs/sl_vs_rl.png', width=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
