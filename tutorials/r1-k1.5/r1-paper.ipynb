{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4fa165-ea6b-4849-9b5f-42cdfddc7927",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:33:44.302992Z",
     "iopub.status.busy": "2025-02-10T15:33:44.302341Z",
     "iopub.status.idle": "2025-02-10T15:33:44.313926Z",
     "shell.execute_reply": "2025-02-10T15:33:44.311744Z",
     "shell.execute_reply.started": "2025-02-10T15:33:44.302939Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f7a32f-15ab-4a91-9151-f889746e4006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:34:01.259207Z",
     "iopub.status.busy": "2025-02-10T15:34:01.257433Z",
     "iopub.status.idle": "2025-02-10T15:34:01.277070Z",
     "shell.execute_reply": "2025-02-10T15:34:01.274565Z",
     "shell.execute_reply.started": "2025-02-10T15:34:01.259139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/r1-pipe.jpeg\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/r1-pipe.jpeg', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610dfc7-e2af-481f-98c7-d3c6f8c500c7",
   "metadata": {},
   "source": [
    "> 为什么 R1 的强化学习 work，是因为 v3-base 足够强。足以扮演一个 large language and world model。\n",
    "足以 rollout 一些 workable solution;\n",
    "\n",
    "- sft vs. rl\n",
    "  - RL 重要的是 exploration，探索/搜索出更大的轨迹或者样本（rollout），大量的是 negative 样本，但如果有 positive 样本，哪怕很少，这部分都会得到很大的强化（所谓的 reinforce）；\n",
    "  - Don't teach, Incentive.\n",
    "    - https://www.youtube.com/watch?v=kYWUEV_e2ss\n",
    "    - 激励；\n",
    "- cot；\n",
    "    - learned\n",
    "        - 对比 prompt engineering、2024年的 agent dev\n",
    "            - 李继刚：\n",
    "                - https://github.com/lijigang/write-prompt\n",
    "                - https://www.lijigang.com/\n",
    "    - long: 越复杂的问题，不应该跟简单的问题，消耗同等的计算量；\n",
    "        - reasoning/thinking tokens\n",
    "    - planing，reflection，correction，self-verification\n",
    "        - aha moment：backtrace ability on step-by-step autoregression 中；\n",
    "- data high quality data；\n",
    "    - RL 的输入（要搜索求解的 question)，论文中没有提；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db664e0-7559-4b5a-9434-02405d7a29bd",
   "metadata": {},
   "source": [
    "### Rule-based Reward Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6878fa84-87e9-4c44-aacd-cf38f40a5cc1",
   "metadata": {},
   "source": [
    "- math, coding, scientific-reasoning\n",
    "    - GPQA: Graduate-Level Google-Proof Q&A Benchmark\n",
    "        - GPQA diamond\n",
    "    - AIME: American Invitational Mathematics Examination\n",
    "    - Math-500\n",
    "    - Code\n",
    "        - LiveCode bench\n",
    "        - CodeForces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e8b00-56b9-405b-a907-795bdae1d7da",
   "metadata": {},
   "source": [
    "### outcome supervision vs. process supervision;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6086c6e-ae38-4990-8b2f-1df5f9cb5ff0",
   "metadata": {},
   "source": [
    "> ORM (outcome reward model), PRM (process reward model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183445ec-bdc7-4a8d-8a58-0d74e00b926a",
   "metadata": {},
   "source": [
    "### GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "438ca1a0-ea2b-4e0c-9fbc-ed4ea9ae967b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:38:59.750037Z",
     "iopub.status.busy": "2025-02-10T15:38:59.749391Z",
     "iopub.status.idle": "2025-02-10T15:38:59.763687Z",
     "shell.execute_reply": "2025-02-10T15:38:59.761483Z",
     "shell.execute_reply.started": "2025-02-10T15:38:59.749987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-media-1.freecodecamp.org/images/1*fmWayfCY4QVIounYXWi2rg.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://cdn-media-1.freecodecamp.org/images/1*fmWayfCY4QVIounYXWi2rg.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dffca07-ed07-4285-968c-5ceb983fd4f8",
   "metadata": {},
   "source": [
    "- without value funciton (critic)\n",
    "    - from actor-critic rl to deepseek RL\n",
    "        - Actor: Policy model,\n",
    "        - Critic: Value model，providing the feedback\n",
    "- advantage **estimation**: $A_t$\n",
    "    - how much better (or worse) an action (given state) is compared to the average behavior;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
