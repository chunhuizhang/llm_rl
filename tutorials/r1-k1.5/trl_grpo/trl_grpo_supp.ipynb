{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fad3be3-6387-477c-8265-d6717e5ec954",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T15:20:02.044706Z",
     "iopub.status.busy": "2025-02-25T15:20:02.044176Z",
     "iopub.status.idle": "2025-02-25T15:20:04.057366Z",
     "shell.execute_reply": "2025-02-25T15:20:04.056478Z",
     "shell.execute_reply.started": "2025-02-25T15:20:02.044676Z"
    }
   },
   "outputs": [],
   "source": [
    "import trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa77c66-b0a8-4e15-a223-fb7f21f9b7a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T15:20:08.516752Z",
     "iopub.status.busy": "2025-02-25T15:20:08.515777Z",
     "iopub.status.idle": "2025-02-25T15:20:08.532157Z",
     "shell.execute_reply": "2025-02-25T15:20:08.530297Z",
     "shell.execute_reply.started": "2025-02-25T15:20:08.516701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.15.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trl.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74247e1a-2f69-4f1f-8116-087dab0dffd0",
   "metadata": {},
   "source": [
    "## loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f288163-9a7f-4262-bf8a-34b026d14660",
   "metadata": {},
   "source": [
    "> loss 为 0 为什么还可以反向传播，更新梯度；\n",
    "\n",
    "- loss 为 0，不意味着 gradient 为 0\n",
    "    - $f(w)=(w-1)^2-1$，在 $w=0$ 时，$f(w)=0$，但其实其 gradient 为 -2\n",
    "        - 梯度 * 学习率 才是 learning 的本质；\n",
    "    - $w-\\eta\\cdot g=0-(0.1*-2)=0.2$\n",
    "- loss 不再是一个好的 monitor 指标，而是 reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c7adb-ac9e-4280-84a8-c2adb3b28b91",
   "metadata": {},
   "source": [
    "### loss 为 0 不代表 gradient 为 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "650bd366-b8a6-42fc-b721-bf4263f93560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T14:51:11.551680Z",
     "iopub.status.busy": "2025-02-24T14:51:11.550384Z",
     "iopub.status.idle": "2025-02-24T14:51:13.134473Z",
     "shell.execute_reply": "2025-02-24T14:51:13.133479Z",
     "shell.execute_reply.started": "2025-02-24T14:51:11.551619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for x - x: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 情况1: x - x (梯度为0)\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "y1 = x - x  # 数学上等价于 0，但计算图保留关联\n",
    "y1.backward()  # 反向传播计算梯度\n",
    "print(\"Gradient for x - x:\", x.grad.item())  # 输出 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d233e3-9427-4ade-a545-c0ea55bea7d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T14:51:19.747382Z",
     "iopub.status.busy": "2025-02-24T14:51:19.746857Z",
     "iopub.status.idle": "2025-02-24T14:51:19.755712Z",
     "shell.execute_reply": "2025-02-24T14:51:19.754302Z",
     "shell.execute_reply.started": "2025-02-24T14:51:19.747351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for x - x.detach(): 1.0\n"
     ]
    }
   ],
   "source": [
    "# 清除梯度，准备下一个示例\n",
    "x.grad.zero_()\n",
    "\n",
    "# 情况2: x - x.detach() (梯度为1)\n",
    "y2 = x - x.detach()  # 分离第二个x，使其视为常数\n",
    "y2.backward()  # 反向传播计算梯度\n",
    "print(\"Gradient for x - x.detach():\", x.grad.item())  # 输出 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b010b58b-2c2f-4dc3-8e8e-67e6584c2353",
   "metadata": {},
   "source": [
    "### loss = $\\beta kl$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11afa9d-02ac-4d58-8239-80a3b9cc2725",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/open-r1/issues/239#issuecomment-2646297851\n",
    "\n",
    "- trl grpo\n",
    "    - $\\beta = 0.04$（default，`GRPOConfig`）\n",
    "    - 这个值其实是比较大的，math 用 0.001？？\n",
    "- 抛开 kl\n",
    "    - 一个 prompt 多个 generations（为一个 group）\n",
    "        - 每个 generation 对应的 loss = -advantage (likelihood ratio = 1, $\\pi_\\theta=\\pi_{\\theta_{old}}$)\n",
    "    - 一个 group 的 mean loss = - mean advantage = 0\n",
    "- kl 的位置\n",
    "    - 定义在 advantage 计算 reward 时\n",
    "    - 定义在外部\n",
    "    - grpo 原始公式是定义在外部的；\n",
    "        - the GRPO implementation does not include the KL-divergence as part of the reward function. Instead, it directly incorporates the KL-divergence into the loss function, arguing that this approach simplifies the computation and avoids unnecessary complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b6f39-5fda-41a1-93b1-5fc97578f0b8",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{J}_{GRPO}(\\theta) = \\mathbb{E}_{q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{old}}(O|q)} \\left[ \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\min \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})} \\hat{A}_{i,t}, \\text{clip} \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\\varepsilon, 1+\\varepsilon \\right) \\hat{A}_{i,t} \\right) - \\beta D_{KL} (\\pi_\\theta || \\pi_{ref}) \\right]\n",
    "$$\n",
    "\n",
    "- If you are using the GRPO trainer then the old policy is in effect updated every step, this means you just use a detached version of the current policy.\n",
    "    - 公式中的 $\\pi_{\\theta_{old}}$ 是 $\\pi_\\theta$ 的 detach 版（不参与计算图，视为常数）；\n",
    "    - $r=\\frac{\\pi_\\theta}{\\pi_{\\theta_{old}}}=1$,\n",
    "    - $\\text{clip}(1, 1-\\epsilon, 1+\\epsilon)=1$\n",
    "- $\\hat A_{i,t}=\\tilde r_i=\\frac{r_i-\\mu}{\\sigma}$ (z score) （token 级别的 adv = output 级别的 reward 组内 z-score 而来）\n",
    " \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathcal{J}_{GRPO}(\\theta)&= \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\min \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})} \\hat{A}_{i,t}, \\text{clip} \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\\varepsilon, 1+\\varepsilon \\right) \\hat{A}_{i,t} \\right) - \\beta D_{KL} (\\pi_\\theta || \\pi_{ref}) \\\\\n",
    "&=\\frac1G\\sum_i^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\hat A_{i,t} -\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=\\frac1G\\sum_i^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\hat A_i -\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=\\frac1G\\sum_i^G\\frac1{|o_i|} {|o_i|}\\cdot \\hat A_i -\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=\\frac1G\\sum_i^G\\hat A_i-\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=\\frac1G\\sum_i^G\\frac{r_i-\\mu}{\\sigma}-\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=\\frac{\\sum_i r_i-G\\mu}{G}-\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&= 0 -\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=-\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181d447-04d0-4939-a50d-721bf0d27b65",
   "metadata": {},
   "source": [
    "### per_device_train_batch_size & num_generations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48000cc9-d5bc-44b6-aecc-38280ef99479",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/trl/pull/2776\n",
    "\n",
    "- (`num_processes * per_device_batch_size`) must be divisible by `G`.\n",
    "    - `per_device_batch_size` 刻画的是 gpu device 粒度 generations 的数量\n",
    "    - `num_processes` 是 gpu 进程的数量；\n",
    "    - `num_processes * per_device_batch_size` / `G`: prompts 吞吐量\n",
    "- https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py#L571-L598\n",
    "    - ensures each prompt is repeated across multiple processes. This guarantees that identical prompts are distributed to different GPUs, allowing rewards to be computed and normalized correctly within each prompt group. Using the same seed across processes ensures consistent prompt assignment, preventing discrepancies in group formation.\n",
    "    - repeats the batch multiple times to allow reusing generations across multiple updates. Refer to _prepare_inputs to see how the generations are stored and reused.\n",
    "    - In the following figure, the values are the prompt indices. The first row shows the first sampled batch, the\n",
    "second row shows the second sampled batch, and so on.\n",
    "    - 3 个 gpus，num_generations = 3，per_device_train_batch_size = 4\n",
    "        - 3*4 / 3  = 4\n",
    "\n",
    "    |      | GPU0   | GPU1       | GPU2     |\n",
    "    |------|--------|------------|----------|\n",
    "    | P0   | P00    | P01        | P02      |\n",
    "    | P1   | P10    | P11        | P12      |\n",
    "    | P2   | P20    | P21        | P22      |\n",
    "    | P3   | P30    | P31        | P32      |\n",
    "\n",
    "    - 进一步还考虑到了 `grad_accum` = 3，累加 batch forward，统一 backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb424c-e2f4-48a7-b30d-3e32703d5fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
