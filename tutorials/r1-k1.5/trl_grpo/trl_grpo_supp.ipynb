{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fad3be3-6387-477c-8265-d6717e5ec954",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T15:20:02.044706Z",
     "iopub.status.busy": "2025-02-25T15:20:02.044176Z",
     "iopub.status.idle": "2025-02-25T15:20:04.057366Z",
     "shell.execute_reply": "2025-02-25T15:20:04.056478Z",
     "shell.execute_reply.started": "2025-02-25T15:20:02.044676Z"
    }
   },
   "outputs": [],
   "source": [
    "import trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa77c66-b0a8-4e15-a223-fb7f21f9b7a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T15:20:08.516752Z",
     "iopub.status.busy": "2025-02-25T15:20:08.515777Z",
     "iopub.status.idle": "2025-02-25T15:20:08.532157Z",
     "shell.execute_reply": "2025-02-25T15:20:08.530297Z",
     "shell.execute_reply.started": "2025-02-25T15:20:08.516701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.15.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trl.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9039de-9f73-48df-be2b-415d3935cdbf",
   "metadata": {},
   "source": [
    "## RL4LLM roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac45c86-0b16-4238-853e-b790f6ac0d03",
   "metadata": {},
   "source": [
    "- 从 trl 开始学起，框架较为基础和简单；\n",
    "    - 深入地学习 GRPO，基于 1.5B 复现 R1，复现 aha moments；\n",
    "        - 大致也基本能搞清楚 RLHF 阶段的 PPO 算法原理，二者在公式上主要只有 adv（advantage）的估计方法不同；\n",
    "- 后续可以陆陆续续迁移到更现代更多工程性能优化的 RL4LLM 的框架上\n",
    "    - 比如 veRL 和 OpenRLHF\n",
    "    - 假如都是零基础，优先 veRL 吧，除非继承而来的项目是 OpenRLHF；\n",
    "    - OpenRLHF：2405.11143，5k stars；\n",
    "    - veRL：2409.19256，3.8k stars；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74247e1a-2f69-4f1f-8116-087dab0dffd0",
   "metadata": {},
   "source": [
    "## loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f288163-9a7f-4262-bf8a-34b026d14660",
   "metadata": {},
   "source": [
    "> loss 为 0 为什么还可以反向传播，更新梯度；\n",
    "\n",
    "- loss 为 0，不意味着 gradient 为 0\n",
    "    - $f(w)=(w-1)^2-1$，在 $w=0$ 时，$f(w)=0$，但其实其 gradient 为 -2\n",
    "        - 梯度 * 学习率 才是 learning 的本质；\n",
    "    - $w-\\eta\\cdot g=0-(0.1*-2)=0.2$\n",
    "- loss 不再是一个好的 monitor 指标，而是 reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c7adb-ac9e-4280-84a8-c2adb3b28b91",
   "metadata": {},
   "source": [
    "### loss 为 0 不代表 gradient 为 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "650bd366-b8a6-42fc-b721-bf4263f93560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T14:51:11.551680Z",
     "iopub.status.busy": "2025-02-24T14:51:11.550384Z",
     "iopub.status.idle": "2025-02-24T14:51:13.134473Z",
     "shell.execute_reply": "2025-02-24T14:51:13.133479Z",
     "shell.execute_reply.started": "2025-02-24T14:51:11.551619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for x - x: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 情况1: x - x (梯度为0)\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "y1 = x - x  # 数学上等价于 0，但计算图保留关联\n",
    "y1.backward()  # 反向传播计算梯度\n",
    "print(\"Gradient for x - x:\", x.grad.item())  # 输出 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d233e3-9427-4ade-a545-c0ea55bea7d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T14:51:19.747382Z",
     "iopub.status.busy": "2025-02-24T14:51:19.746857Z",
     "iopub.status.idle": "2025-02-24T14:51:19.755712Z",
     "shell.execute_reply": "2025-02-24T14:51:19.754302Z",
     "shell.execute_reply.started": "2025-02-24T14:51:19.747351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for x - x.detach(): 1.0\n"
     ]
    }
   ],
   "source": [
    "# 清除梯度，准备下一个示例\n",
    "x.grad.zero_()\n",
    "\n",
    "# 情况2: x - x.detach() (梯度为1)\n",
    "y2 = x - x.detach()  # 分离第二个x，使其视为常数\n",
    "y2.backward()  # 反向传播计算梯度\n",
    "print(\"Gradient for x - x.detach():\", x.grad.item())  # 输出 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b010b58b-2c2f-4dc3-8e8e-67e6584c2353",
   "metadata": {},
   "source": [
    "### loss = $\\beta kl$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11afa9d-02ac-4d58-8239-80a3b9cc2725",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/open-r1/issues/239#issuecomment-2646297851\n",
    "\n",
    "- trl grpo\n",
    "    - $\\beta = 0.04$（default）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b6f39-5fda-41a1-93b1-5fc97578f0b8",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{J}_{GRPO}(\\theta) = \\mathbb{E}_{q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{old}}(O|q)} \\left[ \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\min \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})} \\hat{A}_{i,t}, \\text{clip} \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\\varepsilon, 1+\\varepsilon \\right) \\hat{A}_{i,t} \\right) - \\beta D_{KL} (\\pi_\\theta || \\pi_{ref}) \\right]\n",
    "$$\n",
    "\n",
    "- If you are using the GRPO trainer then the old policy is in effect updated every step, this means you just use a detached version of the current policy.\n",
    "    - 公式中的 $\\pi_{\\theta_{old}}$ 是 $\\pi_\\theta$ 的 detach 版（不参与计算图，视为常数）；\n",
    "    - $r=\\frac{\\pi_\\theta}{\\pi_{\\theta_{old}}}=1$,\n",
    "    - $\\text{clip}(1, 1-\\epsilon, 1+\\epsilon)=1$\n",
    "- $\\hat A_{i,t}=\\tilde r_i=\\frac{r_i-\\mu}{\\sigma}$ (z score) （token 级别的 adv = output 级别的 reward 组内 z-score 而来）\n",
    " \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathcal{J}_{GRPO}(\\theta)&= \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\min \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})} \\hat{A}_{i,t}, \\text{clip} \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\\varepsilon, 1+\\varepsilon \\right) \\hat{A}_{i,t} \\right) - \\beta D_{KL} (\\pi_\\theta || \\pi_{ref}) \\\\\n",
    "&=\\frac1G\\sum_i^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\hat A_{i,t} -\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=\\frac1G\\sum_i^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\hat A_i -\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=\\frac1G\\sum_i^G\\frac1{|o_i|} {|o_i|}\\cdot \\hat A_i -\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=\\frac1G\\sum_i^G\\hat A_i-\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=\\frac1G\\sum_i^G\\frac{r_i-\\mu}{\\sigma}-\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=\\frac{\\sum_i r_i-G\\mu}{G}-\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&= 0 -\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=-\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\n",
    "\\end{split}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
