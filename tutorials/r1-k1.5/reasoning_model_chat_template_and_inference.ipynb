{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f725cbf-889c-4be4-a7e1-bef3c55dd959",
   "metadata": {},
   "source": [
    "> 目前的 AI = 数据（data） + 算法（algorithm） + 架构（infra）\n",
    ">> 不要排斥基础，觉得简单就不关注相关的细节。越是复杂的系统，越要从基础从原理，分解从模块来看。\n",
    "- 不只是 tokenize，还有 chat template，什么时候轮到 llm 输出，如何区分 user 和 assistant（包括这期要介绍的 reasoning tokenizer，所谓的 reasoning tokens & answer tokens）；\n",
    "    - fancy 和 powerful 的 llm，似乎 tokenizer 很 low level，显得很没有意思，甚至繁琐；\n",
    "    - chat temaplte (for chat models, 目前的 reasoning models 首先也得是一个 chat models)\n",
    "        - 添加特殊 token id，标记身份（user/assistant/tool）；\n",
    "            - System: 建立初始的身份认知；\n",
    "           - tool_call，tool_response（也是一种身份）\n",
    "        - 添加 system prompt，如果用户没有显示地传入的话\n",
    "        - 解析历史对话：循环解析的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dd2d98e-ec23-4369-ab18-2aa194bb2eff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:10.218962Z",
     "iopub.status.busy": "2025-03-01T00:52:10.218546Z",
     "iopub.status.idle": "2025-03-01T00:52:12.320416Z",
     "shell.execute_reply": "2025-03-01T00:52:12.319510Z",
     "shell.execute_reply.started": "2025-03-01T00:52:10.218928Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bcb93d1-2fe4-4358-a036-d78f4e34ea1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:12.321279Z",
     "iopub.status.busy": "2025-03-01T00:52:12.321084Z",
     "iopub.status.idle": "2025-03-01T00:52:12.325107Z",
     "shell.execute_reply": "2025-03-01T00:52:12.324358Z",
     "shell.execute_reply.started": "2025-03-01T00:52:12.321267Z"
    }
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-Math-1.5B\",\n",
    "    \"Qwen/Qwen2.5-1.5B\",\n",
    "    \"Qwen/QwQ-32B-Preview\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8887e5e-adeb-4b71-9fda-c4d4f41b9226",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:12.325835Z",
     "iopub.status.busy": "2025-03-01T00:52:12.325703Z",
     "iopub.status.idle": "2025-03-01T00:52:12.345795Z",
     "shell.execute_reply": "2025-03-01T00:52:12.345009Z",
     "shell.execute_reply.started": "2025-03-01T00:52:12.325824Z"
    }
   },
   "outputs": [],
   "source": [
    "def hf_tokenizer(name_or_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name_or_path)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        print(f'tokenizer.pad_token_id is None. Now set to {tokenizer.eos_token_id}')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f'tokenizer.pad_token is None. Now set to {tokenizer.eos_token}')\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b47a84-15f8-4bae-849e-87c239146e3a",
   "metadata": {},
   "source": [
    "## tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f927115b-1a53-460d-a05d-00b9bb31a923",
   "metadata": {},
   "source": [
    "- DeepSeek-R1-Distill-Qwen-1.5B 由 Qwen2.5-Math-1.5B 蒸馏而来，而不是 Qwen/Qwen2.5-1.5B-Instruct；\n",
    "- 复用了词表，重新定义了一些特殊的 token id；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f0cd09a-d29b-4f8c-b33e-34352a77cc88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:12.346522Z",
     "iopub.status.busy": "2025-03-01T00:52:12.346391Z",
     "iopub.status.idle": "2025-03-01T00:52:12.355041Z",
     "shell.execute_reply": "2025-03-01T00:52:12.354279Z",
     "shell.execute_reply.started": "2025-03-01T00:52:12.346512Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_tokenizer(tokenizer, text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    print(f'{text}, tokens: {tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b4875c-ac69-4665-b9eb-def7ba3e1de6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:12.355728Z",
     "iopub.status.busy": "2025-03-01T00:52:12.355585Z",
     "iopub.status.idle": "2025-03-01T00:52:15.411109Z",
     "shell.execute_reply": "2025-03-01T00:52:15.410227Z",
     "shell.execute_reply.started": "2025-03-01T00:52:12.355717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, tokenizer.pad_token: <｜end▁of▁sentence｜>, tokenizer.pad_token_id: 151643\n",
      "hello world, tokens: [151646, 14990, 1879]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Qwen/Qwen2.5-1.5B-Instruct, tokenizer.pad_token: <|endoftext|>, tokenizer.pad_token_id: 151643\n",
      "hello world, tokens: [14990, 1879]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Qwen/Qwen2.5-Math-1.5B, tokenizer.pad_token: <|endoftext|>, tokenizer.pad_token_id: 151643\n",
      "hello world, tokens: [14990, 1879]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Qwen/Qwen2.5-1.5B, tokenizer.pad_token: <|endoftext|>, tokenizer.pad_token_id: 151643\n",
      "hello world, tokens: [14990, 1879]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Qwen/QwQ-32B-Preview, tokenizer.pad_token: <|endoftext|>, tokenizer.pad_token_id: 151643\n",
      "hello world, tokens: [14990, 1879]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name_or_path in models:\n",
    "    tokenizer = hf_tokenizer(name_or_path)\n",
    "    print(f'{name_or_path}, tokenizer.pad_token: {tokenizer.pad_token}, tokenizer.pad_token_id: {tokenizer.pad_token_id}')\n",
    "    test_tokenizer(tokenizer, \"hello world\")\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090c0d1f-41e0-4893-8c4d-0f399e078ebd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:15.413112Z",
     "iopub.status.busy": "2025-03-01T00:52:15.412962Z",
     "iopub.status.idle": "2025-03-01T00:52:18.081933Z",
     "shell.execute_reply": "2025-03-01T00:52:18.081162Z",
     "shell.execute_reply.started": "2025-03-01T00:52:15.413100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>\n",
      "<|object_ref_start|>\n",
      "<|object_ref_start|>\n",
      "<|object_ref_start|>\n",
      "<|object_ref_start|>\n"
     ]
    }
   ],
   "source": [
    "distill_tokenizer = hf_tokenizer('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B')\n",
    "print(distill_tokenizer.decode(151646))\n",
    "qwen_math_tokenizer = hf_tokenizer('Qwen/Qwen2.5-Math-1.5B')\n",
    "print(qwen_math_tokenizer.decode(151646))\n",
    "qwen_chat_tokenizer = hf_tokenizer('Qwen/Qwen2.5-1.5B-Instruct')\n",
    "print(qwen_chat_tokenizer.decode(151646))\n",
    "qwen_base_tokenizer = hf_tokenizer('Qwen/Qwen2.5-1.5B')\n",
    "print(qwen_base_tokenizer.decode(151646))\n",
    "qwen_reason_tokenizer = hf_tokenizer('Qwen/QwQ-32B-Preview')\n",
    "print(qwen_base_tokenizer.decode(151646))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b908f242-cde9-435f-bd3e-e4d79eeab2be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.082710Z",
     "iopub.status.busy": "2025-03-01T00:52:18.082553Z",
     "iopub.status.idle": "2025-03-01T00:52:18.089465Z",
     "shell.execute_reply": "2025-03-01T00:52:18.088703Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.082697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151646, 151644]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distill_tokenizer.encode('<｜User｜>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df7609f5-8c46-4925-b197-912b8c9319d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.090181Z",
     "iopub.status.busy": "2025-03-01T00:52:18.090036Z",
     "iopub.status.idle": "2025-03-01T00:52:18.107858Z",
     "shell.execute_reply": "2025-03-01T00:52:18.107080Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.090169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([27, 130957, 1474, 130957, 29],\n",
       " [27, 130957, 1474, 130957, 29],\n",
       " [27, 130957, 1474, 130957, 29])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_math_tokenizer.encode('<｜User｜>'), qwen_chat_tokenizer.encode('<｜User｜>'), qwen_base_tokenizer.encode('<｜User｜>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0780db8c-e866-4add-950e-b7712feaad55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.108630Z",
     "iopub.status.busy": "2025-03-01T00:52:18.108459Z",
     "iopub.status.idle": "2025-03-01T00:52:18.116523Z",
     "shell.execute_reply": "2025-03-01T00:52:18.115685Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.108615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜end▁of▁sentence｜>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is <｜end▁of▁sentence｜>\n",
    "# https://chat.deepseek.com/a/chat/s/569c8476-7b64-48fa-865b-9e01718b961b\n",
    "# what is <|im_end|>\n",
    "# https://chat.qwen.ai/c/da88d4f3-c279-4851-acbb-d3f051c11e86\n",
    "distill_tokenizer.decode(151643)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7713b696-fec9-4d2e-bbe6-5fdbb1989731",
   "metadata": {},
   "source": [
    "## chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e2fac5-0b65-4380-b99a-7fac6a6fbe0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.117257Z",
     "iopub.status.busy": "2025-03-01T00:52:18.117071Z",
     "iopub.status.idle": "2025-03-01T00:52:18.804400Z",
     "shell.execute_reply": "2025-03-01T00:52:18.803461Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.117241Z"
    }
   },
   "outputs": [],
   "source": [
    "AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B').chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089fb66e-dc13-4d0e-a79f-c14e546bc610",
   "metadata": {},
   "source": [
    "- jinja template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c5d8dd8-7c49-4a55-83f6-cc4e5df0f755",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.805497Z",
     "iopub.status.busy": "2025-03-01T00:52:18.805350Z",
     "iopub.status.idle": "2025-03-01T00:52:18.813232Z",
     "shell.execute_reply": "2025-03-01T00:52:18.812465Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.805486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用标准语法 {% ... %} 的结果：\n",
      "'\\n\\n    Hello\\n'\n",
      "\n",
      "使用去除空白字符的语法 {% - ... -%} 的结果：\n",
      "'Hello'\n"
     ]
    }
   ],
   "source": [
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "# 创建 Jinja2 环境\n",
    "env = Environment(loader=FileSystemLoader(\".\"))\n",
    "\n",
    "# 模板 1: 使用标准语法 {% ... %}\n",
    "template1 = env.from_string(\"\"\"\n",
    "{% if True %}\n",
    "    Hello\n",
    "{% endif %}\n",
    "\"\"\")\n",
    "\n",
    "# 模板 2: 使用去除空白字符的语法 {% - ... -%}\n",
    "template2 = env.from_string(\"\"\"\n",
    "{%- if True -%}\n",
    "    Hello\n",
    "{%- endif -%}\n",
    "\"\"\")\n",
    "\n",
    "# 渲染模板\n",
    "result1 = template1.render()\n",
    "result2 = template2.render()\n",
    "\n",
    "# 打印结果\n",
    "print(\"使用标准语法 {% ... %} 的结果：\")\n",
    "print(repr(result1))  # repr 用于显示换行符和空白字符\n",
    "\n",
    "print(\"\\n使用去除空白字符的语法 {% - ... -%} 的结果：\")\n",
    "print(repr(result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1127d0a-9a5b-4c8a-baca-aac993652205",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.813979Z",
     "iopub.status.busy": "2025-03-01T00:52:18.813841Z",
     "iopub.status.idle": "2025-03-01T00:52:18.833011Z",
     "shell.execute_reply": "2025-03-01T00:52:18.832194Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.813968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We do not recommend using base language models for conversations. Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., on this model.\n",
    "# https://huggingface.co/Qwen/Qwen2.5-1.5B\n",
    "print(qwen_base_tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9b3637-ac1b-4fbf-92ef-b70a6f01c599",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    S(System) -->|初始化身份| A(Assistant)\n",
    "    U(User) -->|提问/响应| A\n",
    "    A -->|自然语言回复| U\n",
    "    A -->|调用工具| T(Tool)\n",
    "    T -->|返回结果| A\n",
    "    T -.->|直接响应| U\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7a5315-ed70-4af3-8fcf-e6fa11a7be76",
   "metadata": {},
   "source": [
    "### distill tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d11103a-dfb7-443e-8093-06a96dad8e5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:55:06.355667Z",
     "iopub.status.busy": "2025-03-01T00:55:06.354096Z",
     "iopub.status.idle": "2025-03-01T00:55:06.365351Z",
     "shell.execute_reply": "2025-03-01T00:55:06.363189Z",
     "shell.execute_reply.started": "2025-03-01T00:55:06.355610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(distill_tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748e6d60-0c94-4106-9660-ea5229e78f9e",
   "metadata": {},
   "source": [
    "```jinja\n",
    "{% if not add_generation_prompt is defined %}\n",
    "    {% set add_generation_prompt = false %}\n",
    "{% endif %}\n",
    "\n",
    "{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}\n",
    "\n",
    "{%- for message in messages %}\n",
    "    {%- if message['role'] == 'system' %}\n",
    "        {% set ns.system_prompt = message['content'] %}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "\n",
    "{{bos_token}}{{ns.system_prompt}}\n",
    "\n",
    "{%- for message in messages %}\n",
    "    {%- if message['role'] == 'user' %}\n",
    "        {%- set ns.is_tool = false -%}\n",
    "        {{'<｜User｜>' + message['content']}}\n",
    "    {%- endif %}\n",
    "\n",
    "    {%- if message['role'] == 'assistant' and message['content'] is none %}\n",
    "        {%- set ns.is_tool = false -%}\n",
    "        {%- for tool in message['tool_calls']%}\n",
    "            {%- if not ns.is_first %}\n",
    "                {{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}\n",
    "                {%- set ns.is_first = true -%}\n",
    "            {%- else %}\n",
    "                {{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}\n",
    "                {{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}\n",
    "            {%- endif %}\n",
    "        {%- endfor %}\n",
    "    {%- endif %}\n",
    "\n",
    "    {%- if message['role'] == 'assistant' and message['content'] is not none %}\n",
    "        {%- if ns.is_tool %}\n",
    "            {{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}\n",
    "            {%- set ns.is_tool = false -%}\n",
    "        {%- else %}\n",
    "            {% set content = message['content'] %}\n",
    "            {% if '</think>' in content %}\n",
    "                {% set content = content.split('</think>')[-1] %}\n",
    "            {% endif %}\n",
    "            {{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}\n",
    "        {%- endif %}\n",
    "    {%- endif %}\n",
    "\n",
    "    {%- if message['role'] == 'tool' %}\n",
    "        {%- set ns.is_tool = true -%}\n",
    "        {%- if ns.is_output_first %}\n",
    "            {{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}\n",
    "            {%- set ns.is_output_first = false %}\n",
    "        {%- else %}\n",
    "            {{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}\n",
    "        {%- endif %}\n",
    "    {%- endif %}\n",
    "{%- endfor -%}\n",
    "\n",
    "{% if ns.is_tool %}\n",
    "    {{'<｜tool▁outputs▁end｜>'}}\n",
    "{% endif %}\n",
    "\n",
    "{% if add_generation_prompt and not ns.is_tool %}\n",
    "    {{'<｜Assistant｜><think>\\n'}}\n",
    "{% endif %}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7afc458-5289-40cb-8408-9b67e0a8bcc3",
   "metadata": {},
   "source": [
    "### qwen tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98f7c948-f1bd-474c-8a1d-0443d7c46d66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.843733Z",
     "iopub.status.busy": "2025-03-01T00:52:18.843575Z",
     "iopub.status.idle": "2025-03-01T00:52:18.851820Z",
     "shell.execute_reply": "2025-03-01T00:52:18.851002Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.843720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# Tools\n",
      "\n",
      "You may call one or more functions to assist with the user query.\n",
      "\n",
      "You are provided with function signatures within <tools></tools> XML tags:\n",
      "<tools>\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42be454f-b7ac-4cc9-86a2-0c5f0e3d5d54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.852582Z",
     "iopub.status.busy": "2025-03-01T00:52:18.852421Z",
     "iopub.status.idle": "2025-03-01T00:52:18.861291Z",
     "shell.execute_reply": "2025-03-01T00:52:18.860428Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.852568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# default system prompt\n",
    "print(qwen_chat_tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d59e93cb-b1df-4f2f-94cc-ff58551e54c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.862156Z",
     "iopub.status.busy": "2025-03-01T00:52:18.861966Z",
     "iopub.status.idle": "2025-03-01T00:52:18.870138Z",
     "shell.execute_reply": "2025-03-01T00:52:18.869252Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.862141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(qwen_reason_tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3bf83e-7551-406a-90fb-8f2144aa19d4",
   "metadata": {},
   "source": [
    "## apply chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f2ff5f3-24e5-49a5-b368-556334ebbd28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.871214Z",
     "iopub.status.busy": "2025-03-01T00:52:18.870980Z",
     "iopub.status.idle": "2025-03-01T00:52:18.877829Z",
     "shell.execute_reply": "2025-03-01T00:52:18.876940Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.871195Z"
    }
   },
   "outputs": [],
   "source": [
    "basic_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eccfdbca-1e55-4e93-9214-687b1cef3a66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.878938Z",
     "iopub.status.busy": "2025-03-01T00:52:18.878699Z",
     "iopub.status.idle": "2025-03-01T00:52:18.913995Z",
     "shell.execute_reply": "2025-03-01T00:52:18.913126Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.878908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜>You are a helpful assistant.<｜User｜>Hello, how are you?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distill_tokenizer.apply_chat_template(basic_messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42c3f4d4-fc2b-46fb-affa-50af32b9f80e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.915087Z",
     "iopub.status.busy": "2025-03-01T00:52:18.914865Z",
     "iopub.status.idle": "2025-03-01T00:52:18.920815Z",
     "shell.execute_reply": "2025-03-01T00:52:18.919936Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.915070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜>You are a helpful assistant.<｜User｜>Hello, how are you?<｜Assistant｜><think>\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "distill_tokenizer.apply_chat_template(basic_messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae6a1248-b64a-4d36-b1c3-f84d0298df84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.921816Z",
     "iopub.status.busy": "2025-03-01T00:52:18.921598Z",
     "iopub.status.idle": "2025-03-01T00:52:18.951353Z",
     "shell.execute_reply": "2025-03-01T00:52:18.950439Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.921798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHello, how are you?<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_chat_tokenizer.apply_chat_template(basic_messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cb35ff5-aa6c-40a6-aedd-b4cee08b977f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T01:04:29.930288Z",
     "iopub.status.busy": "2025-03-01T01:04:29.928846Z",
     "iopub.status.idle": "2025-03-01T01:04:29.942603Z",
     "shell.execute_reply": "2025-03-01T01:04:29.940296Z",
     "shell.execute_reply.started": "2025-03-01T01:04:29.930224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHello, how are you?<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_reason_tokenizer.apply_chat_template(basic_messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8454dd25-25dc-4e0f-8049-abb3ecfc5e5c",
   "metadata": {},
   "source": [
    "## vllm inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c46ca00-bcac-4aa1-a438-9b16c04dc3ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.978249Z",
     "iopub.status.busy": "2025-03-01T00:52:18.977987Z",
     "iopub.status.idle": "2025-03-01T00:52:18.984699Z",
     "shell.execute_reply": "2025-03-01T00:52:18.983698Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.978228Z"
    }
   },
   "outputs": [],
   "source": [
    "gsm8k_inference_test = \"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"\n",
    "gt_ans = '18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9358ee4a-134e-4a7e-bcd1-8d615648cd6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.986336Z",
     "iopub.status.busy": "2025-03-01T00:52:18.985778Z",
     "iopub.status.idle": "2025-03-01T00:52:18.995990Z",
     "shell.execute_reply": "2025-03-01T00:52:18.995010Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.986313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜begin▁of▁sentence｜><｜Assistant｜><think>\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distill_tokenizer.apply_chat_template([gsm8k_inference_test], add_generation_prompt=True, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80342aa3-8d10-43ce-93ac-379fb60998d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:18.997291Z",
     "iopub.status.busy": "2025-03-01T00:52:18.997012Z",
     "iopub.status.idle": "2025-03-01T00:52:19.005680Z",
     "shell.execute_reply": "2025-03-01T00:52:19.004142Z",
     "shell.execute_reply.started": "2025-03-01T00:52:18.997268Z"
    }
   },
   "outputs": [],
   "source": [
    "instruction = \"Let's think step by step and output the final answer within \\\\boxed{}.\"\n",
    "chat_test = [{'role': 'user', 'content': f'{gsm8k_inference_test} {instruction}'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f6cdee7-052d-41b8-b471-b5a2ae8289f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:19.007216Z",
     "iopub.status.busy": "2025-03-01T00:52:19.006900Z",
     "iopub.status.idle": "2025-03-01T00:52:19.019209Z",
     "shell.execute_reply": "2025-03-01T00:52:19.017223Z",
     "shell.execute_reply.started": "2025-03-01T00:52:19.007194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? Let's think step by step and output the final answer within \\\\boxed{}.\"}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0b23bd4-3cc8-48e4-b30a-dbeb8ee06156",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:19.021007Z",
     "iopub.status.busy": "2025-03-01T00:52:19.020647Z",
     "iopub.status.idle": "2025-03-01T00:52:19.029864Z",
     "shell.execute_reply": "2025-03-01T00:52:19.028591Z",
     "shell.execute_reply.started": "2025-03-01T00:52:19.020980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<｜begin▁of▁sentence｜><｜User｜>Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? Let's think step by step and output the final answer within \\\\boxed{}.<｜Assistant｜><think>\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distill_tokenizer.apply_chat_template(chat_test, add_generation_prompt=True, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0d0a4ea-4df0-4541-a426-3c80719923bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:19.032687Z",
     "iopub.status.busy": "2025-03-01T00:52:19.032277Z",
     "iopub.status.idle": "2025-03-01T00:52:19.041213Z",
     "shell.execute_reply": "2025-03-01T00:52:19.039538Z",
     "shell.execute_reply.started": "2025-03-01T00:52:19.032657Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_ids = distill_tokenizer.apply_chat_template(chat_test, add_generation_prompt=True, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d030e1b-3a53-4191-aa1d-8afcd42090bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:19.043136Z",
     "iopub.status.busy": "2025-03-01T00:52:19.042750Z",
     "iopub.status.idle": "2025-03-01T00:52:20.623617Z",
     "shell.execute_reply": "2025-03-01T00:52:20.622728Z",
     "shell.execute_reply.started": "2025-03-01T00:52:19.043106Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a631962-b10b-4f37-ae08-b6f1a57bb5a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:20.624311Z",
     "iopub.status.busy": "2025-03-01T00:52:20.624081Z",
     "iopub.status.idle": "2025-03-01T00:52:43.047571Z",
     "shell.execute_reply": "2025-03-01T00:52:43.046677Z",
     "shell.execute_reply.started": "2025-03-01T00:52:20.624299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-01 08:52:25 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 03-01 08:52:27 model_runner.py:1060] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "INFO 03-01 08:52:28 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 03-01 08:52:28 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edf4577bd4b4f9e962a3b481af568cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-01 08:52:29 model_runner.py:1071] Loading model weights took 3.3460 GB\n",
      "INFO 03-01 08:52:30 gpu_executor.py:122] # GPU blocks: 36442, # CPU blocks: 9362\n",
      "INFO 03-01 08:52:30 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 17.79x\n",
      "INFO 03-01 08:52:32 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-01 08:52:32 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-01 08:52:42 model_runner.py:1530] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B',\n",
    "        max_model_len=32768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f4c77d6-a113-48c4-a733-be66a3007295",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:43.048357Z",
     "iopub.status.busy": "2025-03-01T00:52:43.048205Z",
     "iopub.status.idle": "2025-03-01T00:52:43.051838Z",
     "shell.execute_reply": "2025-03-01T00:52:43.051073Z",
     "shell.execute_reply.started": "2025-03-01T00:52:43.048346Z"
    }
   },
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "        temperature=0.6, max_tokens=32768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "832e9c16-9b13-4ecb-9211-a2bcf05d1f48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T00:52:43.052488Z",
     "iopub.status.busy": "2025-03-01T00:52:43.052357Z",
     "iopub.status.idle": "2025-03-01T00:52:46.944145Z",
     "shell.execute_reply": "2025-03-01T00:52:46.943260Z",
     "shell.execute_reply.started": "2025-03-01T00:52:43.052478Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.87s/it, est. speed input: 21.98 toks/s, output: 197.83 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, let's tackle this problem step by step. So, Janet has ducks that lay 16 eggs every day. Hmm, okay, that's a lot! She eats three eggs for breakfast every morning and bakes muffins for her friends with four eggs each day. Then, she sells the rest at the farmers' market for $2 per egg. I need to figure out how much money she makes every day from selling the eggs.\n",
      "\n",
      "First, let me break down the information given:\n",
      "\n",
      "1. **Duck eggs per day:** 16\n",
      "2. **Eggs eaten for breakfast:** 3 per day\n",
      "3. **Eggs used for muffins:** 4 per day\n",
      "4. **Selling price per egg:** $2\n",
      "\n",
      "So, the plan is to subtract the eggs Janet eats and uses for muffins from the total eggs laid each day. The remaining eggs will be what she sells, and then we can multiply that by the selling price to get her daily earnings.\n",
      "\n",
      "Let me write this down in a more structured way:\n",
      "\n",
      "Total eggs per day = 16\n",
      "\n",
      "Eggs eaten for breakfast = 3\n",
      "\n",
      "Eggs used for muffins = 4\n",
      "\n",
      "So, the eggs available for selling = Total eggs - Eggs eaten - Eggs used for muffins\n",
      "\n",
      "That is:\n",
      "\n",
      "Eggs for selling = 16 - 3 - 4\n",
      "\n",
      "Let me compute that:\n",
      "\n",
      "16 - 3 is 13, and then 13 - 4 is 9. So, 9 eggs are left for selling.\n",
      "\n",
      "Now, she sells each egg for $2. So, the total revenue from selling the eggs would be:\n",
      "\n",
      "Total revenue = Eggs for selling × Selling price per egg\n",
      "\n",
      "Which is:\n",
      "\n",
      "Total revenue = 9 × $2\n",
      "\n",
      "Calculating that, 9 times 2 is 18. So, she makes $18 each day from selling the eggs.\n",
      "\n",
      "Wait, let me double-check my calculations to make sure I didn't make a mistake.\n",
      "\n",
      "Total eggs: 16\n",
      "\n",
      "Eggs eaten for breakfast: 3, so 16 - 3 = 13\n",
      "\n",
      "Eggs used for muffins: 4, so 13 - 4 = 9\n",
      "\n",
      "Yes, 9 eggs left.\n",
      "\n",
      "9 eggs × $2 = $18. That seems right.\n",
      "\n",
      "Alternatively, I can check by adding up the eggs used:\n",
      "\n",
      "3 breakfast + 4 muffins = 7 eggs\n",
      "\n",
      "So, 7 eggs are eaten or used, and 16 - 7 = 9 eggs left. Yep, same result.\n",
      "\n",
      "So, 9 × $2 is definitely $18.\n",
      "\n",
      "I don't think I've missed anything here. She starts the day with 16 eggs, uses 7 of them, sells the rest, and that's the amount she makes. So, the answer should be $18.\n",
      "\n",
      "**Final Answer**\n",
      "\\boxed{18}\n",
      "</think>\n",
      "\n",
      "Janet's ducks lay 16 eggs per day. She eats 3 eggs for breakfast and uses 4 eggs for baking muffins. The remaining eggs are sold at the farmers' market for $2 each.\n",
      "\n",
      "1. Total eggs per day: 16\n",
      "2. Eggs eaten for breakfast: 3\n",
      "3. Eggs used for muffins: 4\n",
      "4. Eggs available for selling: \\(16 - 3 - 4 = 9\\)\n",
      "\n",
      "The revenue from selling the remaining eggs is calculated as:\n",
      "\\[ 9 \\text{ eggs} \\times \\$2 \\text{ per egg} = \\$18 \\]\n",
      "\n",
      "Thus, Janet makes \\(\\boxed{18}\\) dollars every day at the farmers' market.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "response = llm.generate(prompt_token_ids=prompt_ids, sampling_params=sampling_params)[0]\n",
    "print(response.outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
