{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd6fecc7-b55e-4781-a86e-389a31cdd15f",
   "metadata": {},
   "source": [
    "- k1.5 和 R1\n",
    "    - 中文 reasoning models：更好的中文理解及运用，都是国产之光；\n",
    "        - openai reasoning llms 路线的探索和复现者\n",
    "        - 即如何利用 RL 更好地激化 LLMs 的长链推理能力；\n",
    "    - k1.5 应该是第一个次 kimi 发布的 technical report\n",
    "    - k1.5 和 R1 可以对比着看，互相补充，交叉验证一些内容\n",
    "        - k1.5 的细节更为丰富，全面；\n",
    "        - 都舍去了 mcts、value function、prm（process reward models），追求 simple & scaling；\n",
    "            - **Simplistic Framework**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7c337-3511-4436-8608-a51b4a01eb56",
   "metadata": {},
   "source": [
    "- test cases\n",
    "    - Using the numbers {1,3,5,37}, create an equation that equals {24}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once.\n",
    "        - `Alright, ... Let me think ... Hmm, ... Alternatively, ... Wait, `\n",
    "    - 用忆秦娥的词牌，创作一手乡愁主题的词。\n",
    "        - 不是简单的语言问题，还包括很多很难的约束，平仄、押韵、重复等等；\n",
    "    - 学校组织出游，班长带了不超过150包的湿巾纸，如果40个人平均分则多7包，25个人平均分则多2包。问班长共带了多少包湿巾纸？\n",
    "        - 127：127 = 40*3 + 7；127 = 25\\*5 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743dc24f-825a-47ad-a64a-39b5d8c3c692",
   "metadata": {},
   "source": [
    "## reasoning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e1610c-9d5f-480e-a8b3-6872c444f44f",
   "metadata": {},
   "source": [
    "- think aloud：思考的具象化\n",
    "    - thinking process\n",
    "    - thinking steps\n",
    "    - thinking tokens\n",
    "- long context, long cot;\n",
    "- emergence\n",
    "    - planning, evaluation, reflection, exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79894ee3-474b-4c0e-9316-050923d46807",
   "metadata": {},
   "source": [
    "### Expert CoT => learned CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82607253-d3c8-4782-a52a-8f997b52108a",
   "metadata": {},
   "source": [
    "- 回答复杂问题不应该像简单问题消耗同样的计算量；\n",
    "    - CoT 发生在 decoding/test/inference-time compute\n",
    "    - 复杂问题：多跳推理，相比较事实性（fact）问答；\n",
    "- training a model for reasoning \n",
    "    - humans (experts) write out their thought process and train on that\n",
    "    - train the model using RL to **generate and hone** its own chain of thoughts\n",
    "        - it can do even better than having humans right chains of thought for it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de2b14e-08dd-4baa-a9bc-88e9270ccb9a",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c096f6c8-459a-491c-b3c9-7d22cd103227",
   "metadata": {},
   "source": [
    "> 数据从哪里来；\n",
    "> 数据质量的定义；\n",
    "\n",
    "- data quality\n",
    "    - Diverse Coverage;\n",
    "    - Balanced Difficulty;\n",
    "    - Accurate Evaluability; => reward design;\n",
    "        - rule-based reward design (vs. RLHF 中的 learned/neural reward modeling)\n",
    "        - $r(x,y,y^\\star)$\n",
    "- 数据从哪里来（非常非常 engineering）\n",
    "    - data triplets\n",
    "        - questions\n",
    "            - we employ automatic filters to select questions that require rich reasoning and are straightforward to evaluate. \n",
    "        - cot\n",
    "        - answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b5f4c8-3e0b-44b2-a13f-8bcc505fd9f8",
   "metadata": {},
   "source": [
    "## RL 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032677f8-8fe2-451c-acf6-d8bd6a9f7fd4",
   "metadata": {},
   "source": [
    "- PG (Policy gradient)\n",
    "    - TRPO, PPO (GRPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12d61ad-37f0-4bb5-805e-e3bf39884ebf",
   "metadata": {},
   "source": [
    "### don't teach. Incentive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2807acb1-289b-4d12-a3f8-2950fb947fb0",
   "metadata": {},
   "source": [
    "### Improved policy optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db3929-455e-44e1-8df0-4dbcefa82035",
   "metadata": {},
   "source": [
    "> pg => PPO;\n",
    "\n",
    "$$\n",
    "\\mathbb E_{\\tau\\sim \\pi_\\theta}[\\nabla_\\theta \\log\\pi_\\theta(\\tau)\\cdot R(\\tau)]\n",
    "$$\n",
    "- On-policy: the agent learned and the agent interacting with the env is same;\n",
    "    - 边实践边学习；数据利用率低；\n",
    "    - policy-gradient (on-policy)\n",
    "- Off-policy: the agent learned and the agent interacting with the env is different;\n",
    "    - 观察他人学习；数据利用率高；\n",
    "$$\n",
    "\\mathbb E_{\\tau\\sim \\pi_{\\theta'}}\\left[\\nabla_\\theta \\log\\pi_\\theta(\\tau)\\cdot R(\\tau)\\frac{\\pi_\\theta(\\tau)}{\\pi_{\\theta'}(\\tau)}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc018810-f15e-4c65-9856-b82e7cec23ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### reward design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7527e02-013c-4a1f-8d55-1966aaf1a21d",
   "metadata": {},
   "source": [
    "## misc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af323d03-6937-4d54-8727-d805e0a6ed22",
   "metadata": {},
   "source": [
    "### long2short"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
