{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b87ff7d4-7a2f-4833-a2e5-8ff8d07ea079",
   "metadata": {},
   "source": [
    "## RL4LLM roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f6052b-0f10-4aa1-827d-799b67e330f4",
   "metadata": {},
   "source": [
    "- 从 trl 开始学起，框架较为基础和简单；\n",
    "    - 深入地学习 GRPO，基于 1.5B 复现 R1，复现 aha moments；\n",
    "        - https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb\n",
    "        - 大致也基本能搞清楚 RLHF 阶段的 PPO 算法原理，二者在公式上主要只有 adv（advantage）的估计方法不同；\n",
    "- 后续可以陆陆续续迁移到更现代更多工程性能优化的 RL4LLM 的框架上\n",
    "    - 比如 veRL 和 OpenRLHF\n",
    "    - 假如都是零基础，优先 veRL 吧，除非继承而来的项目是 OpenRLHF；    \n",
    "    - veRL：2409.19256，3.8k stars；\n",
    "        - https://github.com/Jiayi-Pan/TinyZero\n",
    "        - https://github.com/agentica-project/deepscaler\n",
    "        - https://github.com/Unakar/Logic-RL\n",
    "    - OpenRLHF：2405.11143，5k stars；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ff9d1-9efe-476f-8f28-83bc6f9e8669",
   "metadata": {},
   "source": [
    "### TRL ppo trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb024e-249c-4bf6-96d8-1b7ce2f3b2a0",
   "metadata": {},
   "source": [
    "- https://github.com/huggingface/trl/blob/main/trl/trainer/ppo_trainer.py\n",
    "- make experiences\n",
    "    - forward\n",
    "        - queries: `[4, 56]`\n",
    "        - reponses: `[4, 53]`（$\\pi_{\\theta_{old}}$）\n",
    "        - logprobs: `[4, 53]` （$\\pi_{\\theta_{old}}$）\n",
    "        - ref_logprobs: `[4, 53]`（$\\pi_{ref}$）\n",
    "        - values: `[4, 53]`\n",
    "        - scores: `[4]` (last token's, the whole query + reponse)\n",
    "    - 计算 rewards (token 级别)\n",
    "        - $r_t = r_{T} - \\beta (\\log\\pi_\\theta-\\log\\pi_{ref})$\n",
    "            - 内循环；\n",
    "            - KL 项是 k1 近似；\n",
    "    - 计算 advantage & return\n",
    "        - GAE：\n",
    "            - $\\delta_t=r_t+\\gamma V(s_{t+1})-V(s_t)$\n",
    "            - $A_t=\\sum_{k=0}^T(\\gamma\\lambda)^k\\delta_{t+k}$\n",
    "        - return: advantage + value\n",
    "- ppo update ($\\pi_\\theta$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b01e2-7a3d-4c70-8fda-686025aa2d27",
   "metadata": {},
   "source": [
    "## adv(advantage) estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fca817-8bb4-43f3-89ba-723ba1078d50",
   "metadata": {},
   "source": [
    "- GAE\n",
    "- GRPO\n",
    "- RLOO\n",
    "- REINFORCE++\n",
    "- ReMax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ae1b9-7a70-42d2-9671-2828f40f1936",
   "metadata": {},
   "source": [
    "> verl/trainer/ppo/ray_trainer.py/compute_advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae76e7db-8970-4bfa-9a85-cc96d05aaec4",
   "metadata": {},
   "source": [
    "> verl/trainer/ppo/core_algos.py\n",
    "\n",
    "https://verl.readthedocs.io/en/latest/examples/config.html\n",
    "- compute_gae_advantage_return\n",
    "    - `token_level_rewards`, `values`\n",
    "    - $A_t^{GAE}=\\sum_{\\ell}^{T-t}(\\gamma\\lambda)^{\\ell}\\delta_{t+\\ell}, \\quad \\delta_t=r_t+\\gamma V(s_{t+1})-V(s_t)$\n",
    "    - return: $ret_t=V(s_t)+A_t^{GAE}$\n",
    "- compute_grpo_outcome_advantage\n",
    "    - `token_level_rewards`\n",
    "    - $A_i=\\frac{r_i-\\mu}{\\sigma+\\epsilon}$\n",
    "- compute_rloo_outcome_advantage\n",
    "    - `token_level_rewards`\n",
    "    - $A_i=R_i-\\frac1{n-1}\\sum_{k\\neq i}R_k$\n",
    "- compute_reinforce_plus_plus_outcome_advantage\n",
    "    - `token_level_rewards`\n",
    "    - $A_t=\\frac{G_t-\\mu}{\\sigma}, \\quad G_t=\\sum_{k=t}^T\\gamma^{k-t}r_k$\n",
    "        - return: accumulate discounted reward\n",
    "- compute_remax_outcome_advantage（Reward-Maximization with Baseline）\n",
    "    - `token_level_rewards`, `reward_baselines`\n",
    "    - $A_t=G_t-b, \\quad G_t=\\sum_{k=t}^Tr_k$\n",
    "        - no discounted return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
