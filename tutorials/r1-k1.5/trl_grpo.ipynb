{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ff3aff-cb61-4d7d-8a9b-0b1d2de0bcaf",
   "metadata": {},
   "source": [
    "- latest `trl` version\n",
    "    - https://huggingface.co/docs/trl/main/en/grpo_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53fadb6f-cf06-4442-b9f5-e7a2e1dd0a11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T00:36:21.462434Z",
     "iopub.status.busy": "2025-02-15T00:36:21.461864Z",
     "iopub.status.idle": "2025-02-15T00:36:21.480739Z",
     "shell.execute_reply": "2025-02-15T00:36:21.478984Z",
     "shell.execute_reply.started": "2025-02-15T00:36:21.462387Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b0bafa-226e-46a5-b542-df136bef77bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T00:36:21.483428Z",
     "iopub.status.busy": "2025-02-15T00:36:21.482917Z",
     "iopub.status.idle": "2025-02-15T00:36:28.947888Z",
     "shell.execute_reply": "2025-02-15T00:36:28.946707Z",
     "shell.execute_reply.started": "2025-02-15T00:36:21.483385Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"trl-lib/tldr\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "603fa520-6c20-4682-8120-23774c96b3e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T00:36:28.949197Z",
     "iopub.status.busy": "2025-02-15T00:36:28.948865Z",
     "iopub.status.idle": "2025-02-15T00:36:28.958874Z",
     "shell.execute_reply": "2025-02-15T00:36:28.957803Z",
     "shell.execute_reply.started": "2025-02-15T00:36:28.949176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 116722\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b731ff-f91b-4caa-a169-4bbe3531772e",
   "metadata": {},
   "source": [
    "## GRPOConfig & GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfdcc69c-16bc-4cd0-84f3-637af5358e44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T00:36:28.959686Z",
     "iopub.status.busy": "2025-02-15T00:36:28.959481Z",
     "iopub.status.idle": "2025-02-15T00:36:28.978428Z",
     "shell.execute_reply": "2025-02-15T00:36:28.977353Z",
     "shell.execute_reply.started": "2025-02-15T00:36:28.959669Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa6b08e-4c2f-45ee-84c4-7df591a0dfab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T00:36:28.979440Z",
     "iopub.status.busy": "2025-02-15T00:36:28.979225Z",
     "iopub.status.idle": "2025-02-15T00:36:35.589093Z",
     "shell.execute_reply": "2025-02-15T00:36:35.588172Z",
     "shell.execute_reply.started": "2025-02-15T00:36:28.979423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-15 08:36:32,909] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/whaow/anaconda3/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2512be76-37df-4484-b5a7-8894b68d95d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T00:39:05.269887Z",
     "iopub.status.busy": "2025-02-15T00:39:05.269475Z",
     "iopub.status.idle": "2025-02-15T00:39:05.277213Z",
     "shell.execute_reply": "2025-02-15T00:39:05.275290Z",
     "shell.execute_reply.started": "2025-02-15T00:39:05.269854Z"
    }
   },
   "outputs": [],
   "source": [
    "from rich.pretty import pprint\n",
    "# pprint(GRPOConfig(output_dir='test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc13a52-802d-4939-b2c9-a1a13bfa3b30",
   "metadata": {},
   "source": [
    "- GRPOConfig\n",
    "    - num_generations=8,\n",
    "    - old\n",
    "        - per_device_train_batch_size=1, * gradient_accumulation_steps=8,\n",
    "            - per_device_train_batch_size * gradient_accumulation_steps * world_size ==> train_batch\n",
    "    - new: https://github.com/huggingface/trl/pull/2776#issue-2833772774\n",
    "        - per_device_train_batch_size:\n",
    "            - it now represents the number of generations per device.\n",
    "        - per_device_train_batch_size/num_generations * gradient_accumulation_steps\n",
    "            - per_device_train_batch_size/num_generations: prompts per device\n",
    "            - 也因此要求，per_device_train_batch_size 必须能被 num_generations 整除；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6f363-ba83-4244-889c-6bf103dc72a4",
   "metadata": {},
   "source": [
    "## running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92461b4-940c-4da6-9ddf-f581fdf33dd0",
   "metadata": {},
   "source": [
    "\n",
    "> dataset: 7473;\n",
    "\n",
    "- 单 4090:\n",
    "    ```\n",
    "    export CUDA_VISIBLE_DEVICES=0\n",
    "    python grpo_demo.py\n",
    "    ```\n",
    "    - 4/4 * 4 => 4,\n",
    "        - 1868;\n",
    "    - 13 小时；\n",
    "- 双 4090，ddp （accelerate map）\n",
    "    - (4/4 * 4 * 2) => 8;\n",
    "        - 934;\n",
    "    - < 10小时；\n",
    "- 双 4090，deepspeed stage 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ccea27-8abe-4e9a-90d2-6c844d7ba91a",
   "metadata": {},
   "source": [
    "## GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8c25b74-d4c5-483e-a28b-e649d7d2d3cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T00:36:35.670204Z",
     "iopub.status.busy": "2025-02-15T00:36:35.670020Z",
     "iopub.status.idle": "2025-02-15T00:36:35.675881Z",
     "shell.execute_reply": "2025-02-15T00:36:35.675014Z",
     "shell.execute_reply.started": "2025-02-15T00:36:35.670188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/grpo_visual.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bs = 2\n",
    "# G = 4\n",
    "Image(url='https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/grpo_visual.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c4896-c696-4e4f-b063-b70a2cd8b6fc",
   "metadata": {},
   "source": [
    "- GRPO is an **online learning**(on-policy) algorithm, meaning it improves iteratively by using the data generated by the trained model itself during training.\n",
    "- four main steps:\n",
    "    - Generating completions,\n",
    "        - At each training step, we sample a batch of prompts and generate a set of $G$ completions(`num_generations`) for each prompt (denoted as $o_i$).\n",
    "    - computing the advantage,\n",
    "        - $\\hat A_{i,t}=\\frac{r_i-\\mu(\\mathbf r)}{\\sigma(\\mathbf r)}$\n",
    "        - Outcome supervision provides the normalized reward at the end of each output $o_i$ and sets the advantages $\\hat A_{i,t}$ of all tokens in the output as the normalized reward\n",
    "    - **estimating** the KL divergence, (token-level see the figure)\n",
    "        - https://huggingface.co/docs/trl/main/en/grpo_trainer#estimating-the-kl-divergence\n",
    "        - `per_token_kl = torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1`\n",
    "    - and computing the loss.\n",
    "        - $\\pi_{ref}, (\\pi_{old}, \\pi_\\theta)$\n",
    "        - https://github.com/huggingface/trl/issues/2608\n",
    "```\n",
    "# x - x.detach() allows for preserving gradients from x\n",
    "per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)\n",
    "per_token_loss = -(per_token_loss - self.beta * per_token_kl)\n",
    "loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55557f0-c492-4ff2-b430-f2848b245b53",
   "metadata": {},
   "source": [
    "### reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b4006-f991-409b-aa10-bd993dcda587",
   "metadata": {},
   "source": [
    "```\n",
    "def reward_func(completions, **kwargs):\n",
    "    \"\"\"Reward function that gives higher scores to longer completions.\"\"\"\n",
    "    return [float(len(completion)) for completion in completions]\n",
    "```\n",
    "- completions\n",
    "    - `[bs, G]`\n",
    "- 20\n",
    "    - https://github.com/huggingface/trl/issues/2771"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c25db4-1d21-4e16-b003-341d8bbf85c1",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac22ab-7881-4dae-b4fc-133d65fa79ee",
   "metadata": {},
   "source": [
    "> qwen2.5 vocab size: 151936\n",
    "\n",
    "- prompts => tokenizer.apply_chat_template => model.generate\n",
    "    - prompt_completion_ids = prompt_ids + prompt_completion_ids\n",
    "    - rewards_per_func: (n_prompts, len(reward_funcs))\n",
    "    - ref_per_token_logps($\\pi_{ref}(q, o)$), per_token_logps ($\\pi_\\theta(q,o)$)\n",
    "        - completion token level\n",
    "            - selective_log_softmax(logits, index)\n",
    "                - logits.shape: (n_prompts, n_completion, n_vocab)\n",
    "                - index.shape: (n_prompts, n_complection)\n",
    "                - => (n_prompts, n_complection)\n",
    "$$\n",
    "\\exp(\\log{\\pi'}-\\log{\\pi})=\\frac{\\pi'}{\\pi}\n",
    "$$\n",
    "\n",
    "- 目前的实现只有 $\\pi_{\\theta}, \\pi_{ref}$，没有 $\\pi_{old}$\n",
    "    - https://github.com/huggingface/trl/issues/2608\n",
    "        - The policy model only has **a single update** following each exploration stage. (deepseekmath)\n",
    "        - $\\pi_\\theta$ 每次（rollout a group generations）只进行一次更新，而不是多次更新；\n",
    "            - 对应 `for step = 1, . . . , M do` (M == 1)\n",
    "    - $\\pi_{old}=\\pi_\\theta$\n",
    "    - `torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)`\n",
    "        - per_token_logps.detach() 不参与计算图的梯度计算；\n",
    "    - 没有用到 clip，只有 $\\frac{\\pi}{\\pi_{old}}A=1\\cdot A$（ratio * advantage）\n",
    "        - ratio = 1，一定在 $(1-\\epsilon, 1+\\epsilon)$ 之间的；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04501bf-23b9-40bf-89f1-516c57ae2722",
   "metadata": {},
   "source": [
    "### 显存分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc67110-bdd5-46d2-9100-042722fce6cc",
   "metadata": {},
   "source": [
    "- models, data: 从这两个角度分析算法流程以及可能的显存占用；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0db5b51-341b-4823-a41d-09b857adbe41",
   "metadata": {},
   "source": [
    "### PPO vs. GRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e618fc0-4125-4ae2-b93b-ce263a10af05",
   "metadata": {},
   "source": [
    "在 RLHF 中，我们需要把人类反馈或reward model 对整个序列的打分（例如，一次对话的最终质量分）融合到强化学习训练中。此外，为了让模型在训练时不要偏离参考策略（reference model）太远，我们常常还会引入一个基于 KL 的惩罚项。\n",
    "\n",
    "因此，在 PPO 里面，最关键的变化在于——如何构造每个 token（每个时间步）的奖励 $r_t$\n",
    " 。这往往通过下面两步完成："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1122455-2c19-4c43-b342-616a3de78303",
   "metadata": {},
   "source": [
    "## metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed2a9e-57f2-4ce3-bda1-9cd2b4f6b9bf",
   "metadata": {},
   "source": [
    "### training monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d41f7-bcb5-4aca-9965-e5134ab9224e",
   "metadata": {},
   "source": [
    "- You should rely mostly on the reward. And keep an eye on the generations (risk of reward hacking)\n",
    "    - https://github.com/huggingface/trl/issues/2703"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
