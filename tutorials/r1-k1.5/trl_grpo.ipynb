{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ff3aff-cb61-4d7d-8a9b-0b1d2de0bcaf",
   "metadata": {},
   "source": [
    "- deepseekmath (grpo)\n",
    "    - https://arxiv.org/abs/2402.03300\n",
    "- latest `trl` version\n",
    "    - https://huggingface.co/docs/trl/main/en/grpo_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53fadb6f-cf06-4442-b9f5-e7a2e1dd0a11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T00:36:21.462434Z",
     "iopub.status.busy": "2025-02-15T00:36:21.461864Z",
     "iopub.status.idle": "2025-02-15T00:36:21.480739Z",
     "shell.execute_reply": "2025-02-15T00:36:21.478984Z",
     "shell.execute_reply.started": "2025-02-15T00:36:21.462387Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c9d64-6c4e-4f44-9dc3-72a622740422",
   "metadata": {},
   "source": [
    "## RL roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23361af-1814-4509-b70a-8fa4a8b0ea19",
   "metadata": {},
   "source": [
    "- 基本概念: on-policy vs. off-policy\n",
    "    - 理解复杂的公式（$\\pi_\\theta,\\pi_{\\theta_{old}}, \\pi_{ref}$），理解计算过程；\n",
    "    - 对哪个概率分布进行采样获得数据；\n",
    "- GRPO <= PPO(CLIP) <= TRPO <= **PG** (policy gradient)\n",
    "    - PPO: GAE (Generalized **Advantage** Estimation), TD-error\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\pi_\\theta)=\\mathbb E_{\\tau \\sim (\\pi_\\theta, T)}\\left[\\sum_{t=0}^T\\nabla_\\theta \\log\\pi_\\theta(a_t|s_t)R(\\tau)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6f363-ba83-4244-889c-6bf103dc72a4",
   "metadata": {},
   "source": [
    "## grpo demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92461b4-940c-4da6-9ddf-f581fdf33dd0",
   "metadata": {},
   "source": [
    "\n",
    "> dataset: 7473;\n",
    "\n",
    "https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb#file-grpo_demo-py\n",
    "\n",
    "- 单 4090:\n",
    "    ```\n",
    "    export CUDA_VISIBLE_DEVICES=0\n",
    "    python grpo_demo.py\n",
    "    ```\n",
    "    - 4/4 * 4 => 4,\n",
    "        - 1868;\n",
    "    - 13 小时；\n",
    "- 双 4090，ddp （accelerate map）\n",
    "    - (4/4 * 4 * 2) => 8;\n",
    "        - 934;\n",
    "    - < 10小时；\n",
    "- 双 4090，\n",
    "    - deepspeed stage-2/3;\n",
    "    - fsdp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b731ff-f91b-4caa-a169-4bbe3531772e",
   "metadata": {},
   "source": [
    "### GRPOConfig & GRPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc13a52-802d-4939-b2c9-a1a13bfa3b30",
   "metadata": {},
   "source": [
    "- GRPOConfig\n",
    "    - num_generations=8,\n",
    "    - old\n",
    "        - per_device_train_batch_size=1, * gradient_accumulation_steps=8,\n",
    "            - per_device_train_batch_size * gradient_accumulation_steps * world_size ==> train_batch\n",
    "    - new: https://github.com/huggingface/trl/pull/2776#issue-2833772774\n",
    "        - per_device_train_batch_size:\n",
    "            - it now represents the number of generations per device.\n",
    "        - per_device_train_batch_size/num_generations * gradient_accumulation_steps\n",
    "            - per_device_train_batch_size/num_generations: prompts per device\n",
    "            - 也因此要求，per_device_train_batch_size 必须能被 num_generations 整除；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ccea27-8abe-4e9a-90d2-6c844d7ba91a",
   "metadata": {},
   "source": [
    "## GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8c25b74-d4c5-483e-a28b-e649d7d2d3cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T00:36:35.670204Z",
     "iopub.status.busy": "2025-02-15T00:36:35.670020Z",
     "iopub.status.idle": "2025-02-15T00:36:35.675881Z",
     "shell.execute_reply": "2025-02-15T00:36:35.675014Z",
     "shell.execute_reply.started": "2025-02-15T00:36:35.670188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/grpo_visual.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bs = 2\n",
    "# G = 4\n",
    "Image(url='https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/grpo_visual.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c4896-c696-4e4f-b063-b70a2cd8b6fc",
   "metadata": {},
   "source": [
    "- GRPO is an **online learning** algorithm, meaning it improves iteratively by using the data generated by the trained model itself during training.\n",
    "- four main steps:\n",
    "    - Generating completions,\n",
    "        - At each training step, we sample a batch of prompts and generate a set of $G$ completions(`num_generations`) for each prompt (denoted as $o_i$).\n",
    "    - computing the advantage,\n",
    "        - $\\hat A_{i,t}=\\frac{r_i-\\mu(\\mathbf r)}{\\sigma(\\mathbf r)}$\n",
    "        - Outcome supervision provides the normalized reward at the end of each output $o_i$ and sets the advantages $\\hat A_{i,t}$ of all tokens in the output as the normalized reward\n",
    "    - **estimating** the KL divergence, (token-level see the figure)\n",
    "        - https://huggingface.co/docs/trl/main/en/grpo_trainer#estimating-the-kl-divergence\n",
    "        - `per_token_kl = torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1`\n",
    "    - and computing the loss.\n",
    "        - $\\pi_{ref}, (\\pi_{\\theta_{old}}, \\pi_\\theta)$\n",
    "        - https://github.com/huggingface/trl/issues/2608\n",
    "```\n",
    "# x - x.detach() allows for preserving gradients from x\n",
    "per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)\n",
    "per_token_loss = -(per_token_loss - self.beta * per_token_kl)\n",
    "loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c25db4-1d21-4e16-b003-341d8bbf85c1",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b616923-41fd-4002-9abe-b7ba14e4d7e2",
   "metadata": {},
   "source": [
    "- grpo_trainer.py\n",
    "    - `_prepare_inputs()`\n",
    "    ```\n",
    "    return {\n",
    "        \"prompt_ids\": prompt_ids,\n",
    "        \"prompt_mask\": prompt_mask,\n",
    "        \"completion_ids\": completion_ids,\n",
    "        \"completion_mask\": completion_mask,\n",
    "        \"ref_per_token_logps\": ref_per_token_logps,\n",
    "        \"advantages\": advantages,\n",
    "    }\n",
    "    ```\n",
    "    - `compute_loss()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac22ab-7881-4dae-b4fc-133d65fa79ee",
   "metadata": {},
   "source": [
    "> qwen2.5 vocab size: 151936\n",
    "\n",
    "- prompts => tokenizer.apply_chat_template => model.generate\n",
    "    - prompt_completion_ids = prompt_ids + prompt_completion_ids\n",
    "    - rewards_per_func: (n_prompts, len(reward_funcs))\n",
    "    - ref_per_token_logps($\\pi_{ref}(q, o)$), per_token_logps ($\\pi_\\theta(q,o)$)\n",
    "        - completion token level\n",
    "            - selective_log_softmax(logits, index)\n",
    "                - logits.shape: (n_prompts, n_completion, n_vocab)\n",
    "                - index.shape: (n_prompts, n_complection)\n",
    "                - => (n_prompts, n_complection)\n",
    "$$\n",
    "\\exp(\\log{\\pi'}-\\log{\\pi})=\\frac{\\pi'}{\\pi}\n",
    "$$\n",
    "\n",
    "- 目前的实现只有 $\\pi_{\\theta}, \\pi_{ref}$，没有 $\\pi_{\\theta_{old}}$\n",
    "    - https://github.com/huggingface/trl/issues/2608\n",
    "        - The policy model only has **a single update** following each exploration stage. (deepseekmath)\n",
    "        - $\\pi_\\theta$ 每次（rollout a group generations）只进行一次更新，而不是多次更新；\n",
    "            - 对应 `for GRPO iteration = 1, . . . , 𝜇` (𝜇 == 1)\n",
    "    - $\\pi_{\\theta_{old}}=\\pi_\\theta$\n",
    "    - `torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)`\n",
    "        - per_token_logps.detach() 不参与计算图的梯度计算；\n",
    "    - 没有用到 clip，只有 $\\frac{\\pi_\\theta}{\\pi_{\\theta_{old}}}A=1\\cdot A$（ratio * advantage）\n",
    "        - ratio = 1，一定在 $(1-\\epsilon, 1+\\epsilon)$ 之间的；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed2a9e-57f2-4ce3-bda1-9cd2b4f6b9bf",
   "metadata": {},
   "source": [
    "### training monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d41f7-bcb5-4aca-9965-e5134ab9224e",
   "metadata": {},
   "source": [
    "- You should rely mostly on the reward. And keep an eye on the generations (risk of reward hacking)\n",
    "    - https://github.com/huggingface/trl/issues/2703"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
