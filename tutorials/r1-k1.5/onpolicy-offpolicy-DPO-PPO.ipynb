{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d724fb2-6a02-45f0-bf73-2da0c97b0124",
   "metadata": {},
   "source": [
    "### DPO vs. PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ae616-2d8b-41ac-866c-c582c8a52bb7",
   "metadata": {},
   "source": [
    "- It is learning directly from preferences vs. using RL update rules.\n",
    "- ppo\n",
    "    - gae based on the rewards $\\{r_{\\gt t}\\}$ and a learned value function $v_\\psi$\n",
    "    - By clipping the ratio, we ensure that we do not have a too large policy update because the current policy can't be too different from the older one. To do that, we have two solutions:\n",
    "        - TRPO (Trust Region Policy Optimization) uses KL divergence constraints outside the objective function to constrain the policy update. But this method is complicated to implement and takes more computation time.\n",
    "        - PPO clip probability ratio directly in the objective function with its Clipped surrogate objective function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
