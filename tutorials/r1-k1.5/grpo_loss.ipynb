{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db17d9d0-c075-4547-a1f0-f02e92543761",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T02:05:21.078819Z",
     "iopub.status.busy": "2025-03-23T02:05:21.075646Z",
     "iopub.status.idle": "2025-03-23T02:05:21.099527Z",
     "shell.execute_reply": "2025-03-23T02:05:21.097614Z",
     "shell.execute_reply.started": "2025-03-23T02:05:21.078727Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fad3be3-6387-477c-8265-d6717e5ec954",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T02:05:22.118954Z",
     "iopub.status.busy": "2025-03-23T02:05:22.118242Z",
     "iopub.status.idle": "2025-03-23T02:05:24.334911Z",
     "shell.execute_reply": "2025-03-23T02:05:24.332902Z",
     "shell.execute_reply.started": "2025-03-23T02:05:22.118888Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/envs/verl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d46f7e-77aa-4526-8d1d-d3170fd3b758",
   "metadata": {},
   "source": [
    "- DAPO:\n",
    "    - https://dapo-sia.github.io\n",
    "- Dr. GRPO\n",
    "    - https://github.com/sail-sg/understand-r1-zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74247e1a-2f69-4f1f-8116-087dab0dffd0",
   "metadata": {},
   "source": [
    "## loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f288163-9a7f-4262-bf8a-34b026d14660",
   "metadata": {},
   "source": [
    "> loss 为 0 为什么还可以反向传播，更新梯度；\n",
    "\n",
    "- loss 为 0，不意味着 gradient 为 0\n",
    "    - $f(w)=(w-1)^2-1$，在 $w=0$ 时，$f(w)=0$，但其实其 gradient 为 -2\n",
    "        - 梯度 * 学习率 才是 learning 的本质；\n",
    "    - $w-\\eta\\cdot g=0-(0.1*-2)=0.2$\n",
    "- loss 不再是一个好的 monitor 指标，而是 reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c7adb-ac9e-4280-84a8-c2adb3b28b91",
   "metadata": {},
   "source": [
    "### loss 为 0 不代表 gradient 为 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c257272-b6ac-4d84-8106-85a92a092ca6",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/trl/issues/2608#issuecomment-2609844003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650bd366-b8a6-42fc-b721-bf4263f93560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T02:05:27.247678Z",
     "iopub.status.busy": "2025-03-23T02:05:27.246967Z",
     "iopub.status.idle": "2025-03-23T02:05:27.265370Z",
     "shell.execute_reply": "2025-03-23T02:05:27.264318Z",
     "shell.execute_reply.started": "2025-03-23T02:05:27.247648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for x - x: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 情况1: x - x (梯度为0)\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "y1 = x - x  \n",
    "y1.backward()  # 反向传播计算梯度\n",
    "print(\"Gradient for x - x:\", x.grad.item())  # 输出 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d233e3-9427-4ade-a545-c0ea55bea7d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T02:05:28.734922Z",
     "iopub.status.busy": "2025-03-23T02:05:28.734313Z",
     "iopub.status.idle": "2025-03-23T02:05:28.747560Z",
     "shell.execute_reply": "2025-03-23T02:05:28.745339Z",
     "shell.execute_reply.started": "2025-03-23T02:05:28.734851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for x - x.detach(): 1.0\n"
     ]
    }
   ],
   "source": [
    "# 清除梯度，准备下一个示例\n",
    "x.grad.zero_()\n",
    "\n",
    "# 情况2: x - x.detach() (梯度为1)\n",
    "y2 = x - x.detach()  # 分离第二个x，使其视为常数\n",
    "y2.backward()  # 反向传播计算梯度\n",
    "print(\"Gradient for x - x.detach():\", x.grad.item())  # 输出 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b010b58b-2c2f-4dc3-8e8e-67e6584c2353",
   "metadata": {},
   "source": [
    "### loss = $\\beta kl$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11afa9d-02ac-4d58-8239-80a3b9cc2725",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/open-r1/issues/239#issuecomment-2646297851\n",
    "\n",
    "- trl grpo\n",
    "    - $\\beta = 0.04$（default，`GRPOConfig`）\n",
    "    - 这个值其实是比较大的，math 用 0.001？？\n",
    "- 抛开 kl\n",
    "    - 一个 prompt 多个 generations（为一个 group）\n",
    "        - 每个 generation 对应的 loss = -advantage (likelihood ratio = 1, $\\pi_\\theta=\\pi_{\\theta_{old}}$)\n",
    "    - 一个 group 的 mean loss = - mean advantage = 0\n",
    "- kl 的位置\n",
    "    - 定义在 advantage 计算 reward 时\n",
    "    - 定义在外部\n",
    "    - grpo 原始公式是定义在外部的；\n",
    "        - the GRPO implementation does not include the KL-divergence as part of the reward function. Instead, it directly incorporates the KL-divergence into the loss function, arguing that this approach simplifies the computation and avoids unnecessary complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8c492b-0f18-48bf-82f0-345169256f49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T02:41:31.937965Z",
     "iopub.status.busy": "2025-03-23T02:41:31.937232Z",
     "iopub.status.idle": "2025-03-23T02:41:31.958449Z",
     "shell.execute_reply": "2025-03-23T02:41:31.956791Z",
     "shell.execute_reply.started": "2025-03-23T02:41:31.937894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/grpo_demo.jpeg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/grpo_demo.jpeg', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b6f39-5fda-41a1-93b1-5fc97578f0b8",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{J}_{GRPO}(\\theta) = \\mathbb{E}_{q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{old}}(O|q)} \\left[ \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\min \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})} \\hat{A}_{i,t}, \\text{clip} \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\\varepsilon, 1+\\varepsilon \\right) \\hat{A}_{i,t} \\right) - \\beta D_{KL} (\\pi_\\theta || \\pi_{ref}) \\right]\n",
    "$$\n",
    "\n",
    "- first averaging the losses by token within each sample and then aggregating the losses across samples.\n",
    "    - each sample is assigned an equal weight in the final loss computation\n",
    "    - 对比看下 DAPO 的公式（12）\n",
    "- If you are using the GRPO trainer then the old policy is in effect updated every step, this means you just use a detached version of the current policy.\n",
    "    - 公式中的 $\\pi_{\\theta_{old}}$ 是 $\\pi_\\theta$ 的 detach 版（不参与计算图，视为常数）；\n",
    "    - $r=\\frac{\\pi_\\theta}{\\pi_{\\theta_{old}}}=1$,\n",
    "    - $\\text{clip}(1, 1-\\epsilon, 1+\\epsilon)=1$\n",
    "- $\\hat A_{i,t}=\\tilde r_i=\\frac{r_i-\\mu}{\\sigma}$ (z score) （token 级别的 adv = output 级别的 reward 组内 z-score 而来）\n",
    " \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathcal{J}_{GRPO}(\\theta)&= \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\min \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})} \\hat{A}_{i,t}, \\text{clip} \\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\\varepsilon, 1+\\varepsilon \\right) \\hat{A}_{i,t} \\right) - \\beta D_{KL} (\\pi_\\theta || \\pi_{ref}) \\\\\n",
    "&=\\frac1G\\sum_i^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\hat A_{i,t} -\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=\\frac1G\\sum_i^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\hat A_i -\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=\\frac1G\\sum_i^G\\frac1{|o_i|} {|o_i|}\\cdot \\hat A_i -\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=\\frac1G\\sum_i^G\\hat A_i-\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=\\frac1G\\sum_i^G\\frac{r_i-\\mu}{\\sigma}-\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=\\frac{\\sum_i r_i-G\\mu}{G}-\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&= 0 -\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\\\\\n",
    "&=-\\frac1G\\sum_{i=1}^G\\frac1{|o_i|}\\sum_{t=1}^{|o_i|}\\beta D_{kl}[\\pi_\\theta|\\pi_{ref}]\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bd2d9-8259-455e-899a-8736294a3f6d",
   "metadata": {},
   "source": [
    "### gradients of GRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605d8934-aebc-46a4-8e07-348a4fd1cc9c",
   "metadata": {},
   "source": [
    "$$\n",
    "f'(x)=f(x)\\nabla\\log f(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09d821b-c712-44d3-8bcb-bb167f5adfb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T01:51:31.506258Z",
     "iopub.status.busy": "2025-03-23T01:51:31.505538Z",
     "iopub.status.idle": "2025-03-23T01:51:31.520347Z",
     "shell.execute_reply": "2025-03-23T01:51:31.518373Z",
     "shell.execute_reply.started": "2025-03-23T01:51:31.506189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/grpo_grad.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/grpo_grad.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd13491-9d0e-489e-b0f6-636f568b17a7",
   "metadata": {},
   "source": [
    "- For example for GRPO, if all outputs $\\{o_i\\}^G_{i=1}$ of a particular prompt are correct and receive the same reward 1, the resulting advantage for this group is zero. A zero advantage results in no gradients for policy\n",
    "updates, thereby reducing sample efficiency.\n",
    "- deepseelmath disscussion 部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84e4114-5d26-461e-8f62-a834b6d59fff",
   "metadata": {},
   "source": [
    "### token-level pg loss (DAPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26839ad-f787-4e89-b108-6dbee10e088e",
   "metadata": {},
   "source": [
    "- grpo: generation-level loss, dapo: token-level pg loss\n",
    "    - grpo: 先部分（generation）去平均，再在 group 级别取平均\n",
    "    - dapo: group 里，所有的 generations，所有的tokens 取平均\n",
    "- ga (gradient accumulation)\n",
    "    - https://unsloth.ai/blog/gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f273a0f-518e-45b9-ad7b-faa4457f48ad",
   "metadata": {},
   "source": [
    "## Dr. GRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cf4c68-fe92-4d43-b1f7-702390506f6f",
   "metadata": {},
   "source": [
    "$$\n",
    "A_i=R_i-\\frac1N\\sum_{j=1}^N R_j\n",
    "$$\n",
    "\n",
    "- $R_i=\\theta+\\epsilon_i$，带入上式得\n",
    "    - $A_i=\\theta+\\epsilon_i-\\frac1N\\sum_j (\\theta+\\epsilon_i)=\\epsilon_i-\\frac1N\\sum \\epsilon_j$\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathbb E[A_i|\\epsilon_i]&=\\mathbb E [\\epsilon_i - \\frac1N\\sum\\epsilon_j | \\epsilon_i]\\\\\n",
    "&=\\epsilon_i - \\frac1N\\epsilon_i-\\frac1N\\sum_{j\\neq i}^N 0\\\\\n",
    "&=\\frac{N-1}N\\epsilon_i\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181d447-04d0-4939-a50d-721bf0d27b65",
   "metadata": {},
   "source": [
    "\n",
    "## per_device_train_batch_size & num_generations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48000cc9-d5bc-44b6-aecc-38280ef99479",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/trl/pull/2776\n",
    "\n",
    "- (`num_processes * per_device_batch_size`) must be divisible by `G`.\n",
    "    - `per_device_batch_size` 刻画的是 gpu device 粒度 generations 的数量\n",
    "    - `num_processes` 是 gpu 进程的数量；\n",
    "    - `num_processes * per_device_batch_size` / `G`: prompts 吞吐量\n",
    "- https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py#L571-L598\n",
    "    - ensures each prompt is repeated across multiple processes. This guarantees that identical prompts are distributed to different GPUs, allowing rewards to be computed and normalized correctly within each prompt group. Using the same seed across processes ensures consistent prompt assignment, preventing discrepancies in group formation.\n",
    "    - repeats the batch multiple times to allow reusing generations across multiple updates. Refer to _prepare_inputs to see how the generations are stored and reused.\n",
    "    - In the following figure, the values are the prompt indices. The first row shows the first sampled batch, the\n",
    "second row shows the second sampled batch, and so on.\n",
    "    - 3 个 gpus，num_generations = 3，per_device_train_batch_size = 4\n",
    "        - 3*4 / 3  = 4\n",
    "\n",
    "    |      | GPU0   | GPU1       | GPU2     |\n",
    "    |------|--------|------------|----------|\n",
    "    | P0   | P00    | P01        | P02      |\n",
    "    | P1   | P10    | P11        | P12      |\n",
    "    | P2   | P20    | P21        | P22      |\n",
    "    | P3   | P30    | P31        | P32      |\n",
    "\n",
    "    - 进一步还考虑到了 `grad_accum` = 3，累加 batch forward，统一 backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb424c-e2f4-48a7-b30d-3e32703d5fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "verl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
