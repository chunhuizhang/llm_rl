{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497149c3-ab14-452e-a830-0b26bc2c9c9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T09:23:53.965762Z",
     "iopub.status.busy": "2025-04-13T09:23:53.965114Z",
     "iopub.status.idle": "2025-04-13T09:23:53.985771Z",
     "shell.execute_reply": "2025-04-13T09:23:53.983941Z",
     "shell.execute_reply.started": "2025-04-13T09:23:53.965711Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e454b8-661a-4b84-910e-c6447e36fe95",
   "metadata": {},
   "source": [
    "### on-policy vs. off-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0458a01-6bf0-46de-a5db-6d2644a3efab",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/700149886\n",
    "\n",
    "- on policy即时生成的数据，off policy几代之前的数据\n",
    "    - sft 是 off policy；\n",
    "- 以 PPO 为代表的 On Policy 路线\n",
    "    - 凡是需要 LLM 在训练过程中做 generation 的方法就是 On Policy，反之为 Off Policy。\n",
    "- 以 DPO 为代表的 Off Policy 路线"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d724fb2-6a02-45f0-bf73-2da0c97b0124",
   "metadata": {},
   "source": [
    "### DPO vs. PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ae616-2d8b-41ac-866c-c582c8a52bb7",
   "metadata": {},
   "source": [
    "- It is learning directly from preferences vs. using RL update rules.\n",
    "- ppo\n",
    "    - gae based on the rewards $\\{r_{\\gt t}\\}$ and a learned value function $v_\\psi$\n",
    "    - By clipping the ratio, we ensure that we do not have a too large policy update because the current policy can't be too different from the older one. To do that, we have two solutions:\n",
    "        - TRPO (Trust Region Policy Optimization) uses KL divergence constraints outside the objective function to constrain the policy update. But this method is complicated to implement and takes more computation time.\n",
    "        - PPO clip probability ratio directly in the objective function with its Clipped surrogate objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb06655-a049-408e-9477-fe33e6aa21d2",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7284b4fe-c4a7-404b-99b6-50d9b3c2f8e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T09:23:57.812542Z",
     "iopub.status.busy": "2025-04-13T09:23:57.811852Z",
     "iopub.status.idle": "2025-04-13T09:23:57.831171Z",
     "shell.execute_reply": "2025-04-13T09:23:57.829342Z",
     "shell.execute_reply.started": "2025-04-13T09:23:57.812493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../imgs/GAE.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../imgs/GAE.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7cc7b-884f-470a-be9f-4676da36d803",
   "metadata": {},
   "source": [
    "- 标准的 ppo surrogate loss\n",
    "    - 没有 kl div；\n",
    "    - $\\text{clip}(r_t,1-\\epsilon,1+\\epsilon)$ 确保 ratio 不会超出 $(1-\\epsilon,1+\\epsilon)$\n",
    "        - $r_t=\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$\n",
    "            - $r_t>1$: 相比较于旧的策略，当前策略会给该动作更高的概率；\n",
    "            - $r_t<1$：相比较于旧的策略，当前策略会给该动作更低的概率（降低该动作出现的概率）；\n",
    "    - 以阻止 large policy update，making training more stable；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
