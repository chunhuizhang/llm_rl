{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dbc9645-3add-45a6-b39c-063ef4570f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e2746-660b-4f1d-8c46-c197169b7568",
   "metadata": {},
   "source": [
    "- 输入是 state，输出是各 action 对应的 `Q(s,a)`\n",
    "    - Model-free\n",
    "- 如何转换为 policy，应用到交互中\n",
    "    -  ε-greedy (epsilon-greedy)\n",
    " \n",
    "$$\n",
    "a = \\arg\\max(Q(s, a))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc53f6-387c-4bfe-8472-c467274af075",
   "metadata": {},
   "source": [
    "### DVN? => Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a1db29-1f6c-4bda-a89a-dc403e499087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://incompleteideas.net/book/ebook/figtmp34.png\" width=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='http://incompleteideas.net/book/ebook/figtmp34.png', width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5909e0a-c48c-4edc-aadb-4885aa8b25ab",
   "metadata": {},
   "source": [
    "- 假如 DVN 学到的是 $V(s)$\n",
    "    - 它不直接提供任何关于“哪个动作更好”的信息。为了做出决策，你必须能够“向前看一步”，弄清楚每个动作会把你带到哪个新状态 s'，然后比较那些新状态的价值 V(s')。\n",
    "    - 这个“**向前看**”的过程需要一个环境模型 (Environment Model，model-based)。这个模型需要能回答这个问题：T(s, a) -> s'，即在状态 s 执行动作 a，下一个状态 s' 是什么？\n",
    "\n",
    "$$\n",
    "a = \\arg\\max_a [ R(s,a) + γ * V(s') ] \n",
    "$$\n",
    "\n",
    "- DVN 不像 DQN 可以独立存在，而是跟 policy network 一起 work，所谓的 Actor-Critic\n",
    "    - $A(s,a) ≈ r + γV(s') - V(s)$\n",
    "        - “在状态 s 下，执行动作 a 到底比**通常情况（平均水平）**好多少？”\n",
    "    - 学习 V(s) 的 Critic 网络不直接用于决策，而是用于指导 Actor 网络进行更有效的学习。这结合了 DQN (价值学习) 和策略梯度 (直接学习策略) 的优点。像 A2C、A3C、PPO 等都是非常成功的 Actor-Critic 算法。\n",
    "- actor-critic\n",
    "    - critic：价值网络($v(s)$)，如果仅有价值网络（策略网络？？），容易贪心，陷入局部左右值；\n",
    "        - critic 单步打分，reward model：最终裁决；\n",
    "        - critic 网络的评价（estimated advantage，优势函数）提供了 actor 的更新方向；\n",
    "        - 优势函数：实际期望 - 预期期望（评价家预测）\n",
    "            - 大于 0，这个策略就值得鼓励；\n",
    "    - actor：策略网络($\\pi(a|s)$)，概率输出，增加探索的可能；\n",
    "    - 两者都需要不断训练；\n",
    "- 仅有 actor 模型（REINFORCE）的问题\n",
    "    - 高方差 (High Variance)：学习信号极其不稳定，导致训练过程像坐过山车，收敛缓慢且困难。\n",
    "    - 样本效率低 (Low Sample Efficiency)：必须等待整个回合（episode）结束后才能进行学习，浪费了大量经验。\n",
    "        - 信用分配不当和学习缓慢"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
