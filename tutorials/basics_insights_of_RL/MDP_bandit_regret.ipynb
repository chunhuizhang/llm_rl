{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "883a8a03-d806-4bdb-9104-b8833c6614d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbb1bc-9482-487f-80f8-2e82686dcdf6",
   "metadata": {},
   "source": [
    "> 强化学习是MDP，必须multi-step，把整个回答写成一个step不是RL\n",
    "\n",
    "- fixed horizon MDP setting in RL.\n",
    "    - 一条轨迹最终的回报很高，并不能代表这条轨迹中的每一个动作都是好的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf2afa5-91a8-449b-bcf3-e19675aafac4",
   "metadata": {},
   "source": [
    "### Markov chain => MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bff6d13-e087-4658-b716-d79d3b2c415a",
   "metadata": {},
   "source": [
    "- Markov chain: $\\mathcal M=\\{\\mathcal S, \\mathcal T\\}$\n",
    "    - $\\mathcal S$: state space, $\\mathcal T$: transition operator\n",
    "    - 令 $\\mu_{t,i}=p(s_t=i)$, $\\vec \\mu_t$ 是一个概率向量\n",
    "    - 令 $\\mathcal T_{i,j}=p(s_{t+1}=j|s_t=i)$\n",
    "        - 状态离散时，是一个矩阵\n",
    "    - 则有 $\\vec \\mu_{t+1}=\\mathcal T\\vec\\mu_t$\n",
    "- Markov chain 不构成 mdp，因为不包含动作\n",
    "- MDP：$s,a,r(s,a),p(s'|s,a)$\n",
    "    - $\\mathcal M=\\{\\mathcal S,\\mathcal A, \\mathcal T, r\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f964ecd6-3d2a-46b8-ba96-fd00bc56df00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./figs/mdp.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./figs/mdp.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a8bda0-b6ae-4f4b-9851-131a47b297a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### MDP as a DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa1ecada-5264-4c23-906e-0427f0057c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./figs/mdp_dag.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./figs/mdp_dag.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a593a5-2512-43cd-8a40-fd6e9af2d4bc",
   "metadata": {},
   "source": [
    "### UCB: 探索即奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c89daed-0fbc-40bc-baf7-dcfa7f5bf25e",
   "metadata": {},
   "source": [
    "- 多臂老虎机\n",
    "    - 没有状态的概念，也没有状态转移的概念；\n",
    "    - $k$ 个不同的动作（拉哪个臂）\n",
    "        - 每个动作 $a_k$ 都对应一个未知的奖励分布 $\\mathbb P_k$\n",
    "    - 每次选择一个动作，观察对应的奖励 $r_t\\sim \\mathbb P_{a_t}$\n",
    "- UCB\n",
    "    - $UCB(a)$ 动作 $a$ 的置信上界值；\n",
    "    - $t$ 总实验次数；$\\ln t$：随着时间 $t$ 的增加，$\\ln t$ 的增长会变慢；（即随着时间的推移，探索的紧迫性降低）\n",
    "$$\n",
    "UCB(a)=\\bar X_a+c\\cdot \\sqrt\\frac{\\ln t}{N_a}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
