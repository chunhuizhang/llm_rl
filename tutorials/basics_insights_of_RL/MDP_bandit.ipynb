{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eabbb1bc-9482-487f-80f8-2e82686dcdf6",
   "metadata": {},
   "source": [
    "> 强化学习是MDP，必须multi-step，把整个回答写成一个step不是RL\n",
    "\n",
    "- fixed horizon MDP setting in RL.\n",
    "    - 一条轨迹最终的回报很高，并不能代表这条轨迹中的每一个动作都是好的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a593a5-2512-43cd-8a40-fd6e9af2d4bc",
   "metadata": {},
   "source": [
    "### UCB: 探索即奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c89daed-0fbc-40bc-baf7-dcfa7f5bf25e",
   "metadata": {},
   "source": [
    "- 多臂老虎机\n",
    "    - 没有状态的概念，也没有状态转移的概念；\n",
    "    - $k$ 个不同的动作（拉哪个臂）\n",
    "        - 每个动作 $a_k$ 都对应一个未知的奖励分布 $\\mathbb P_k$\n",
    "    - 每次选择一个动作，观察对应的奖励 $r_t\\sim \\mathbb P_{a_t}$\n",
    "- UCB\n",
    "    - $UCB(a)$ 动作 $a$ 的置信上界值；\n",
    "    - $t$ 总实验次数；$\\ln t$：随着时间 $t$ 的增加，$\\ln t$ 的增长会变慢；（即随着时间的推移，探索的紧迫性降低）\n",
    "$$\n",
    "UCB(a)=\\bar X_a+c\\cdot \\sqrt\\frac{\\ln t}{N_a}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "verl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
