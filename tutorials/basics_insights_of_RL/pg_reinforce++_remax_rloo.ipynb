{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e6b590-16c4-4ccf-8e13-a9ff771b8c0a",
   "metadata": {},
   "source": [
    "- $G_t = \\sum_{k=t+1}^T γ^{k-t} r_k$\n",
    "- $r_t(θ) = \\frac{π_θ(a_t|s_t)}{π_{θ_\\text{old}}(a_t|s_t)}$\n",
    "- REINFORCE\n",
    "    - $∇θ J(θ) = E_{τ∼π_θ}[ ∇_θ \\log π_θ(a_t|s_t) G_t ]$\n",
    "        - 这里的 $G_t$ 是从蒙特卡洛采样中直接得到的，方差很高。\n",
    "- PPO\n",
    "    - $L^{PPO}(θ, φ) = E_t [ L_t^{CLIP}(θ) - c_1 L_t^{VF}(φ) + c_2 S(π_θ) ]$\n",
    "        - $L_t^{CLIP}(θ) = \\min( r_t(θ) A_t, \\text{clip}(r_t(θ), 1-ϵ, 1+ϵ) A_t )$\n",
    "            - $A_t^{GAE} = ∑_{l=0}^{T-t-1} (γλ)^l δ_{t+l}$ ，其中 $δ_t = r_t + γV_φ(s_{t+1}) - V_φ(s_t)$\n",
    "        - 价值函数损失 (Value Function Loss)： $L_t^{VF}(φ) = (V_φ(s_t) - G_t^{target})^2$ 用于更新 Critic 网络自身。\n",
    "        - 熵奖励 (Entropy Bonus) S：鼓励探索。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cdaad9-48d2-4c8e-bfec-509d17fa4a3d",
   "metadata": {},
   "source": [
    "### REINFORCE++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7bfb5-4ade-4b36-a4c8-1d4b5130c4c6",
   "metadata": {},
   "source": [
    "- https://hijkzzz.notion.site/reinforce-plus-plus\n",
    "- 在不引入 Critic 网络的情况下，借鉴 PPO 的稳定性技术，从而实现比 PPO 更高效、比 REINFORCE 更稳定的训练。\n",
    "- 核心思想：做减法和加法。\n",
    "    - 减法（丢弃的）：它丢弃了 Critic 网络。这是它计算效率提升的根本原因。\n",
    "    - 加法（借鉴的）：它从 PPO 和其他 RLHF 实践中“借”来了多个关键的优化技巧：\n",
    "\n",
    "$$\n",
    "L^{REINFORCE++}(θ) = E_t [\\min(r_t(θ) A_t^{norm}, clip(r_t(θ), 1-ϵ, 1+ϵ) A_t^{norm} ) ]\n",
    "$$\n",
    "\n",
    "这里的关键区别在于 $A_t^{norm}$ 的计算：\n",
    "\n",
    "- 定义单步奖励：首先，它在奖励中直接融入 KL 惩罚。根据论文，这个奖励 $r(x,y)$ 只在序列末端（EOS token）给出。\n",
    "    - $r'(s_t, a_t) = I(s_t=\\text{EOS})r(x,y) - β KL(π_θ(·|s_t) || π_{SFT}(·|s_t))$\n",
    "    - `[小惩罚, 小惩罚, ..., 小惩罚, 最终大奖+小惩罚]。`\n",
    "- 计算“伪优势”：它计算一个从当前步到结尾的累积奖励作为“伪优势” $A_t$。\n",
    "    - $A_t(s_t, a_t) = r(x,y) - β ∑_{i=t}^T KL(π_θ(·|s_i) || π_{SFT}(·|s_i))$\n",
    "        - $A_t = r'(s_t, a_t) + r'(s_{t+1}, a_{t+1}) + ... + r'(s_T, a_T)$ ($γ=1$)\n",
    "    - 在标准的强化学习中，“优势函数 (Advantage)”的定义是 $A(s,a) = Q(s,a) - V(s)$，即某个动作的价值减去当前状态的平均价值。它衡量的是一个动作比“平均水平”好多少。 而 REINFORCE++ 里的 $A_t$ 并没有减去一个基线（Baseline）或价值函数 $V(s)$，它就是对未来奖励的原始累加。所以我们称之为“伪优势”，它只是一个中间计算值。\n",
    "- 进行归一化：这是替代 Critic 的关键步骤。对一个批次（batch）内计算出的所有 $A_t$ 进行 Z-score 归一化。\n",
    "    - $A_t^{norm} = \\frac{A_t - μ_A}{σ_A + ε_{tiny}}$ 其中 μ_A 和 σ_A 分别是当前批次中所有 A_t 的均值和标准差。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ec110-b1a7-4c19-884a-0f6824a9ec19",
   "metadata": {},
   "source": [
    "| 特性 | REINFORCE | PPO (Proximal Policy Optimization) | REINFORCE++ |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **基本思想** | 基础策略梯度 | Actor-Critic，带信任域约束 | 借鉴了 PPO 技巧的无 Critic 策略梯度 |\n",
    "| **方差缩减** | 无（或仅有简单基线） | **Critic 网络**计算优势函数 `A(s,a)` | **优势归一化** (Z-score Normalization) |\n",
    "| **策略更新约束** | 无 | **PPO-Clip 损失函数** | **PPO-Clip 损失函数** |\n",
    "| **核心组件** | 策略网络 (Actor) | 策略网络 (Actor) + **价值网络 (Critic)** | 策略网络 (Actor) |\n",
    "| **计算复杂度** | 低 | 高 | 中等（显著低于 PPO） |\n",
    "| **稳定性** | 低 | 高 | 中等（显著高于 REINFORCE） |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2367a975-3949-479a-85e3-3038c4447326",
   "metadata": {},
   "source": [
    "### norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069bb92f-d570-4b91-90b4-3bf4121a7b72",
   "metadata": {},
   "source": [
    "让我们参考论文中的超参数来构建一个真实的场景：\n",
    "- Rollout Batch Size: 256\n",
    "- Samples per Prompt: 4\n",
    "\n",
    "这意味着在一个“Rollout”阶段，我们有 256 / 4 = 64 个不同的 prompt。\n",
    "\n",
    "- 对于 Prompt 1，生成了：回答1A, 回答1B, 回答1C, 回答1D。\n",
    "- 对于 Prompt 2，生成了：回答2A, 回答2B, 回答2C, 回答2D。\n",
    "- ...\n",
    "- 对于 Prompt 64，生成了：回答64A, 回答64B, 回答64C, 回答64D。\n",
    "\n",
    "256 * 200 = 51200 个 A_t, \n",
    "- “拉平”数据: `All_A_values = [A_{1A, 1}, A_{1A, 2}, ..., A_{1A, 150}, A_{1B, 1}, ..., A_{64D, 210}]`\n",
    "- 计算统计量: 对这个包含 51,200 个数值的 All_A_values 列表，我们计算它的总均值 $μ_A$ 和总标准差 $σ_A$。\n",
    "    - 这个 $μ_A$ 非常有意义，它代表了在当前这个批次的所有情境下（不同 prompt，不同回答），一个 token 平均能带来的未来期望收益。它是一个动态的、基于当前批次数据的全局基线。\n",
    " \n",
    "计算最终的 A_t^{norm}\n",
    "- 对“回答1A”的 token 1: $A_{1A,1}^{norm} = \\frac{A_{1A,1} - μ_A}{σ_A}$\n",
    "- 对“回答64D”的 token 210: $A_{64D,210}^{norm} = \\frac{A_{210,64D} - μ_A}{σ_A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a245f-c137-46b5-a97f-4895f603aac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
