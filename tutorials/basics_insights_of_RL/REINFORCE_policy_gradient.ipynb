{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5249be-3b8e-4d96-96ad-90f4f92eb4f5",
   "metadata": {},
   "source": [
    "- https://sage-kite-7e1.notion.site/reinforce-plus-plus\n",
    "- https://www.bilibili.com/video/BV1eg4y1s7TN\n",
    "    - [pytorch 强化学习] 13 基于 pytorch 神经网络实现 policy gradient（REINFORCE）求解 CartPole\n",
    "- https://medium.com/@2468086464/understanding-reinforcement-learning-from-human-feedback-rlhf-theory-and-the-mechanism-ef45485a5070"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edea97f-1b7a-4d25-91d8-ed3a461ecbc2",
   "metadata": {},
   "source": [
    "## PG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5a6582-7e8a-4a36-a7d0-e5bdffdf58cb",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb E_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)]\n",
    "$$\n",
    "\n",
    "- 求其 gradient\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\sum_{\\tau}R(\\tau)\\nabla p_\\theta(\\tau)&=\\sum_\\tau R(\\tau) p_\\theta(\\tau)\\nabla \\log p_\\theta(\\tau)\\\\\n",
    "&=\\mathbb E_{\\tau\\sim p_\\theta(\\tau)}R(\\tau)\\nabla \\log p_\\theta(\\tau)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- 对 $\\nabla \\log p_\\theta(\\tau)$ 进行展开\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\nabla \\log p_\\theta(\\tau)&=\\nabla \\log\\left(p(s_1)\\prod p(a_t|s_t)\\prod p(s_{t+1}|s_t,a_t)\\right)\\\\\n",
    "&=\\nabla \\left(\\log p(s_1)+\\sum \\log p(a_t|s_t)+\\sum \\log p(s_{t+1}|s_t,a_t)\\right)\\\\\n",
    "&=\\sum_{t=1}^T\\nabla \\log p(a_t|s_t)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- 因此\n",
    "$$\n",
    "\\begin{split}\n",
    "\\sum_{\\tau}R(\\tau)\\nabla p_\\theta(\\tau)&=\\sum_\\tau R(\\tau) p_\\theta(\\tau)\\nabla \\log p_\\theta(\\tau)\\\\\n",
    "&=\\mathbb E_{\\tau\\sim p_\\theta(\\tau)}R(\\tau)\\nabla \\log p_\\theta(\\tau)\\\\\n",
    "&\\approx\\frac1N\\sum_{n=1}^NR(\\tau^n)\\nabla \\log p_\\theta(\\tau^n)\\\\\n",
    "&=\\frac1N\\sum_{n=1}^N\\sum_{t=1}^{T_n}R(\\tau^n)\\nabla \\log p_\\theta(a_t^n|s_t^n)\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986bd2a9-e267-446d-ad6e-7b2d26e2bf3e",
   "metadata": {},
   "source": [
    "### PG vs. REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463659d2-07d8-4a5c-9fa6-8c7293cf7c70",
   "metadata": {},
   "source": [
    "> REINFORCE is a crutial and simple PG method in RL designed to maximize expected cumulative rewards through direct PO (policy optimization).\n",
    "> > **RE**ward **I**ncrement = **N**onnegative **F**actor × **O**ffset **R**einforcement × **C**haracteristic **E**ligibility.”\n",
    "\n",
    "- discounted cumulative future reward\n",
    "    - $G_t=\\sum_{k=t+1}^{T}\\gamma^{k-t}r_k$\n",
    "    - $r_t$: the immediate reward at time step $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dfa5dc-9a1b-43db-9ad0-65f43f860bf9",
   "metadata": {},
   "source": [
    "$$\\nabla_\\theta J(\\theta)=\\mathbb E_{\\tau\\sim\\pi_\\theta}[G_t\\nabla_\\theta \\log\\pi_\\theta(A_t|S_t)]$$\n",
    "\n",
    "- $G(\\tau)\\rightarrow G_t$\n",
    "- PG: $G(\\tau)=r_0+\\gamma r_1+\\gamma^2 r_2+\\cdots=\\sum_{t=0}^T\\gamma^tr_t$\n",
    "    - $G(\\tau)\\nabla_\\theta\\log\\pi_\\theta(A_t|S_t)$，无论在哪个时刻 $t$，我们始终都会使用固定不变的权重 $G(\\tau)$ 来增加或减少采取行动 $A_t$ 的条件概率；\n",
    "- REINFORCE: $G_t=\\sum_{k=t+1}^{T}\\gamma^{k-t}r_k$\n",
    "    - agent 行动的好坏是根据行动之后获得的奖励综合来评估的，采取某个行动之前获得的奖励与该行动的好坏无关；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdba7ed-a670-42e2-a650-026bcb88432f",
   "metadata": {},
   "source": [
    "### baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f74c512-5195-4439-a445-73b4137b2d52",
   "metadata": {},
   "source": [
    "- minus baseline\n",
    "    - 在一些坏的局势下，做什么动作都会得到负的奖励。所以要减去baseline。（即坏的局势下，仍然有好的 action，比如走出当前困境的 action）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5841dce-cc03-4423-9a8d-0246c8cc8ced",
   "metadata": {},
   "source": [
    "## REINFORCE++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5304d0-0594-42fd-878a-b8233cf81013",
   "metadata": {},
   "source": [
    "> 为了稳定训练\n",
    "\n",
    "- token-level KL-penalty\n",
    "    - $r(s_t,a_t)=\\mathbb I(s_t=[EOS])r(x,y)-\\beta KL(t)$\n",
    "    - The advantage of this Token-level KL is that it seamlessly integrates with Process Reward Models (PRM) and achieves credit assignment, which is only necessary to add $r^{\\text{process}}(s_t, a_t)$at the position of the reward token.\n",
    "        - Recently, some resachers discovered that using an external KL loss (不直接定义在 token 级别的 reward 上) with GRPO for REINFORCE++ also works.\n",
    "- Mini-batch Updates\n",
    "- Reward Normalization and Clipping\n",
    "- Advantage Normalization\n",
    "    - $A_t(s_t, a_t) = r(x, y) - \\beta \\cdot \\sum_{i=t}^{T} \\text{KL}(i)$\n",
    " \n",
    "https://github.com/OpenRLHF/OpenRLHF/blob/main/examples/scripts/train_reinforce_llama_ray.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
