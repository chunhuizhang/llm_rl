{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5249be-3b8e-4d96-96ad-90f4f92eb4f5",
   "metadata": {},
   "source": [
    "- https://hijkzzz.notion.site/reinforce-plus-plus\n",
    "- https://www.bilibili.com/video/BV1eg4y1s7TN\n",
    "    - [pytorch 强化学习] 13 基于 pytorch 神经网络实现 policy gradient（REINFORCE）求解 CartPole\n",
    "- https://medium.com/@2468086464/understanding-reinforcement-learning-from-human-feedback-rlhf-theory-and-the-mechanism-ef45485a5070"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edea97f-1b7a-4d25-91d8-ed3a461ecbc2",
   "metadata": {},
   "source": [
    "### PG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5a6582-7e8a-4a36-a7d0-e5bdffdf58cb",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb E_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau)]\n",
    "$$\n",
    "\n",
    "- 求其 gradient\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\sum_{\\tau}R(\\tau)\\nabla p_\\theta(\\tau)&=\\sum_\\tau R(\\tau) p_\\theta(\\tau)\\nabla \\log p_\\theta(\\tau)\\\\\n",
    "&=\\mathbb E_{\\tau\\sim p_\\theta(\\tau)}R(\\tau)\\nabla \\log p_\\theta(\\tau)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- 对 $\\nabla \\log p_\\theta(\\tau)$ 进行展开\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\nabla \\log p_\\theta(\\tau)&=\\nabla \\log\\left(p(s_1)\\prod p(a_t|s_t)\\prod p(s_{t+1}|s_t,a_t)\\right)\\\\\n",
    "&=\\nabla \\left(\\log p(s_1)+\\sum \\log p(a_t|s_t)+\\sum \\log p(s_{t+1}|s_t,a_t)\\right)\\\\\n",
    "&=\\sum_{t=1}^T\\nabla \\log p(a_t|s_t)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- 因此\n",
    "$$\n",
    "\\begin{split}\n",
    "\\sum_{\\tau}R(\\tau)\\nabla p_\\theta(\\tau)&=\\sum_\\tau R(\\tau) p_\\theta(\\tau)\\nabla \\log p_\\theta(\\tau)\\\\\n",
    "&=\\mathbb E_{\\tau\\sim p_\\theta(\\tau)}R(\\tau)\\nabla \\log p_\\theta(\\tau)\\\\\n",
    "&\\approx\\frac1N\\sum_{n=1}^NR(\\tau^n)\\nabla \\log p_\\theta(\\tau^n)\\\\\n",
    "&=\\frac1N\\sum_{n=1}^N\\sum_{t=1}^{T_n}R(\\tau^n)\\nabla \\log p_\\theta(a_t^n|s_t^n)\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c23ff4-94da-4481-aad3-8695c2c1de5d",
   "metadata": {},
   "source": [
    "## $R(\\tau^n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463659d2-07d8-4a5c-9fa6-8c7293cf7c70",
   "metadata": {},
   "source": [
    "- discounted future reward\n",
    "- minus baseline\n",
    "    - 在一些坏的局势下，做什么动作都会得到负的奖励。所以要减去baseline。（即坏的局势下，仍然有好的 action，比如走出当前困境的 action）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
