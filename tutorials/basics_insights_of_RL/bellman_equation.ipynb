{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f22167-0459-42b7-8597-dc11cedfb0f5",
   "metadata": {},
   "source": [
    "- reward, return, Q(s,a), V(s), advantage\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "A(s,a)&=Q(s,a)-V(s)\\\\\n",
    "&=\\underbrace{\\left(r+\\gamma V(s')\\right)-V(s)}_{\\text{TD error}}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- GAE\n",
    "    - $\\text{GAE}(\\gamma, 0)$, $\\hat A_t:=\\delta_t=r_t+\\gamma V(s_{t+1})-V(s_t)$\n",
    "    - $\\text{GAE}(\\gamma, 1)$, $\\hat A_t:=\\sum_{\\ell=0}^\\infty \\gamma^\\ell\\delta_{t+\\ell}=\\sum_{\\ell=0}^\\infty\\gamma^\\ell r_{t+l}-V(s_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c1adc3-37c3-4cd6-b4c6-34ec5b9216ec",
   "metadata": {},
   "source": [
    "### PG & bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac131ae0-d1f6-4431-87d4-c0d3ae158f0d",
   "metadata": {},
   "source": [
    "策略梯度方法虽然直接优化策略，但其优化信号往往依赖于价值函数的估计（$V^\\pi(s)$，$Q^\\pi(s,a)$），而这些价值函数是通过贝尔曼方程定义或学习得到的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a469c6-8f47-4a38-8c61-8b9860f0ef1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
