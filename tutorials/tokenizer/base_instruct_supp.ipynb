{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a86ba4c-86ed-46ac-98d2-18d872a85840",
   "metadata": {},
   "source": [
    "- verl\n",
    "    - data_preprocess\n",
    "        - 封装数据 dataproto\n",
    "    - generation\n",
    "        - RLHFDataset\n",
    "            - prompt_key: default is `prompt` (cite data_preprocess)\n",
    "            - `tokenizer.apply_chat_template(doc[prompt_key], add_generation_prompt=True)`\n",
    "    - reward_score/\n",
    "- vLLMRollout => gen_batch_padded\n",
    "    - idx.shape: `(, 256)`\n",
    "    - response.shape: `(, 512)`\n",
    "    - `input_ids = torch.cat([idx, response], dim=-1)`\n",
    "\n",
    "```py\n",
    "batch = TensorDict(\n",
    "    {\n",
    "        'prompts': idx,\n",
    "        'responses': response,\n",
    "        'input_ids': seq,  # here input_ids become the whole sentences\n",
    "        # 'old_log_probs': log_probs, # we will recompute old log prob with actor\n",
    "        'attention_mask': attention_mask,\n",
    "        'position_ids': position_ids\n",
    "    },\n",
    "    batch_size=batch_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea760d8-933a-43ab-8bdc-f48c7b6821de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:40:31.121804Z",
     "iopub.status.busy": "2025-03-30T15:40:31.121163Z",
     "iopub.status.idle": "2025-03-30T15:40:36.376523Z",
     "shell.execute_reply": "2025-03-30T15:40:36.374347Z",
     "shell.execute_reply.started": "2025-03-30T15:40:31.121736Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/envs/verl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-30 23:40:35,506\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/home/whaow/anaconda3/envs/verl/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d639e32-f379-49e4-b97b-077f8e74e091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:40:36.378275Z",
     "iopub.status.busy": "2025-03-30T15:40:36.377803Z",
     "iopub.status.idle": "2025-03-30T15:40:36.385764Z",
     "shell.execute_reply": "2025-03-30T15:40:36.383743Z",
     "shell.execute_reply.started": "2025-03-30T15:40:36.378244Z"
    }
   },
   "outputs": [],
   "source": [
    "base_id = 'Qwen/Qwen2.5-1.5B'\n",
    "instruct_id = 'Qwen/Qwen2.5-1.5B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba7bd7d-8679-4f18-a5da-444558d1b2b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:40:36.387049Z",
     "iopub.status.busy": "2025-03-30T15:40:36.386775Z",
     "iopub.status.idle": "2025-03-30T15:40:38.412331Z",
     "shell.execute_reply": "2025-03-30T15:40:38.410313Z",
     "shell.execute_reply.started": "2025-03-30T15:40:36.387024Z"
    }
   },
   "outputs": [],
   "source": [
    "base_T = AutoTokenizer.from_pretrained(base_id)\n",
    "instruct_T = AutoTokenizer.from_pretrained(instruct_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61140e9-f00d-4b6a-b1fe-d746cee1bc9e",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd9f422f-20f4-49ac-8fbd-10e1e9e2547b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:40:38.413421Z",
     "iopub.status.busy": "2025-03-30T15:40:38.413142Z",
     "iopub.status.idle": "2025-03-30T15:41:14.334706Z",
     "shell.execute_reply": "2025-03-30T15:41:14.333629Z",
     "shell.execute_reply.started": "2025-03-30T15:40:38.413394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-30 23:40:55 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='Qwen/Qwen2.5-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 03-30 23:40:57 model_runner.py:1060] Starting to load model Qwen/Qwen2.5-1.5B...\n",
      "INFO 03-30 23:40:57 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 03-30 23:40:58 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.62it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.62it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-30 23:40:59 model_runner.py:1071] Loading model weights took 2.9104 GB\n",
      "INFO 03-30 23:41:00 gpu_executor.py:122] # GPU blocks: 38499, # CPU blocks: 9362\n",
      "INFO 03-30 23:41:00 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 75.19x\n",
      "INFO 03-30 23:41:04 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-30 23:41:04 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-30 23:41:13 model_runner.py:1530] Graph capturing finished in 10 secs.\n"
     ]
    }
   ],
   "source": [
    "base_llm = LLM(model=base_id, max_model_len=8*1024)\n",
    "# instruct_llm = LLM(model=instruct_id, max_model_len=8*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92175732-82da-4910-9e3b-07b77703f2e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:41:14.335677Z",
     "iopub.status.busy": "2025-03-30T15:41:14.335482Z",
     "iopub.status.idle": "2025-03-30T15:41:14.341257Z",
     "shell.execute_reply": "2025-03-30T15:41:14.339999Z",
     "shell.execute_reply.started": "2025-03-30T15:41:14.335658Z"
    }
   },
   "outputs": [],
   "source": [
    "target = 98\n",
    "numbers = [44, 19, 35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f09bee29-1c9e-4f9f-a280-a36d8268a167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:41:14.343196Z",
     "iopub.status.busy": "2025-03-30T15:41:14.343034Z",
     "iopub.status.idle": "2025-03-30T15:41:14.351176Z",
     "shell.execute_reply": "2025-03-30T15:41:14.350258Z",
     "shell.execute_reply.started": "2025-03-30T15:41:14.343181Z"
    }
   },
   "outputs": [],
   "source": [
    "prefix = f\"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\n",
    "User: Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.\n",
    "Assistant: Let me solve this step by step.\n",
    "<think>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f84c2814-0f6c-4954-9587-2a38cd67e7f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:41:14.351838Z",
     "iopub.status.busy": "2025-03-30T15:41:14.351676Z",
     "iopub.status.idle": "2025-03-30T15:41:14.361468Z",
     "shell.execute_reply": "2025-03-30T15:41:14.360590Z",
     "shell.execute_reply.started": "2025-03-30T15:41:14.351821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\n",
      "User: Using the numbers [44, 19, 35], create an equation that equals 98. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.\n",
      "Assistant: Let me solve this step by step.\n",
      "<think>\n"
     ]
    }
   ],
   "source": [
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "241984f9-837c-47cd-80ca-f391d2e90ed2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:41:14.362019Z",
     "iopub.status.busy": "2025-03-30T15:41:14.361848Z",
     "iopub.status.idle": "2025-03-30T15:41:14.379809Z",
     "shell.execute_reply": "2025-03-30T15:41:14.379247Z",
     "shell.execute_reply.started": "2025-03-30T15:41:14.362002Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids = base_T(prefix, return_tensors='pt', add_special_tokens=False)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8428987d-b3df-41e0-baae-152b58866777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:41:14.380602Z",
     "iopub.status.busy": "2025-03-30T15:41:14.380399Z",
     "iopub.status.idle": "2025-03-30T15:41:16.385675Z",
     "shell.execute_reply": "2025-03-30T15:41:16.383891Z",
     "shell.execute_reply.started": "2025-03-30T15:41:14.380585Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  2.00s/it, est. speed input: 71.25 toks/s, output: 190.15 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(\n",
    "        temperature=0.6, max_tokens=4*1024)\n",
    "resp = base_llm.generate(prompt_token_ids=input_ids.squeeze().tolist(), sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ef8dd6e-e6fa-40f4-a07e-df5374d947e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:41:33.914135Z",
     "iopub.status.busy": "2025-03-30T15:41:33.913346Z",
     "iopub.status.idle": "2025-03-30T15:41:33.928269Z",
     "shell.execute_reply": "2025-03-30T15:41:33.926066Z",
     "shell.execute_reply.started": "2025-03-30T15:41:33.914064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35e943ed-cd4c-43b5-981d-4fe51eb7b27e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:42:28.802412Z",
     "iopub.status.busy": "2025-03-30T15:42:28.801632Z",
     "iopub.status.idle": "2025-03-30T15:42:28.819400Z",
     "shell.execute_reply": "2025-03-30T15:42:28.816889Z",
     "shell.execute_reply.started": "2025-03-30T15:42:28.802343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.\n",
      "User: Using the numbers [44, 19, 35], create an equation that equals 98. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.\n",
      "Assistant: Let me solve this step by step.\n",
      "<think>\n"
     ]
    }
   ],
   "source": [
    "print(base_T.decode(resp[0].prompt_token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d31804e2-b41b-405a-80ed-b96861cd96fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:42:30.614702Z",
     "iopub.status.busy": "2025-03-30T15:42:30.613943Z",
     "iopub.status.idle": "2025-03-30T15:42:30.626271Z",
     "shell.execute_reply": "2025-03-30T15:42:30.623956Z",
     "shell.execute_reply.started": "2025-03-30T15:42:30.614636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We need to create an equation using the numbers [44, 19, 35] and basic arithmetic operations (+, -, *, /) such that the equation equals 98. We can use each number only once. </think>\n",
      "\n",
      "1. Let's start by taking the largest number, 44, and see if we can use it to get close to 98.\n",
      "2. If we subtract 35 from 44, we get 9.\n",
      "3. Now, we have 19 left. If we divide 9 by 19, we get a decimal number (0.4736842105263158).\n",
      "4. Since we can't use a decimal number directly, let's try a different approach.\n",
      "5. Let's try adding 19 to 19, which gives us 38.\n",
      "6. Now, we need to find a way to get 60 (98 - 38) using the remaining numbers.\n",
      "7. If we multiply 44 by 35, we get 1540.\n",
      "8. We can divide 1540 by 23 (the result of dividing 19 by 19) to get 66.\n",
      "9. Now, we have 66 left.\n",
      "10. If we divide 66 by 1, we get 66.\n",
      "11. Finally, we can divide 66 by 1 to get the result, which is 98.\n",
      "\n",
      "Therefore, the equation that equals 98 using the numbers [44, 19, 35] and basic arithmetic operations is <answer> (44 - 35) / (19 * 19) + 19 </answer>.\n"
     ]
    }
   ],
   "source": [
    "print(resp[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0a570f-637b-41d0-9180-1dd517a918cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c622d2c8-d5d9-4cb1-acca-09db341e4ea0",
   "metadata": {},
   "source": [
    "### chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c41b735a-9640-448e-b0f4-5c8bef8e6c47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:41:16.408197Z",
     "iopub.status.busy": "2025-03-30T15:41:16.407941Z",
     "iopub.status.idle": "2025-03-30T15:41:16.420311Z",
     "shell.execute_reply": "2025-03-30T15:41:16.419175Z",
     "shell.execute_reply.started": "2025-03-30T15:41:16.408166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|endoftext|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_T.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3fe9f99-8e1c-4265-975b-44b801573ff5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:41:16.421434Z",
     "iopub.status.busy": "2025-03-30T15:41:16.421199Z",
     "iopub.status.idle": "2025-03-30T15:41:16.430483Z",
     "shell.execute_reply": "2025-03-30T15:41:16.429276Z",
     "shell.execute_reply.started": "2025-03-30T15:41:16.421411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(base_T.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1894dd3-d22c-4445-b226-d648a300000d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T15:41:16.431476Z",
     "iopub.status.busy": "2025-03-30T15:41:16.431233Z",
     "iopub.status.idle": "2025-03-30T15:41:16.442129Z",
     "shell.execute_reply": "2025-03-30T15:41:16.440658Z",
     "shell.execute_reply.started": "2025-03-30T15:41:16.431454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(instruct_T.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783464ce-7eef-4056-8c26-026e8a7f4702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
